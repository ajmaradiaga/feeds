<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>https://raw.githubusercontent.com/ajmaradiaga/feeds/main/scmt/topics/SAP-Datasphere-blog-posts.xml</id>
  <title>SAP Community - SAP Datasphere</title>
  <updated>2025-11-20T12:12:59.704768+00:00</updated>
  <link href="https://community.sap.com/t5/c-khhcw49343/SAP Datasphere/pd-p/73555000100800002141" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <subtitle>SAP Datasphere blog posts in SAP Community</subtitle>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/converting-fsv-financial-statement-version-into-a-flattened-structure-using/ba-p/14253164</id>
    <title>Converting FSV (Financial Statement Version) into a Flattened Structure using Python</title>
    <updated>2025-10-29T12:02:14.807000+01:00</updated>
    <author>
      <name>LaxminarayanGupta</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1949744</uri>
    </author>
    <content>&lt;P&gt;&lt;FONT size="6"&gt;&lt;STRONG&gt;Introduction&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;The Financial Statement Version (FSV) is one of the most critical structures in SAP for representing how financial data rolls up across accounts and reporting nodes. This article walks through how to use &lt;STRONG&gt;Python within Datasphere Dataflows&lt;/STRONG&gt; to flatten the hierarchical FSV structure into a simple, analysis-ready table.&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;P&gt;Use case A: Often flattened structures are required by a downstream system (e.g. Onestream, Snowflake or Azure) from SAP Datasphere.&lt;/P&gt;&lt;P&gt;Use case B: Reporting requirements on flattened structure in SAC (SAP Analytics Cloud) stories.&amp;nbsp; &amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="LaxminarayanGupta_0-1761387188585.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/332345i875F2228BC6DCF71/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="LaxminarayanGupta_0-1761387188585.png" alt="LaxminarayanGupta_0-1761387188585.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;EM&gt;*Figure : Example of a Financial Statement Version hierarchy with multiple reporting levels.*&amp;nbsp;&lt;/EM&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="6"&gt;Python Transform&lt;/FONT&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;/STRONG&gt;The FSV in SAP stores hierarchical information - each node represents a reporting group, which may have sub-nodes or leaf nodes pointing to G/L accounts.&lt;/P&gt;&lt;P&gt;Source Data:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="LaxminarayanGupta_2-1761387480550.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/332348iAF920E150328F4E9/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="LaxminarayanGupta_2-1761387480550.png" alt="LaxminarayanGupta_2-1761387480550.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Using a &lt;STRONG&gt;Python Transform&lt;/STRONG&gt; node inside a Datasphere Dataflow, we can achieve this through a recursive traversal. The logic builds two key lookup dictionaries:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;One mapping each &lt;STRONG&gt;parent node&lt;/STRONG&gt; to its &lt;STRONG&gt;children&lt;/STRONG&gt;&lt;/LI&gt;&lt;LI&gt;Another mapping each &lt;STRONG&gt;node ID&lt;/STRONG&gt; to its &lt;STRONG&gt;node name&lt;/STRONG&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;Then, a recursive build_paths() function walks through the hierarchy using &lt;STRONG&gt;Depth-First Search (DFS)&lt;/STRONG&gt; - visiting each node, building its full path (e.g., Financial Statement Version &amp;gt;&amp;nbsp;&lt;EM&gt;Assets &amp;gt; Current Assets&lt;/EM&gt;), and recording its level details (LEVEL1, LEVEL2, etc.).&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# Ensure no missing values in parent IDs
    data["PARENT_ID"] = data["PARENT_ID"].fillna("ROOT")

    # Build lookup dictionary: parent → children
    children_map = data.groupby("PARENT_ID")["NODE_ID"].apply(list).to_dict()
    names_map = data.set_index("NODE_ID")["NODE_NAME"].to_dict()

    # Recursive function to build paths
    def build_paths(node, path):
        name = names_map.get(node, "")
        current_path = path + [name] if name else path
        results = [{"NODE_ID": node,
                    "NODE_NAME": name,
                    "FULL_PATH": " &amp;gt; ".join(current_path),
                    "LEVEL1": current_path[0] if len(current_path) &amp;gt; 0 else None,
                    "LEVEL2": current_path[1] if len(current_path) &amp;gt; 1 else None,
                    "LEVEL3": current_path[2] if len(current_path) &amp;gt; 2 else None,
                    "LEVEL4": current_path[3] if len(current_path) &amp;gt; 3 else None,
                    "LEVEL5": current_path[4] if len(current_path) &amp;gt; 4 else None,
                    "LEVEL6": current_path[5] if len(current_path) &amp;gt; 5 else None,
                    "LEVEL7": current_path[6] if len(current_path) &amp;gt; 6 else None,
                    "LEVEL8": current_path[7] if len(current_path) &amp;gt; 7 else None}]
        for child in children_map.get(node, []):
            results.extend(build_paths(child, current_path))
        return results

    # Start from ROOT (or null parent)
    flattened = []
    for root in children_map.get("ROOT", []):
        flattened.extend(build_paths(root, []))

    # Convert back to DataFrame
    data = pd.DataFrame(flattened)

    return data&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;BR /&gt;The final output is a &lt;STRONG&gt;flat table&lt;/STRONG&gt; showing node ID, name, full path, and level columns - ready for SAC visualizations or direct joins with GL balances.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="LaxminarayanGupta_5-1761387609566.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/332351i2FF4A237E880430D/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="LaxminarayanGupta_5-1761387609566.png" alt="LaxminarayanGupta_5-1761387609566.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;FONT size="6"&gt;&lt;STRONG&gt;Conclusion&amp;nbsp;&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;This approach ensures that the FSV - a cornerstone of SAP Financials remains a powerful, adaptable asset for unified financial reporting and planning.&amp;nbsp; This method simplifies downstream modelling and Python gives developers precise control over custom FSV structures or hybrid hierarchies that span SAP and non-SAP sources.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;(To know more about FSV- refer the SAP learning &lt;A href="https://learning.sap.com/learning-journeys/designing-the-record-to-report-process-in-sap-s-4hana/configuring-a-new-financial-statement-version_fca3de26-5c46-4cfd-8a21-8c38be6e298e" target="_self" rel="noopener noreferrer"&gt;journey&lt;/A&gt;)&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/converting-fsv-financial-statement-version-into-a-flattened-structure-using/ba-p/14253164"/>
    <published>2025-10-29T12:02:14.807000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/from-sap-datasphere-to-a-local-llm-llama-3-1-hands-on-tutorial/ba-p/14253357</id>
    <title>From SAP Datasphere to a Local LLM (Llama 3.1)  — Hands-On Tutorial</title>
    <updated>2025-10-29T12:05:30.405000+01:00</updated>
    <author>
      <name>SethiR</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1792324</uri>
    </author>
    <content>&lt;P&gt;&lt;FONT size="6"&gt;&lt;BR /&gt;&lt;/FONT&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Introduction&amp;nbsp;&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;This post documents a small, reproducible pattern for bringing SAP Datasphere data to a local large language model (LLM) for lightweight analysis. The goal is simple: keep modeling and governance in Datasphere, pull a view into a Jupyter notebook with pandas, and let a local LLM produce machine-readable JSON that you can filter, join, or visualize. The prototype runs on a CPU-only laptop so anyone can follow along without special hardware.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;What this is&lt;/STRONG&gt;:&lt;BR /&gt;-&amp;nbsp;A step-by-step walkthrough that uses hdbcli to query a Datasphere view and Transformers to run Meta Llama 3.1 locally.&lt;BR /&gt;-&amp;nbsp;A row-by-row prompt pattern that returns compact JSON (total, average, flags) you can plug back into your DataFrame. A row-by-row prompt pattern that returns compact JSON (total, average, flags) you can plug back into your DataFrame.&lt;BR /&gt;-&amp;nbsp;A row-by-row prompt pattern that returns compact JSON (total, average, flags) you can plug back into your DataFrame.&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;What this is not:&lt;/STRONG&gt;&lt;BR /&gt;-&amp;nbsp;A benchmarking or performance guide. CPU runs are slow but convenient for learning.&lt;BR /&gt;-&amp;nbsp;A recommendation to hard-code credentials. The POC mirrors the original notebook for clarity; use environment variables or a vault in real projects.A recommendation to hard-code credentials. The POC mirrors the original notebook for clarity; use environment variables or a vault in real projects.&lt;BR /&gt;-&amp;nbsp;A pattern for sending sensitive data to external endpoints. Use test or masked data if you move past local inference. A pattern for sending sensitive data to external endpoints. Use test or masked data if you move past local inference.&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;What you will build:&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;A compact flow: SAP Datasphere View -&amp;gt; Python/Jupyter (hdbcli + pandas) -&amp;gt; row-level prompt -&amp;gt; Local LLM (Transformers) -&amp;gt; JSON back to DataFrame. The same prompts can be pointed to managed inference later for production.&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;H2 id="toc-hId-1763694472"&gt;Prerequisites&lt;/H2&gt;&lt;P&gt;1.&amp;nbsp;SAP Datasphere space with permission to create a Database User and a SQL View&lt;BR /&gt;2.&amp;nbsp;Python 3.10+ with Jupyter&lt;BR /&gt;3.&amp;nbsp;Libraries: pandas, hdbcli, transformers, torch, accelerate, ipython&lt;BR /&gt;4.&amp;nbsp;Hugging Face account + access token (accept access for the Llama 3.1 model)&lt;BR /&gt;&lt;BR /&gt;Security note: The POC code below uses inline credentials to mirror the original run. In real work, put secrets in environment variables or a vault and keep TLS validation enabled.&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Part A — SAP Datasphere&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;A1. Enable database access for the space&lt;/P&gt;&lt;P&gt;1.&amp;nbsp;Open SAP Datasphere -&amp;gt; Spaces -&amp;gt; select your space.&lt;BR /&gt;2.&amp;nbsp;Go to Database Access and confirm SQL access is enabled for the space&amp;nbsp;&lt;/P&gt;&lt;P&gt;A2. Create a Database User&lt;/P&gt;&lt;P&gt;1.&amp;nbsp;Database Access -&amp;gt; Database Users -&amp;gt; Create.Database Access -&amp;gt; Database Users -&amp;gt; Create.&lt;BR /&gt;2.&amp;nbsp;Grant only read privileges/necessary privileges to the schema/view you will query.&lt;BR /&gt;3.&amp;nbsp;Copy the SQL Endpoint (host) and port 443 for Python connectivity. Make sure you copy password and host details and store it in a safe place.&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;A3. Create a demo view with a few rows&lt;/P&gt;&lt;P&gt;Create a SQL View (or graphical view). Here we have taken&amp;nbsp; "ACN_DWC"."DemoView_SETHIR_PY" with columns:&lt;BR /&gt;Stud_ID, Stud_Fname, Stud_Lname, Stud_DOB, Maths, Physics, Chemistry, Total, Stud_Addr, Stud_Faname for the demo.&lt;BR /&gt;Make sure the view is exposed for Consumption.&lt;BR /&gt;Optional seed SQL you can adapt:&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-sql"&gt;&lt;code&gt;SELECT * FROM (
  SELECT 101 AS Stud_ID, 'Arjun'     AS Stud_Fname, 'Singh'    AS Stud_Lname, DATE'2012-04-06' AS Stud_DOB,
         80 AS Maths, 75 AS Physics, 80 AS Chemistry, 235 AS Total, 'Chandigarh' AS Stud_Addr, 'Sukhdev' AS Stud_Faname
  UNION ALL
  SELECT 102, 'Harpreet', 'Kaur',  DATE'2010-09-08', 85, 78, 85, 248, 'Ludhiana',  'Gurinder'
  UNION ALL
  SELECT 103, 'Gursimran','Gill',  DATE'2011-07-09', 95, 85, 90, 270, 'Amritsar',  'Balwinder'
  UNION ALL
  SELECT 104, 'Manpreet', 'Sidhu', DATE'2014-01-01', 90, 85, 95, 270, 'Patiala',   'Harjit'
  UNION ALL
  SELECT 105, 'Jasleen',  'Dhillon',DATE'2012-03-02', 89, 90, 75, 254, 'Jalandhar', 'Paramjit'
) AS t;
SELECT * FROM (
  SELECT 101 AS Stud_ID, 'Arjun'     AS Stud_Fname, 'Singh'    AS Stud_Lname, DATE'2012-04-06' AS Stud_DOB,
         80 AS Maths, 75 AS Physics, 80 AS Chemistry, 235 AS Total, 'Chandigarh' AS Stud_Addr, 'Sukhdev' AS Stud_Faname
  UNION ALL
  SELECT 102, 'Harpreet', 'Kaur',  DATE'2010-09-08', 85, 78, 85, 248, 'Ludhiana',  'Gurinder'
  UNION ALL
  SELECT 103, 'Gursimran','Gill',  DATE'2011-07-09', 95, 85, 90, 270, 'Amritsar',  'Balwinder'
  UNION ALL
  SELECT 104, 'Manpreet', 'Sidhu', DATE'2014-01-01', 90, 85, 95, 270, 'Patiala',   'Harjit'
  UNION ALL
  SELECT 105, 'Jasleen',  'Dhillon',DATE'2012-03-02', 89, 90, 75, 254, 'Jalandhar', 'Paramjit'
) AS t;&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;SAP Datasphere DB user :&amp;nbsp;&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="SAP Datasphere DB user screen" style="width: 521px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/332993i05D97A6906259C90/image-dimensions/521x754/is-moderation-mode/true?v=v2" width="521" height="754" role="button" title="Screenshot 2025-10-27 170111.png" alt="SAP Datasphere DB user screen" /&gt;&lt;span class="lia-inline-image-caption" onclick="event.preventDefault();"&gt;SAP Datasphere DB user screen&lt;/span&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Demo View :&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="SethiR_0-1761564892069.png" style="width: 709px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/332994i6C5415EEA2B13F92/image-dimensions/709x443/is-moderation-mode/true?v=v2" width="709" height="443" role="button" title="SethiR_0-1761564892069.png" alt="SethiR_0-1761564892069.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Part B — Local environment (quick path)&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="3"&gt;1. Create a project folder and venvCreate a project folder and venv&lt;BR /&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-bash"&gt;&lt;code&gt;mkdir datasphere-local-llm &amp;amp;&amp;amp; cd datasphere-local-llm
python -m venv .venv
# Windows: .venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;2.&amp;nbsp;Install requirements&amp;nbsp;&lt;/P&gt;&lt;pre class="lia-code-sample language-bash"&gt;&lt;code&gt;pip install hdbcli
pip install sqlalchemy
pip install sqlalchemy-hana
pip install pandas
pip install hdbcli
pip install transformers torch accelerate
pip install ipython&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;3.&amp;nbsp;Launch Jupyter&lt;/P&gt;&lt;pre class="lia-code-sample language-bash"&gt;&lt;code&gt;jupyter notebook&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Part C — Original POC notebook&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="3"&gt;1. Open a new notebook and write the code.&lt;BR /&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# --- Imports and setup (original)
import pandas as pd
from hdbcli import dbapi
import warnings
from transformers import pipeline
from IPython.display import display
warnings.filterwarnings('ignore')

# --- Inline credentials (POC-style; replace with environment variables for real use)
db_user = 'ACN_DWC#SETHIR_DB'
db_password = 'secret'
db_host = 'secret'
db_port = 443
db_schema = 'ACN_DWC'


connection = dbapi.connect(
    address = db_host,
    port = db_port,
    user = db_user,
    password = db_password,
    encrypt = True,
    sslValidCertificate = False   # POC-only convenience; prefer True in real use
)
print("Connected to SAP Datasphere - confirmation")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;2. If you see the output - "Connected to SAP Datasphere - confirmation" -- this implies you are on right track.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# --- Query the view
view_name = 'DemoView_SETHIR_PY'
sql_query = f'SELECT * FROM "{db_schema}"."{view_name}"'
print(f"Executing query: {sql_query}")

cursor = connection.cursor()
cursor.execute(sql_query)
rows = cursor.fetchall()
columns = [desc[0] for desc in cursor.description]

df = pd.DataFrame(rows, columns=columns)
display(df.head())&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;3. At this point, you should see the outcome of the view that you have. The last statement is used for displaying the data in the form of pandas dataFrame.&lt;BR /&gt;&lt;BR /&gt;4. Now we need to prepare our data , so that the LLM can process it. This can be done by converting our data in the form of string format.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# ==============================================================================
#Prepare Data and Interact with OpenLLM
# ==============================================================================

# Convert DataFrame to a string
data_as_string = df.to_string(index=False)
print("\n--- Data Prepared for LLM ---")
print("The DataFrame has been converted to the following string format:")
print(data_as_string[:300] + "\n...")  # snippet preview&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;5. Prepare data pipeline :&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# --- LLM imports
import os
import torch
import json
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline

# --- LLM Configuration
MODEL_ID = "meta-llama/Meta-Llama-3.1-8B-Instruct"

def load_llama_pipeline():
    """
    Loads the quantized Llama 3 model, tokenizer, and the text-generation pipeline.
    """
    print("\n--- Loading Llama 3 Model ---")
    hf_token = "secret"  # replace with your HF token; accept model access first

    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=hf_token)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        token=hf_token,
        quantization_config=quantization_config,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )

    print("Model loaded. Creating text generation pipeline...")
    return pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
    )&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;6. Build the prompt for the LLM model :&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# --- Prompt builder 
def build_student_analysis_prompt(tokenizer, student_data: pd.Series) -&amp;gt; str:
    """
    Builds a structured prompt for Llama 3 to analyze student marks.
    """
    input_text = (
        f"Student {student_data['Stud_Fname']} {student_data['Stud_Lname']} "
        f"(ID: {student_data['Stud_ID']}) scored {student_data['Maths']} in Maths, "
        f"{student_data['Physics']} in Physics, and {student_data['Chemistry']} in Chemistry."
    )

    messages = [
        {
            "role": "system",
            "content": (
                "You analyze one student's marks and return ONLY a valid JSON object. "
                "Output must start with '{' and end with '}'. No commentary, no markdown, no backticks.\n\n"
                "Schema (keys and types MUST match exactly):\n"
                "{\n"
                '  "total_marks": &amp;lt;int&amp;gt;,\n'
                '  "average_percentage": &amp;lt;float&amp;gt;,\n'
                '  "is_top_performer": &amp;lt;boolean&amp;gt;\n'
                "}\n\n"
                "Rules:\n"
                "1) total_marks = Maths + Physics + Chemistry (each out of 100).\n"
                "2) average_percentage = total_marks / 3.\n"
                "3) Round average_percentage to TWO decimals.\n"
                "4) is_top_performer = true if average_percentage &amp;gt; 80.0; else false.\n"
                "5) Use lowercase true/false for booleans.\n"
                "6) Do not include extra keys. Do not include trailing commas.\n"
                "7) Return JSON only."
            )
        },
        {
            "role": "user",
            "content": input_text
        }
    ]
    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;7. Do the analysis :&amp;nbsp;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# --- Row-by-row analysis
def analyze_student_data(df, text_pipeline):
    """
    Analyzes the student DataFrame row by row using the LLM pipeline.
    """
    print("\n--- Starting Student Performance Analysis with Llama 3 ---")
    results = []
    for index, row in df.iterrows():
        print(f"\nAnalyzing student ID: {row['Stud_ID']}...")

        prompt = build_student_analysis_prompt(text_pipeline.tokenizer, row)

        raw_output = text_pipeline(
            prompt,
            max_new_tokens=128,
            do_sample=False,  # deterministic
            temperature=None,
            top_p=None,
        )[0]['generated_text']

        json_response_str = raw_output[len(prompt):].strip()
        try:
            analysis_result = json.loads(json_response_str)
            results.append(analysis_result)
            print("Analysis successful.")
        except json.JSONDecodeError:
            print(f"  &amp;gt; Failed to decode JSON from model output.")
            print(f"  &amp;gt; Raw model output: {json_response_str}")
            results.append({"error": "Invalid JSON output", "raw_output": json_response_str})

    print("\n--- Analysis Complete ---")
    return pd.DataFrame(results)

# --- Main
if __name__ == "__main__":
    if df is not None and not df.empty:
        try:
            llm_pipeline = load_llama_pipeline()
            analysis_df = analyze_student_data(df, llm_pipeline)

            if analysis_df is not None:
                final_df = pd.concat([df.reset_index(drop=True), analysis_df.reset_index(drop=True)], axis=1)

                print("\n--- Full Data with LLM Analysis ---")
                display(final_df)

                print("\n--- Top Performing Students (Average &amp;gt; 80%) ---")
                top_performers = final_df[final_df['is_top_performer'] == True]

                if not top_performers.empty:
                    display(top_performers[['Stud_ID', 'Stud_Fname', 'Stud_Lname', 'total_marks', 'average_percentage']])
                else:
                    print("No students found with an average greater than 80%.")

        except Exception as e:
            print(f"\nAn unexpected error occurred during the analysis process: {e}")
    else:
        print("\nDataFrame is empty. Cannot proceed with analysis.")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;8.&amp;nbsp;Example output (simulated for the final cell)&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-markup"&gt;&lt;code&gt;Full Data with LLM Analysis (excerpt)

Stud_ID  Stud_Fname  Stud_Lname  Maths  Physics  Chemistry  total_marks  average_percentage  is_top_performer
101      Arjun       Singh       80     75       80         235          78.33               false
102      Harpreet    Kaur        85     78       85         248          82.67               true
103      Gursimran   Gill        95     85       90         270          90.00               true
104      Manpreet    Sidhu       90     85       95         270          90.00               true
105      Jasleen     Dhillon     89     90       75         254          84.67               true

Top Performing Students (Average &amp;gt; 80%)

Stud_ID  Stud_Fname  Stud_Lname  total_marks  average_percentage
102      Harpreet    Kaur        248          82.67
103      Gursimran   Gill        270          90.00
104      Manpreet    Sidhu       270          90.00
105      Jasleen     Dhillon     254          84.67&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Part D —&amp;nbsp;Production path (same pattern, managed inference)&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="3"&gt;When performance or scale matters, swap the local pipeline for a hosted endpoint and keep the Datasphere extraction and prompts identical. When performance or scale matters, swap the local pipeline for a hosted endpoint and keep the Datasphere extraction and prompts identical.&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="3"&gt;1.&amp;nbsp;Databricks Model Serving / Foundation Model Endpoints (REST).&lt;BR /&gt;2.&amp;nbsp;Hugging Face Inference Endpoints (private endpoints; REST with your HF token)&lt;BR /&gt;3.&amp;nbsp;SAP AI Core (containerized model hosting; REST)SAP AI Core (containerized model hosting; REST)&lt;BR /&gt;&lt;BR /&gt;Minimal REST skeleton --&lt;BR /&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import requests, os, json
endpoint = os.getenv("INFERENCE_URL")
token = os.getenv("INFERENCE_TOKEN")
r = requests.post(endpoint, headers={"Authorization": f"Bearer {token}"},
                  json={"inputs": "your_prompt_here", "parameters": {"max_new_tokens": 128, "temperature": 0.0}})
print(r.json())&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;FONT size="3"&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;FONT size="5"&gt;Conclusion ---&lt;BR /&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;This walkthrough demonstrated how to extract data from SAP Datasphere, process it in pandas, and apply a local LLM for row-level analysis that returns clean, machine-readable JSON. The pattern is intentionally small and portable: you can keep iterating locally to refine prompts and outputs, then swap the model call for a managed endpoint (Databricks, Hugging Face, or SAP AI Core) when performance, cost control, or governance call for it. From here, natural next steps include batching larger datasets, persisting results back to a database table, wiring the outputs to SAP Analytics Cloud dashboards, and adding guardrails around data privacy and prompt consistency.&lt;/P&gt;&lt;P&gt;I would be excited to hear how you adapt this to your solutions. Feel free to reach out to me or comment.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/from-sap-datasphere-to-a-local-llm-llama-3-1-hands-on-tutorial/ba-p/14253357"/>
    <published>2025-10-29T12:05:30.405000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/how-to-monitor-consumption-for-sap-business-data-cloud-bdc-and-its/ba-p/14257413</id>
    <title>How to monitor consumption for SAP Business Data Cloud (BDC) and its components?</title>
    <updated>2025-10-30T18:37:48.747000+01:00</updated>
    <author>
      <name>FernandaFroelich</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1410810</uri>
    </author>
    <content>&lt;P&gt;&lt;STRONG&gt;Symptom&lt;/STRONG&gt;: You don't know how to check&amp;nbsp;SAP Business Data Cloud (BDC) usage.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Requirements&lt;/STRONG&gt;: You need to have an S-user and the role of Super Administrator assigned in SAP for Me.&amp;nbsp;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;If you don't have an S-User, please check&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;A href="https://support.sap.com/en/my-support/users/welcome.html" target="_blank" rel="noopener noreferrer"&gt;this page&lt;/A&gt;.&lt;/LI&gt;&lt;LI&gt;If you are unsure which role you have, please check&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;A href="https://me.sap.com/notes/1282821" target="_blank" rel="noopener noreferrer"&gt;this note&lt;/A&gt;.&amp;nbsp;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Resolution&lt;/STRONG&gt;:&lt;/P&gt;&lt;P&gt;1) Open&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;A href="https://me.sap.com/" target="_blank" rel="noopener noreferrer"&gt;SAP for Me&lt;/A&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;and log in.&lt;/P&gt;&lt;P&gt;2) Navigate to the "&lt;EM&gt;Dashboards&lt;/EM&gt;" section and click "&lt;EM&gt;Finance &amp;amp; Legal&lt;/EM&gt;".&lt;/P&gt;&lt;P&gt;3) Select "&lt;EM&gt;Consumption&lt;/EM&gt;".&lt;/P&gt;&lt;P&gt;4) Filter for "&lt;EM&gt;Business Data Cloud&lt;/EM&gt;".&lt;/P&gt;&lt;P&gt;5) Click on the measured number to open a card; it explains the consumption by each metric.&amp;nbsp;&lt;/P&gt;&lt;P class="lia-align-center" style="text-align: center;"&gt;&lt;STRONG&gt;Image 01&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="FernandaFroelich_0-1761845531617.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334479iD0F371C3EF721D18/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="FernandaFroelich_0-1761845531617.png" alt="FernandaFroelich_0-1761845531617.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P class="lia-align-center" style="text-align: center;"&gt;&lt;STRONG&gt;Image 2&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="FernandaFroelich_0-1761848037864.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334512i78FBDF2E31558FC7/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="FernandaFroelich_0-1761848037864.png" alt="FernandaFroelich_0-1761848037864.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P class="lia-align-left" style="text-align : left;"&gt;&lt;STRONG&gt;Important reminder:&amp;nbsp;&lt;/STRONG&gt;SAP Databricks and SAP Datasphere usage is based on actual consumption instead of what is in the quota assignment. For SAP Analytics Cloud, on the other hand, it is considered the number of users assigned for each SAC license according to the quota assignment. For more information on the components usage (SAP Analytics Cloud, SAP Datasphere), please refer to this &lt;A href="https://support.sap.com/content/dam/support/en_us/library/ssp/my-support/systems-installations/system-measurement/cloud-saas-application-usage/sap-business-data-cloud.pdf" target="_blank" rel="noopener noreferrer"&gt;documentation&lt;/A&gt;. If you want to know more about how the metric itself works, check this &lt;A href="https://www.sap.com/about/trust-center/agreements/cloud/cloud-services.html?search=Business%20Data%20Cloud&amp;amp;sort=latest_desc&amp;amp;pdf-asset=aaa18b5a-267f-0010-bca6-c68f7e60039b&amp;amp;page=4" target="_blank" rel="noopener noreferrer"&gt;pdf&lt;/A&gt;.&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/how-to-monitor-consumption-for-sap-business-data-cloud-bdc-and-its/ba-p/14257413"/>
    <published>2025-10-30T18:37:48.747000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/bw-data-product-generator-to-databricks-enterprise-capability/ba-p/14251192</id>
    <title>BW Data Product Generator to Databricks Enterprise Capability</title>
    <updated>2025-10-31T03:51:37.796000+01:00</updated>
    <author>
      <name>JonGooding</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/202972</uri>
    </author>
    <content>&lt;P&gt;In this blog, I will cover getting BW Business Content data into Databricks using the following 4 steps:&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;Enable the BW Data Product Generator in the SAP BW (7.5 or BW/4HANA) instance.&lt;/LI&gt;&lt;LI&gt;Run the BW Data Product Generator on the BW Content&lt;/LI&gt;&lt;LI&gt;Create the Data Product based on the BW Data &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/LI&gt;&lt;LI&gt;Delta Share the Data Product to Enterprise Databricks.&amp;nbsp;&lt;/LI&gt;&lt;/OL&gt;&lt;H3 id="toc-hId-1892715806"&gt;&lt;STRONG&gt;Step 1 -&amp;nbsp;Enable the BW Data Product Generator in the SAP BW (7.5 or BW/4HANA) instance.&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;Installation documentation is via the note at:&amp;nbsp;&lt;A href="https://me.sap.com/notes/3590400/E" target="_blank" rel="noopener noreferrer"&gt;https://me.sap.com/notes/3590400/E&amp;nbsp;&lt;/A&gt;&lt;/P&gt;&lt;P&gt;To install the BW Data Product Generator the current condition applies (speak to your SAP team for potential options to this):&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;EM&gt;You have an SAP Business Warehouse, private cloud edition system (in SAP Business Data Cloud) in place and the object store in SAP Datasphere is enabled. Now, you want to connect to your SAP BW system to your existing&amp;nbsp;SAP Business Data Cloud tenant and the connection is not configured.&lt;/EM&gt;&lt;/P&gt;&lt;P&gt;In addition to the installation guide, making sure the&amp;nbsp;&lt;STRONG&gt;Note Analyzer for Data Product Generator&amp;nbsp;&lt;/STRONG&gt;is run successfully.&amp;nbsp;&lt;/P&gt;&lt;P&gt;The good news in running the SAP_BW_BDC_CONFIGURATION task, is it is now re-runnable and you can reuse the same target space (BWDPG) in my case.&amp;nbsp; I won't go through the details of running the installation - but it relatively straight forward using the documentation - making sure you have the certificates corrected assigned in the BWPCE STRUST.&amp;nbsp;&lt;/P&gt;&lt;P&gt;Once successfully installing the BW DPG, you can access the Data Subscriptions Tile (in BW/4HANA) :&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 11.06.39 am.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334543iD74023C6792FC8AC/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-10-31 at 11.06.39 am.png" alt="Screenshot 2025-10-31 at 11.06.39 am.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Associated Details :&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 11.05.51 am.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334542iB5C220D7CB0E593F/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-10-31 at 11.05.51 am.png" alt="Screenshot 2025-10-31 at 11.05.51 am.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;Also available via the SAP GUI, for use in BW 7.5 :&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 11.04.23 am.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334541i2CD0479A0AA53268/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-10-31 at 11.04.23 am.png" alt="Screenshot 2025-10-31 at 11.04.23 am.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId-1696202301"&gt;&lt;STRONG&gt;Step 2 - Run the BW Data Product Generator on the BW Content&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;In the next steps, I will use the BW Cockpit version (It is similiar using the GUI)&lt;/P&gt;&lt;P&gt;Initially creating and selecting a source:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 11.08.24 am.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334544i1EC806BE2B544380/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-10-31 at 11.08.24 am.png" alt="Screenshot 2025-10-31 at 11.08.24 am.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&lt;SPAN&gt;The following BW InfoProviders can be used in a subscription for the BW DPG:&lt;/SPAN&gt;&lt;/P&gt;&lt;UL class="lia-align-justify" style="text-align : justify;"&gt;&lt;LI&gt;&lt;SPAN&gt;Base Providers: InfoCubes, Datastore Objects (Classic and Advanced), InfoObjects (Masterdata)&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Composite Provider, MultiProvider&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Queries: Query-as-InfoProvider&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 11.09.27 am.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334545iFEE3FDDE6E7B149C/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-10-31 at 11.09.27 am.png" alt="Screenshot 2025-10-31 at 11.09.27 am.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;And selecting the BW Content that is activated.&amp;nbsp;&lt;/P&gt;&lt;P&gt;Once saved, it still needs to be activated as per message below:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 11.11.25 am.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334547i38FB94E863AB83E1/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-10-31 at 11.11.25 am.png" alt="Screenshot 2025-10-31 at 11.11.25 am.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Also, as part of the Data Subscription,&amp;nbsp;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Settings : Allows Full or Delta execution mode, apply filters to the data extraction and add Process Chain Variants to the subscription.&lt;/LI&gt;&lt;LI&gt;Projections : Able to simpify the output columns&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 11.13.06 am.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334548iC66C4251785EB5D7/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="Screenshot 2025-10-31 at 11.13.06 am.png" alt="Screenshot 2025-10-31 at 11.13.06 am.png" /&gt;&lt;/span&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 11.13.17 am.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334549i1F21E2CB9AB1F7DD/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="Screenshot 2025-10-31 at 11.13.17 am.png" alt="Screenshot 2025-10-31 at 11.13.17 am.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Once you have configured any settings, you can activate the Data Subscription:&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 11.15.32 am.png" style="width: 823px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334550iA9DEA127AF947991/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-10-31 at 11.15.32 am.png" alt="Screenshot 2025-10-31 at 11.15.32 am.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Activating the Subscription will also create the Datasphere Local Table in the BWDPG space (accessible from the link in the BW DPG or via Datasphere: :&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 11.21.35 am.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334551iD1322F8F488D600C/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-10-31 at 11.21.35 am.png" alt="Screenshot 2025-10-31 at 11.21.35 am.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Now the Target table exists on the Object store, which can also be seen in the BWDPG.&lt;/P&gt;&lt;P&gt;Side Note:&amp;nbsp;I originally deployed the BW DPG using an earlier release, and the target objects where the Semantics Usage Type was : Local Table (Relational Dataset). Now after applying some recent notes it is now&amp;nbsp; : Local Table (Fact)&lt;/P&gt;&lt;P&gt;As the Data Subscription is now ready, it can be run directly from the Subscription screen:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 11.25.01 am.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334555iDF785CFC514FB765/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-10-31 at 11.25.01 am.png" alt="Screenshot 2025-10-31 at 11.25.01 am.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Viewing the executions:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 11.28.16 am.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334557iFBE52007ABA12560/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-10-31 at 11.28.16 am.png" alt="Screenshot 2025-10-31 at 11.28.16 am.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Now the data from BW exists in Datasphere Object store and if selected, can be delta updated as required&amp;nbsp;&lt;/P&gt;&lt;P&gt;I have shown the BW/4HANA Cockpit for the above, but it's also fully available for the GUI as well:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 11.33.29 am.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334561i09B6C4C6983CBB00/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-10-31 at 11.33.29 am.png" alt="Screenshot 2025-10-31 at 11.33.29 am.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Viewing the BW Data from Datasphere:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 11.38.22 am.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334563i87C69EBF64288A23/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-10-31 at 11.38.22 am.png" alt="Screenshot 2025-10-31 at 11.38.22 am.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 3 -&amp;nbsp;Create the Data Product based on the BW Data&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;I won't cover every step in this process, as there are other blogs that do this, but essentially&lt;/P&gt;&lt;P&gt;The new Data Product, will use the BWDPG space:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 11.43.08 am.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334565iDD43B6F8A5855334/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-10-31 at 11.43.08 am.png" alt="Screenshot 2025-10-31 at 11.43.08 am.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Then add the 2 Data Products - Purchase Order Header and Line Items BW generated objects:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 11.45.39 am.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334566i2C34870824B96494/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-10-31 at 11.45.39 am.png" alt="Screenshot 2025-10-31 at 11.45.39 am.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Then change the status of the Data Product to Listed. Once the Data Product is Listed, then it can be Delta Shared in the BDC Cockpit.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 11.49.03 am.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334567iAC38144349F353D8/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-10-31 at 11.49.03 am.png" alt="Screenshot 2025-10-31 at 11.49.03 am.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 4 -&amp;nbsp;Delta Share the Data Product to Enterprise Databricks.&amp;nbsp;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Within the Business Data Cloud Cockpit, the new BW Data Product is available:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 11.56.39 am.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334568i7DB7AA14A369DB8D/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-10-31 at 11.56.39 am.png" alt="Screenshot 2025-10-31 at 11.56.39 am.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Sharing the Data Product to Enterprise Databricks is via the share button. I have already added the BDC Partner Connector for the DataBricks instance (seperate to this doco)&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 11.59.41 am.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334569iCAD451C5CBE0C111/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-10-31 at 11.59.41 am.png" alt="Screenshot 2025-10-31 at 11.59.41 am.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;The sharing to Enterprise Databricks is quick and easy (no effort around ETL):&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 12.01.55 pm.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334570iB4F38B2B2E3A82C6/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="Screenshot 2025-10-31 at 12.01.55 pm.png" alt="Screenshot 2025-10-31 at 12.01.55 pm.png" /&gt;&lt;/span&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 12.02.24 pm.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334571i63C1BA6C1B7187F3/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="Screenshot 2025-10-31 at 12.02.24 pm.png" alt="Screenshot 2025-10-31 at 12.02.24 pm.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Within the Databricks environment, accessing the Catalog Explorer and the BDC Connect provider:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 12.42.51 pm.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334572i4E86B9C9706186D5/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-10-31 at 12.42.51 pm.png" alt="Screenshot 2025-10-31 at 12.42.51 pm.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;The difference with Delta Sharing from BDC to Enterprise Databricks (as compared to SAP Databricks) is the requirement to select the mount point for the Delta share:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 12.45.21 pm.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334573i329BBBAC7CF6A733/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-10-31 at 12.45.21 pm.png" alt="Screenshot 2025-10-31 at 12.45.21 pm.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Once mounted, the SAP Data Product can be accessed directly in the Catalog:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-10-31 at 3.24.45 pm.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334574iF9F93ED822DEE815/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-10-31 at 3.24.45 pm.png" alt="Screenshot 2025-10-31 at 3.24.45 pm.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;From here, users can perform all the advanced features of Databricks on the SAP BW data.&amp;nbsp;&lt;/P&gt;&lt;P&gt;This has been a basic end to end showcase - hopefully will have time to add in tips and tricks as we build out more customer Use Cases.&amp;nbsp;&lt;/P&gt;&lt;P&gt;Please post any questions and I can try to answer them.&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/bw-data-product-generator-to-databricks-enterprise-capability/ba-p/14251192"/>
    <published>2025-10-31T03:51:37.796000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/integration-between-sap-cpi-and-sap-datasphere-jdbc-connection/ba-p/14256236</id>
    <title>Integration Between SAP CPI and SAP DataSphere (JDBC Connection)</title>
    <updated>2025-10-31T08:17:01.679000+01:00</updated>
    <author>
      <name>MUGILAN_KANAGARAJ</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/2190179</uri>
    </author>
    <content>&lt;P&gt;&lt;STRONG&gt;Integration Between SAP CPI and SAP DataSphere (JDBC Connection)&lt;/STRONG&gt; &lt;SPAN&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;/SPAN&gt;JDBC – JAVA DATABASE CONNECTIVITY&lt;SPAN&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;/SPAN&gt;Why Recommendation for JDBC Over OData API :&lt;SPAN&gt;&lt;BR /&gt;&lt;/SPAN&gt;JDBC is recommended over OData when consuming large-scale records (e.g., 100,000+) because JDBC streams data directly from the database with better performance and less overhead, while OData is optimized for lightweight, paginated, service-based access.&lt;SPAN&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;/SPAN&gt;Problem statement: &lt;SPAN&gt;&lt;A href="https://userapps.support.sap.com/sap/support/knowledge/en/3337495" target="_blank" rel="noopener noreferrer"&gt;3337495 - OData API returns less records than expected due paging&lt;BR /&gt;&lt;/A&gt;&lt;/SPAN&gt;Pagination limits in OData and Ariba APIs can be handled in SAP CPI using a looping process call. I’ll cover this with a clear explanation in an upcoming post.&lt;SPAN&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;/SPAN&gt;G&lt;STRONG&gt;oal:&lt;/STRONG&gt; Connect CPI to a database used by DataSphere (JDBC) and run a simple read data from the (Analytical Model / Table /View). &lt;SPAN&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;/SPAN&gt;For the Write / Delete / Update method, the attached SAP Help Portal Link has syntax in the reference section.&lt;SPAN&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;/SPAN&gt;&lt;STRONG&gt;Prerequisites:&lt;/STRONG&gt;&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;&lt;STRONG&gt;SAP DataSphere&lt;/STRONG&gt; – Subscribed account (⚠ Trial has limited features, JDBC not supported)&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;SAP Integration Suite&lt;/STRONG&gt; – Subscribed or Trial (JDBC actions supported)&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;SAP DataSphere Step by Step Guide :&lt;/STRONG&gt;&lt;SPAN&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;TABLE&gt;&lt;TBODY&gt;&lt;TR&gt;&lt;TD width="301"&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Step&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;/TD&gt;&lt;TD width="301"&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Action / Notes&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="301"&gt;&lt;P&gt;&lt;SPAN&gt;1. Create a Space&lt;/SPAN&gt;&lt;/P&gt;&lt;/TD&gt;&lt;TD width="301"&gt;&lt;P&gt;&lt;SPAN&gt;DataSphere → Space Management → &lt;EM&gt;New Space&lt;/EM&gt; → Name it → Create.&lt;/SPAN&gt;&lt;/P&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="301"&gt;&lt;P&gt;&lt;SPAN&gt;2. Create Table / Analytical Model&lt;/SPAN&gt;&lt;/P&gt;&lt;/TD&gt;&lt;TD width="301"&gt;&lt;P&gt;&lt;SPAN&gt;Data Builder → In your Space → &lt;EM&gt;New&lt;/EM&gt; → Table or Analytical Model → define fields &amp;amp; data types → Save &amp;amp; Publish.&lt;BR /&gt;*Verify Table/Model Deployed Successfully. *&lt;/SPAN&gt;&lt;/P&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="301"&gt;&lt;P&gt;&lt;SPAN&gt;3. Prepare / Load Data&lt;/SPAN&gt;&lt;/P&gt;&lt;/TD&gt;&lt;TD width="301"&gt;&lt;P&gt;&lt;SPAN&gt;Load data manually for testing cases. Otherwise load CSV/import to table via Data Builder/Data Integration.&lt;/SPAN&gt;&lt;/P&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="301"&gt;&lt;P&gt;&lt;SPAN&gt;4. Note Schema &amp;amp; Object Names&lt;/SPAN&gt;&lt;/P&gt;&lt;/TD&gt;&lt;TD width="301"&gt;&lt;P&gt;&lt;SPAN&gt;Record schema name, table name, and view names for JDBC SQL use.&lt;BR /&gt;* Created space name is the SCHEMA name and Collect Table / Model name *&lt;/SPAN&gt;&lt;/P&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="301"&gt;&lt;P&gt;&lt;SPAN&gt;5. Decide Where to Create DB User&lt;/SPAN&gt;&lt;/P&gt;&lt;/TD&gt;&lt;TD width="301"&gt;&lt;P&gt;&lt;SPAN&gt;If HANA Cloud → use HANA Cockpit/DB Explorer. If on-prem DB → use DB admin tools or contact DB Admin.&lt;BR /&gt;* We are using the HANA cloud system for practical session*&lt;/SPAN&gt;&lt;/P&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="301"&gt;&lt;P&gt;&lt;SPAN&gt;6. Create JDBC DB User&lt;/SPAN&gt;&lt;/P&gt;&lt;/TD&gt;&lt;TD width="301"&gt;&lt;P&gt;&lt;SPAN&gt;DB admin tool → Security/Users → &lt;EM&gt;New User&lt;/EM&gt; → set username &amp;amp; strong password → Save.&lt;BR /&gt;*Check Active status of User*&lt;/SPAN&gt;&lt;/P&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="301"&gt;&lt;P&gt;&lt;SPAN&gt;7. Grant Privileges for the DB user&lt;/SPAN&gt;&lt;/P&gt;&lt;/TD&gt;&lt;TD width="301"&gt;&lt;P&gt;&lt;SPAN&gt;Assign only required privileges (e.g., &lt;STRONG&gt;SELECT&lt;/STRONG&gt; for read; add &lt;STRONG&gt;INSERT/UPDATE/DELETE&lt;/STRONG&gt; for CRUD). Best practice: create role &lt;/SPAN&gt;JDBC_ROLE&lt;SPAN&gt; and assign.&lt;/SPAN&gt;&lt;/P&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="301"&gt;&lt;P&gt;&lt;SPAN&gt;8. Prepare JDBC Connection Details&lt;/SPAN&gt;&lt;/P&gt;&lt;/TD&gt;&lt;TD width="301"&gt;&lt;P&gt;&lt;SPAN&gt;Gather JDBC URL (e.g. &amp;nbsp;sample URL from datasphere: z*********-abc.hana.prod-eu10.hanacloud.ondemand.com&lt;BR /&gt;Format for CPI JDBC Material:&lt;BR /&gt;&lt;/SPAN&gt;jdbc:sap://&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;/?encrypt=true&amp;amp;validateCertificate=true&lt;/P&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;/TBODY&gt;&lt;/TABLE&gt;&lt;P&gt;&lt;SPAN&gt;&lt;BR /&gt;&lt;/SPAN&gt;&lt;STRONG&gt;SAP Integration Suite Step by Step Guide :&lt;/STRONG&gt;&lt;SPAN&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;/SPAN&gt;&lt;STRONG&gt;Create a Package &amp;amp; Artifact&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;In CPI → &lt;EM&gt;Design&lt;/EM&gt; → Create a new package → Add an integration flow artifact.&lt;/LI&gt;&lt;LI&gt;&lt;span class="lia-unicode-emoji" title=":white_heavy_check_mark:"&gt;✅&lt;/span&gt; Make sure your CPI user has the required roles to create and access design-time artifacts.&lt;SPAN&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Go to Monitoring → JDBC Material&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;In CPI → &lt;EM&gt;Monitor&lt;/EM&gt; → Integrations and APIs → Manage Security → JDBC Material&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="MUGILAN_KANAGARAJ_9-1761743104479.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/333936i6D0241B617F00B77/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="MUGILAN_KANAGARAJ_9-1761743104479.png" alt="MUGILAN_KANAGARAJ_9-1761743104479.png" /&gt;&lt;/span&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;→ Add &lt;EM&gt;JDBC Data Source. &lt;/EM&gt;→ Select HANA cloud &lt;SPAN&gt;&lt;BR /&gt;&lt;/SPAN&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="MUGILAN_KANAGARAJ_10-1761743104489.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/333937iE85BFA171AC5354A/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="MUGILAN_KANAGARAJ_10-1761743104489.png" alt="MUGILAN_KANAGARAJ_10-1761743104489.png" /&gt;&lt;/span&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;SPAN&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;Provide JDBC URL in the correct format (e.g., jdbc:sap://&amp;lt;hana-host&amp;gt;:443/?encrypt=true&amp;amp;validateCertificate=true).&lt;SPAN&gt;&lt;BR /&gt;&lt;/SPAN&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="MUGILAN_KANAGARAJ_11-1761743104501.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/333938i78679B85543ED506/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="MUGILAN_KANAGARAJ_11-1761743104501.png" alt="MUGILAN_KANAGARAJ_11-1761743104501.png" /&gt;&lt;/span&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;Enter DB username and password (use the dedicated JDBC user created earlier in DataSphere).&lt;SPAN&gt;&lt;BR /&gt;&lt;/SPAN&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="MUGILAN_KANAGARAJ_12-1761743104508.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/333940iFF24BD979E951623/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="MUGILAN_KANAGARAJ_12-1761743104508.png" alt="MUGILAN_KANAGARAJ_12-1761743104508.png" /&gt;&lt;/span&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;Save and deploy the JDBC material.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Apply JDBC Material in iFlow&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;In your integration flow, configure the JDBC receiver adapter → select the JDBC data source created.&lt;/LI&gt;&lt;LI&gt;Use SQL queries (SELECT) in the &lt;EM&gt;Processing tab&lt;/EM&gt; or provide XML query body. &lt;SPAN&gt;Here, I’m using SQL &lt;/SPAN&gt;SELECT * to&lt;SPAN&gt; fetch all records from the table.&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="MUGILAN_KANAGARAJ_13-1761743104513.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/333939i0C4A23EA2DD7435A/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="MUGILAN_KANAGARAJ_13-1761743104513.png" alt="MUGILAN_KANAGARAJ_13-1761743104513.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&lt;BR /&gt;&lt;STRONG&gt;Step 1: Timer Start &lt;/STRONG&gt;&lt;BR /&gt;&amp;nbsp;In this iFlow, the Start Timer is configured with a Simple Schedule → None → On Deployment, which means the integration flow automatically triggers immediately after deployment.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="MUGILAN_KANAGARAJ_14-1761743104520.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/333941iB3F7F36AA2A7DE16/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="MUGILAN_KANAGARAJ_14-1761743104520.png" alt="MUGILAN_KANAGARAJ_14-1761743104520.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&lt;BR /&gt;&lt;STRONG&gt;Step 2: Content Modifier&lt;/STRONG&gt;&lt;BR /&gt;Use this SQL query to fetch all records with the body operation.&lt;BR /&gt;&amp;nbsp;SELECT * FROM "&amp;lt;Schema&amp;gt;"."&amp;lt;Model/TableName&amp;gt;"&lt;BR /&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="MUGILAN_KANAGARAJ_15-1761743104527.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/333943i58EEBA8EA1D3F197/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="MUGILAN_KANAGARAJ_15-1761743104527.png" alt="MUGILAN_KANAGARAJ_15-1761743104527.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;Step 3: Request Reply &amp;amp; JDBC Receiver Adapter&lt;/STRONG&gt;&lt;BR /&gt;&amp;nbsp;→ Use the deployed JDBC Data Source alias in the JDBC Material in the previous step and set Max records count based on your requirement.&lt;BR /&gt;→ JDBC Maximum Records per call:&amp;nbsp; 2,147,483,647&lt;BR /&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="MUGILAN_KANAGARAJ_16-1761743104534.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/333944i6290D296884CAE5A/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="MUGILAN_KANAGARAJ_16-1761743104534.png" alt="MUGILAN_KANAGARAJ_16-1761743104534.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&lt;BR /&gt;Sample data Response from JDBC Connection:&lt;BR /&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="MUGILAN_KANAGARAJ_17-1761743104538.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/333942iC7431EDC092521C8/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="MUGILAN_KANAGARAJ_17-1761743104538.png" alt="MUGILAN_KANAGARAJ_17-1761743104538.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;References :&lt;SPAN&gt;&lt;BR /&gt;same blog by me for clear picture quality:&amp;nbsp;&lt;A href="https://community.sap.com/t5/technology-q-a/integration-of-sap-cpi-and-sap-datasphere-using-jdbc/qaq-p/14256172" target="_blank"&gt;Integration of SAP CPI and SAP DataSphere using JD... - SAP Community&lt;/A&gt;&lt;BR /&gt;&lt;/SPAN&gt;CPI JDBC – XML Query in Body for CRUD Operations (Syntax Guide)&lt;SPAN&gt;&lt;BR /&gt;&lt;/SPAN&gt;&amp;nbsp;link:&lt;SPAN&gt;&lt;BR /&gt;&lt;A href="https://help.sap.com/docs/cloud-integration/sap-cloud-integration/payload-and-operation" target="_blank" rel="noopener noreferrer"&gt;https://help.sap.com/docs/cloud-integration/sap-cloud-integration/payload-and-operation&lt;/A&gt;&lt;/SPAN&gt;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/integration-between-sap-cpi-and-sap-datasphere-jdbc-connection/ba-p/14256236"/>
    <published>2025-10-31T08:17:01.679000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/sap-bdc-enhance-a-bw-data-product-with-sap-databricks-part-ii/ba-p/14259490</id>
    <title>SAP BDC - Enhance a BW Data product with SAP Databricks - Part II</title>
    <updated>2025-11-03T18:50:26.975000+01:00</updated>
    <author>
      <name>inastasi</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/2251802</uri>
    </author>
    <content>&lt;P&gt;This two-part blog series — created by the Italian Solution Advisory team — provides an end-to-end guide for SAP BW customers looking to modernize their analytics landscape using SAP Business Data Cloud (BDC).&lt;/P&gt;&lt;P&gt;The following diagram summarizes the overall workflow of the activities:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_0-1762187784628.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335526iB5F604C86E415A49/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="inastasi_0-1762187784628.png" alt="inastasi_0-1762187784628.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;The &lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-bdc-enhance-a-bw-data-product-with-sap-databricks/ba-p/14242606" target="_self"&gt;first chapter&lt;/A&gt; of the series is structured as follows:&lt;/P&gt;&lt;P&gt;1 - Create a data product subscription to a custom BW infoprovider and replicate the data into SAP BDC&lt;BR /&gt;2 - Govern the custom data product access by data catalog sharing&lt;/P&gt;&lt;P&gt;The second one will be available shortly with this content:&lt;/P&gt;&lt;P&gt;3 - Derive insights by leveraging SAP Databricks ML capabilities&lt;BR /&gt;4 - Incorporate results to support business decisions&lt;/P&gt;&lt;P&gt;Our goal is to demonstrate how SAP BDC can accelerate BW modernization — delivering a state-of-the-art, end-to-end data management and analytics experience.&lt;/P&gt;&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;3 - Derive insights by leveraging SAP Databricks ML capabilities&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="4"&gt;&lt;STRONG&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;Use&lt;/SPAN&gt;&lt;SPAN class=""&gt;-&lt;/SPAN&gt;&lt;SPAN class=""&gt;c&lt;/SPAN&gt;&lt;SPAN class=""&gt;ase&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Anomaly detection in accounting data is a routine of an auditor that reviews financial statements. In General ledger (GL) data, business transactions are represented as journal entries. An atomic record that belongs to a unique journal entry is a transaction with an account type and corresponding attributes including among others a monetary amount with debit-credit sign. Anomalies in this data include amongst others, incorrect account type combinations, wrong corresponding monetary amount, and abnormal amount proportions for the standard account patterns. Such deviations from standards in GL data may account for mistakes or intentional fraudulent activities. Experienced auditors perform assessment of financial statements based on their professional knowledge by manually ’walking through’ sampled GL data.&amp;nbsp; It is a laborious and time intensive task.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Data Product generated from SAP BW/4HANA in previous steps contains GL data extract, thus we decided to implement a machine learning model to illustrate a possibility to facilitate auditors' tasks and suggest potentially anomalous journal entries.&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;For this purpose, Isolation Forest model for Anomaly detection was chosen.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Isolation forest is a type of tree-based ensemble algorithm similar to random forest. The algorithm is designed to assume that inliers in a given set of observations are harder to isolate than outliers (anomalous observations). At a high level, a non-anomalous point, that is a regular journal entry, would live deeper in a decision tree as they are harder to isolate, and the inverse is true for an anomalous point. This algorithm can&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;be trained on a label-less set of observations and subsequently used to predict anomalous records in previously unseen data.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="4"&gt;&lt;STRONG&gt;Data exploration&amp;nbsp;&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;From the &lt;STRONG&gt;Unity Catalog&lt;/STRONG&gt; in &lt;STRONG&gt;Delta Shares Received&lt;/STRONG&gt; section under data product &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;s4_figl4_raw_data&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; you can get familiar with the content of the data set. It is possible to check column types and to examine Sample data. This is a preliminary phase to get to know the data set.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_1-1762188477089.png" style="width: 687px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335530i19831303D16BE5CC/image-dimensions/687x276/is-moderation-mode/true?v=v2" width="687" height="276" role="button" title="inastasi_1-1762188477089.png" alt="inastasi_1-1762188477089.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;We continued &lt;/SPAN&gt;&lt;SPAN class=""&gt;learning about the data &lt;/SPAN&gt;&lt;SPAN class=""&gt;with&lt;/SPAN&gt;&lt;SPAN class=""&gt;in&lt;/SPAN&gt;&lt;SPAN class=""&gt; Databricks &lt;/SPAN&gt;&lt;SPAN class=""&gt;Notbook&lt;/SPAN&gt;&lt;SPAN class=""&gt; using Python&lt;/SPAN&gt;&lt;SPAN class=""&gt;.&lt;/SPAN&gt;&lt;SPAN class=""&gt;&amp;nbsp;&lt;/SPAN&gt; &lt;SPAN class=""&gt;Another&lt;/SPAN&gt; &lt;SPAN class=""&gt;possible way&lt;/SPAN&gt;&lt;SPAN class=""&gt; is to use &lt;/SPAN&gt;&lt;SPAN class=""&gt;SQL Explorer. For this purpose, we created a separate folder&lt;/SPAN&gt; &lt;SPAN class=""&gt;named &lt;/SPAN&gt;&lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;Anomaly detection GL&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN class=""&gt; &lt;SPAN class=""&gt;to keep all related &lt;/SPAN&gt;&lt;SPAN class=""&gt;artifacts&lt;/SPAN&gt; &lt;SPAN class=""&gt;in&lt;/SPAN&gt;&lt;SPAN class=""&gt; one place.&lt;/SPAN&gt;&lt;SPAN class=""&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;SPAN class=""&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_0-1762188680044.png" style="width: 690px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335535i03B1CF0114CA0057/image-dimensions/690x200/is-moderation-mode/true?v=v2" width="690" height="200" role="button" title="inastasi_0-1762188680044.png" alt="inastasi_0-1762188680044.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_1-1762188770878.png" style="width: 682px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335536iF1840BC53B137C05/image-dimensions/682x174/is-moderation-mode/true?v=v2" width="682" height="174" role="button" title="inastasi_1-1762188770878.png" alt="inastasi_1-1762188770878.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Notebook &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;Anomaly detection IF on GL data&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; contains Python code which was executed for data loading, data exploration, features engineering, model building, and saving the result.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;After importing all the necessary Python libraries, we read data set into Spark Data Frame.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_2-1762188841482.png" style="width: 684px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335537iCFD7A66220B3B0BB/image-dimensions/684x106/is-moderation-mode/true?v=v2" width="684" height="106" role="button" title="inastasi_2-1762188841482.png" alt="inastasi_2-1762188841482.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;We checked &lt;/SPAN&gt;&lt;SPAN class=""&gt;the general&lt;/SPAN&gt;&lt;SPAN class=""&gt; statistics of the data frame&lt;/SPAN&gt;&lt;SPAN class=""&gt;.&lt;/SPAN&gt;&lt;SPAN class=""&gt; We noticed that&lt;/SPAN&gt; &lt;SPAN class=""&gt;s&lt;/SPAN&gt;&lt;SPAN class=""&gt;ome numerical features&lt;/SPAN&gt;&lt;SPAN class=""&gt; like &lt;/SPAN&gt;&lt;/SPAN&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;DEBIT_DC&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt; and&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt; CREDIT_LC3&lt;/SPAN&gt; &lt;/SPAN&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;have all&lt;/SPAN&gt;&lt;SPAN class=""&gt; 0 values.&lt;/SPAN&gt;&lt;SPAN class=""&gt; They could be good candidates to be eliminated from &lt;/SPAN&gt;&lt;SPAN class=""&gt;model&lt;/SPAN&gt;&lt;SPAN class=""&gt; training.&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;SPAN class=""&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_3-1762188906419.png" style="width: 684px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335538i156F85BCE9CC0A63/image-dimensions/684x383/is-moderation-mode/true?v=v2" width="684" height="383" role="button" title="inastasi_3-1762188906419.png" alt="inastasi_3-1762188906419.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;In &lt;/SPAN&gt;&lt;SPAN class=""&gt;the exploration&lt;/SPAN&gt;&lt;SPAN class=""&gt; phase, we discovered that our dataset has very few dat&lt;/SPAN&gt;&lt;SPAN class=""&gt;a points before November 2024 &lt;/SPAN&gt;&lt;SPAN class=""&gt;as&lt;/SPAN&gt;&lt;SPAN class=""&gt; can be seen in the graph below. &lt;/SPAN&gt;&lt;SPAN class=""&gt;At this point, we decided to &lt;/SPAN&gt;&lt;SPAN class=""&gt;work only with the newer data&lt;/SPAN&gt;&lt;SPAN class=""&gt;, the last six months,&lt;/SPAN&gt;&lt;SPAN class=""&gt; as highly likely older data &lt;/SPAN&gt;&lt;SPAN class=""&gt;is not fully &lt;/SPAN&gt;&lt;SPAN class=""&gt;available,&lt;/SPAN&gt;&lt;SPAN class=""&gt; and results &lt;/SPAN&gt;&lt;SPAN class=""&gt;wouldn’t&lt;/SPAN&gt;&lt;SPAN class=""&gt; be relevant.&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;SPAN class=""&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_4-1762188997867.png" style="width: 912px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335541i29583C9C78E93CAF/image-dimensions/912x223/is-moderation-mode/true?v=v2" width="912" height="223" role="button" title="inastasi_4-1762188997867.png" alt="inastasi_4-1762188997867.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="4"&gt;&lt;STRONG&gt;Features engineering&amp;nbsp;&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;In the dataset, there are very few numeric features, only 8 of them referring to Credit and Debit amounts.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;All others are categorical features for which it is useful to determine their cardinality to decide if to keep them in the dataset or not.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_5-1762189072855.png" style="width: 918px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335544iD4E60B1DE07DF6A9/image-dimensions/918x234/is-moderation-mode/true?v=v2" width="918" height="234" role="button" title="inastasi_5-1762189072855.png" alt="inastasi_5-1762189072855.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;Features with cardinality &lt;/SPAN&gt;&lt;SPAN class=""&gt;1&lt;/SPAN&gt;&lt;SPAN class=""&gt;, having the same value &lt;/SPAN&gt;&lt;SPAN class=""&gt;for all data points,&lt;/SPAN&gt;&lt;SPAN class=""&gt; provide no useful information for predictive modeling or anomaly detection because they do not differentiate between data points. &lt;/SPAN&gt;&lt;SPAN class=""&gt;Removing these features reduces the dimensionality of the dataset, which can simplify the model and potentially improve its performance and interpretability.&lt;/SPAN&gt;&lt;SPAN class=""&gt; In this exercise, we &lt;/SPAN&gt;&lt;SPAN class=""&gt;deleted&lt;/SPAN&gt;&lt;SPAN class=""&gt; features with cardinality 1.&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;SPAN class=""&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_0-1762189745647.png" style="width: 700px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335546i3C0ECE5C74DCC305/image-dimensions/700x77/is-moderation-mode/true?v=v2" width="700" height="77" role="button" title="inastasi_0-1762189745647.png" alt="inastasi_0-1762189745647.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;Most machine learning algorithms require numerical input to perform their calculations. Categorical data needs to be transformed into a numerical format for these algorithms to &lt;/SPAN&gt;&lt;SPAN class=""&gt;use&lt;/SPAN&gt;&lt;SPAN class=""&gt; effectively. One-hot encoding &lt;/SPAN&gt;&lt;SPAN class=""&gt;provides&lt;/SPAN&gt;&lt;SPAN class=""&gt; a straightforward way to achieve this transformation, ensuring that categorical variables can be integrated into machine learning models&lt;/SPAN&gt;&lt;SPAN class=""&gt; and it&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt; was applied here.&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;SPAN class=""&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_1-1762189793680.png" style="width: 708px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335547i99EF2DE67C40AE78/image-dimensions/708x216/is-moderation-mode/true?v=v2" width="708" height="216" role="button" title="inastasi_1-1762189793680.png" alt="inastasi_1-1762189793680.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="4"&gt;&lt;STRONG&gt;Model building&amp;nbsp;&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;In this exercise, we worked with &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;MLflow.&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;MLflow&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;&amp;nbsp;is an open-source platform designed to simplify the end-to-end machine learning lifecycle.&lt;/SPAN&gt; &lt;SPAN&gt;It provides a comprehensive set of tools and frameworks to manage and track the end-to-end ML development process, including experimentation, reproducibility, deployment, and collaboration.&amp;nbsp;It was developed by Databricks and consists of four main components:&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;SPAN&gt;MLflow Tracking&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;: Logs and queries experiments, including code, data, and configuration.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;SPAN&gt;MLflow Projects&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;: Packages ML code in a reusable, reproducible form.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;SPAN&gt;MLflow Models&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;: Manages and deploys models from various ML libraries.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;SPAN&gt;MLflow Registry&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;: A central repository to manage the full lifecycle of MLflow models, including versioning and annotations.&lt;/SPAN&gt; &lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;SPAN&gt;To create model, we used Python library &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;sklearn.ensemble &lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;which contains ensemble-based methods for classification, regression and anomaly detection.&lt;/SPAN&gt; &lt;SPAN&gt;We passed following parameters to the Isolation Forest method:&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Number of estimators:&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; n_estimators refers to the number of base estimators or trees in the ensemble, i.e., the number of trees that will get built in the forest.&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Max samples:&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; max_samples is the number of samples to be drawn to train each base estimator.&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Contamination:&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; This is a parameter that the algorithm is quite sensitive to and it refers to the expected proportion of outliers in the dataset. This is used when fitting to define the threshold on the scores of the samples..&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;MLflow enables to record the hyperparametrs used to train Isolation Forest model via &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;mlflwo.log_param()&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; function.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_2-1762189906247.png" style="width: 916px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335548i8E352D90C2DA163A/image-dimensions/916x689/is-moderation-mode/true?v=v2" width="916" height="689" role="button" title="inastasi_2-1762189906247.png" alt="inastasi_2-1762189906247.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;We plotted the distribution of anomaly scores.&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;SPAN class=""&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_3-1762189952208.png" style="width: 681px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335549iC8CA3CE9C834C77A/image-dimensions/681x281/is-moderation-mode/true?v=v2" width="681" height="281" role="button" title="inastasi_3-1762189952208.png" alt="inastasi_3-1762189952208.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_4-1762189988931.png" style="width: 673px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335550i3510EAA4C7002A6E/image-dimensions/673x530/is-moderation-mode/true?v=v2" width="673" height="530" role="button" title="inastasi_4-1762189988931.png" alt="inastasi_4-1762189988931.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;SPAN&gt;Two distinct groups:&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;&amp;nbsp;For datasets with clear anomalies, the histogram will often show two main groupings of scores: a dense cluster around a higher value (the normal instances) and a smaller, distinct cluster of low, negative scores (the anomalies).&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;SPAN&gt;Decision boundary:&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;&amp;nbsp;The vertical dashed red line in the histogram represents the&amp;nbsp;&lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;model.threshold_&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;. Points to the left of this line are classified as anomalies, while points to the right are classified as normal.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;SPAN&gt;In our cases, two clusters are not distinctly different; the score distributions overlap, making it harder to pick a single clear cutoff point. The&amp;nbsp;contamination&amp;nbsp;parameter was&amp;nbsp;used to force the model to classify 1% of the data as outliers.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Given that Isolation Forest is an unsupervised algorithm, we don’t know the ground truth; we saved mean anomaly score as a metric. Also, we logged the model and histogram as artifacts.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_5-1762190071628.png" style="width: 661px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335551i64FAA12771CCEB7E/image-dimensions/661x109/is-moderation-mode/true?v=v2" width="661" height="109" role="button" title="inastasi_5-1762190071628.png" alt="inastasi_5-1762190071628.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Next, we assigned anomaly score and marked each data point as anomalous, value –1 or regular, value 1.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_0-1762190459204.png" style="width: 659px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335554iF46AFE03254D4EB0/image-dimensions/659x168/is-moderation-mode/true?v=v2" width="659" height="168" role="button" title="inastasi_0-1762190459204.png" alt="inastasi_0-1762190459204.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Finally, we saved the result into the table &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;gl_anomalies_prediction&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;, in default schema in our workspace.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_7-1762190105158.png" style="width: 655px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335552iC42CFAC35CD455EF/image-dimensions/655x131/is-moderation-mode/true?v=v2" width="655" height="131" role="button" title="inastasi_7-1762190105158.png" alt="inastasi_7-1762190105158.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;Table&lt;/SPAN&gt; &lt;SPAN class=""&gt;became&lt;/SPAN&gt; &lt;SPAN class=""&gt;visible inside of the &lt;STRONG&gt;Catalog&lt;/STRONG&gt;, under the workspace schema default in &lt;STRONG&gt;Tables&lt;/STRONG&gt; section.&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;SPAN class=""&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_1-1762190494280.png" style="width: 408px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335555iDF402027EE505AF1/image-dimensions/408x464/is-moderation-mode/true?v=v2" width="408" height="464" role="button" title="inastasi_1-1762190494280.png" alt="inastasi_1-1762190494280.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Remark 1: &lt;/STRONG&gt;&lt;SPAN&gt;To be able to expose the table as a Data Product in BDC, the table has to have a primary key.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;In this case, we found columns which uniquely identify each data point and created a composite primary key from following columns: ODQ_TSN,&amp;nbsp; ODQ_UNITNO, ODQ_RECORDNO.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_2-1762190531700.png" style="width: 651px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335556i6A124768061969A1/image-dimensions/651x376/is-moderation-mode/true?v=v2" width="651" height="376" role="button" title="inastasi_2-1762190531700.png" alt="inastasi_2-1762190531700.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;&lt;STRONG&gt;Remark 2:&lt;/STRONG&gt; &lt;/SPAN&gt;&lt;/SPAN&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;The information that was logged while running the model is&amp;nbsp;available in the &lt;/SPAN&gt;&lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;Experiments&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt; section under &lt;/SPAN&gt;&lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;Anomaly detection IF on GL data/ML Model&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;.&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;SPAN class=""&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_3-1762190667776.png" style="width: 884px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335557iA4CAC632CEEDBF43/image-dimensions/884x389/is-moderation-mode/true?v=v2" width="884" height="389" role="button" title="inastasi_3-1762190667776.png" alt="inastasi_3-1762190667776.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;For each run we can see mean anomaly score and hyperparameter values.&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;SPAN class=""&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_4-1762190707420.png" style="width: 900px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335558i15E0445B14114FCB/image-dimensions/900x324/is-moderation-mode/true?v=v2" width="900" height="324" role="button" title="inastasi_4-1762190707420.png" alt="inastasi_4-1762190707420.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;If we drill &lt;/SPAN&gt;&lt;SPAN class=""&gt;down,&lt;/SPAN&gt; &lt;SPAN class=""&gt;we can get all the artifacts related to the given run.&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;SPAN class=""&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_5-1762190744285.png" style="width: 893px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335559iE7E2F63F2E2DD650/image-dimensions/893x384/is-moderation-mode/true?v=v2" width="893" height="384" role="button" title="inastasi_5-1762190744285.png" alt="inastasi_5-1762190744285.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&lt;STRONG&gt;Remark 3:&lt;/STRONG&gt; &lt;/SPAN&gt;&lt;SPAN&gt;There are many opportunities to work further in this use case, for example:&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;As we don’t have ground truth labels, we could i&lt;/SPAN&gt;&lt;SPAN&gt;nvolve domain experts to review the detected anomalies and assess their relevance and importance.&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Generate synthetic anomalies by introducing known outliers into the dataset and evaluate the model's performance on detecting these synthetic anomalies.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Apply clustering techniques to group similar anomalies together, which can help in understanding common patterns or types of anomalies.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Adjust the hyperparameters of the Isolation Forest (e.g., `n_estimators`, `max_samples`, `contamination`) to improve performance.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;FONT size="4"&gt;&lt;STRONG&gt;Create a Delta Share&lt;/STRONG&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;To be able to use the result from our analysis, it is necessary to expose the Delta Share to the workspace.&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;The prerequisite is that the user has the necessary privileges to be able to perform the task.&amp;nbsp; The privileges requested are &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;CREATE CATALOG&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;, &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;CREATE SHARE&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;, &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;SET SHARE PERMISSION&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;, &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;USE PROVIDER&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;, &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;USE RECIPIENT&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;, and &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;USE SHARE.&amp;nbsp; &lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;How to add the privileges is described in the following link&amp;nbsp; &lt;/SPAN&gt;&lt;A href="https://help.sap.com/docs/business-data-cloud/sap-databricks/provisioning-sap-databricks-trough-sap-for-me" target="_blank" rel="noopener noreferrer"&gt;&lt;SPAN&gt;Adding Privileges into SAP Databricks | SAP Help Portal.&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;That can be done with the Catalog, choosing the &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;Manage&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; (Gear icon) button and, from the dropdown menu, select &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;Delta sharing&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_6-1762190861636.png" style="width: 870px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335560iD00062D2CBE28428/image-dimensions/870x548/is-moderation-mode/true?v=v2" width="870" height="548" role="button" title="inastasi_6-1762190861636.png" alt="inastasi_6-1762190861636.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Inside of&lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt; Delta Share&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; you’ll find a button &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;Share data.&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_7-1762190913977.png" style="width: 873px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335564i9BD79BFE2952D3DE/image-dimensions/873x203/is-moderation-mode/true?v=v2" width="873" height="203" role="button" title="inastasi_7-1762190913977.png" alt="inastasi_7-1762190913977.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Clicking on &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;Share data you can Crate share by providing following info:&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;&lt;SPAN&gt;&amp;nbsp;In the &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;Share name&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; field, enter a share name, in our case &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;anomaly_detection_gl&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; and choose &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;Save and continue&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;. The &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;Description&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; field is optional.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;SPAN&gt;Add data assets&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;: Select one or more schemas, which includes all the tables in the schema, or one or more individual tables. Choose &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;Save and continue&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_8-1762190913978.png" style="width: 901px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335563iE613ACC859595704/image-dimensions/901x322/is-moderation-mode/true?v=v2" width="901" height="322" role="button" title="inastasi_8-1762190913978.png" alt="inastasi_8-1762190913978.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;SPAN&gt;Add notebooks&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;: (Optional) Specify the &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;Storage location&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;. Choose &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;Save and continue&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;/OL&gt;&lt;OL&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;SPAN&gt;Add recipients&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;: In the &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;Recipient&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; field, from the dropdown menu, select &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;sap-business-data-cloud&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;. Choose &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;Share data&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_9-1762190913980.png" style="width: 894px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335562i68DA9844FC4A8AF9/image-dimensions/894x295/is-moderation-mode/true?v=v2" width="894" height="295" role="button" title="inastasi_9-1762190913980.png" alt="inastasi_9-1762190913980.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="4"&gt;&lt;STRONG&gt;Publishing the Delta Share as Data Product to BDC&amp;nbsp;&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;To complete this step, it is necessary to use Python SDK. In the new notebook, which we called &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;Sharing Anomaly Detection on GL data to BDC, &lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;we installed the library&lt;/SPAN&gt; &lt;STRONG&gt;&lt;SPAN&gt;sap-bdc-connect-sdk.&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_0-1762191209551.png" style="width: 579px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335567i7FFFEC50308D54AF/image-dimensions/579x68/is-moderation-mode/true?v=v2" width="579" height="68" role="button" title="inastasi_0-1762191209551.png" alt="inastasi_0-1762191209551.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;In the next &lt;/SPAN&gt;&lt;SPAN class=""&gt;step,&lt;/SPAN&gt;&lt;SPAN class=""&gt; we c&lt;/SPAN&gt;&lt;SPAN class=""&gt;reate&lt;/SPAN&gt;&lt;SPAN class=""&gt;d&lt;/SPAN&gt;&lt;SPAN class=""&gt; the share in &lt;/SPAN&gt;&lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;sap-&lt;/SPAN&gt;&lt;SPAN class=""&gt;bdc&lt;/SPAN&gt;&lt;SPAN class=""&gt;-connect-&lt;/SPAN&gt;&lt;SPAN class=""&gt;sdk&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;.&lt;/SPAN&gt; &lt;SPAN class=""&gt;A share is a mechanism for distributing and accessing data across different systems. Creating a share involves including specific attributes, such as @openResourceDiscoveryV1, in the request body, aligning with the Open Resource Discovery protocol. This procedure ensures that the share is properly structured and described according to specified standards, &lt;/SPAN&gt;&lt;SPAN class=""&gt;facilitating&lt;/SPAN&gt;&lt;SPAN class=""&gt; effective data sharing and management.&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;SPAN class=""&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_1-1762191259078.png" style="width: 888px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335568i15C2F2C2CF40A57B/image-dimensions/888x375/is-moderation-mode/true?v=v2" width="888" height="375" role="button" title="inastasi_1-1762191259078.png" alt="inastasi_1-1762191259078.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Then, we created CSN share. The CSN serves as a standardized format for configuring and describing shares within a network. It ensures accuracy and compliance with the CSN interoperability specifications, facilitating consistent and effective share of configuration across systems.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_2-1762191287873.png" style="width: 884px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335569i8E3AB229C0A6AE47/image-dimensions/884x338/is-moderation-mode/true?v=v2" width="884" height="338" role="button" title="inastasi_2-1762191287873.png" alt="inastasi_2-1762191287873.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Finaly, we published the data product using the following Python code.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_3-1762191323881.png" style="width: 883px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335571iEA800AB5C7974331/image-dimensions/883x309/is-moderation-mode/true?v=v2" width="883" height="309" role="button" title="inastasi_3-1762191323881.png" alt="inastasi_3-1762191323881.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;4 - Incorporate results to support business decisions&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;The new enhanced Dataproduct, output of the ML calculation is now visibile in SAP BDC:&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_4-1762191470095.png" style="width: 614px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335572iEA5796B479DF9440/image-dimensions/614x356/is-moderation-mode/true?v=v2" width="614" height="356" role="button" title="inastasi_4-1762191470095.png" alt="inastasi_4-1762191470095.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;In SAP Datasphere we can then build a new Analytical model to support insights. The prerequisite is to install the new data product in one of our spaces from SAP Datasphere catalog.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_5-1762191470096.png" style="width: 886px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335573i91563ABC342532C7/image-dimensions/886x422/is-moderation-mode/true?v=v2" width="886" height="422" role="button" title="inastasi_5-1762191470096.png" alt="inastasi_5-1762191470096.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_6-1762191470097.png" style="width: 620px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335574iAA6C8796ADC237AC/image-dimensions/620x309/is-moderation-mode/true?v=v2" width="620" height="309" role="button" title="inastasi_6-1762191470097.png" alt="inastasi_6-1762191470097.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_7-1762191470098.png" style="width: 615px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335575i56A547A9D915C040/image-dimensions/615x243/is-moderation-mode/true?v=v2" width="615" height="243" role="button" title="inastasi_7-1762191470098.png" alt="inastasi_7-1762191470098.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_8-1762191470099.png" style="width: 623px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335576iDD895C5DB2506958/image-dimensions/623x425/is-moderation-mode/true?v=v2" width="623" height="425" role="button" title="inastasi_8-1762191470099.png" alt="inastasi_8-1762191470099.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;After the successful data product installation, data is ready to be consumed. For this purpose, we built a View&amp;nbsp;&lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;GV_ANOMALIES_PREDICTION&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt; to serve as a Fact table. From Data Builder choose New Graphical Model.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_9-1762191470100.png" style="width: 904px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335577iE2BDF73C6826786D/image-dimensions/904x330/is-moderation-mode/true?v=v2" width="904" height="330" role="button" title="inastasi_9-1762191470100.png" alt="inastasi_9-1762191470100.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;We used table&lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt; gl_anomalies_prediction&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; without any filters, defining Semantic Usage as Fact and configuring all monetary fields, credits and debits as measures with SUM aggregation. &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;Anomaly score&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; and &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;anomaly&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; (values –1 and 1) are considered as attributes as they should be used as filters for exposing anomalous journal enteries.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_10-1762191470101.png" style="width: 896px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335579iE285CC3310F3FA85/image-dimensions/896x449/is-moderation-mode/true?v=v2" width="896" height="449" role="button" title="inastasi_10-1762191470101.png" alt="inastasi_10-1762191470101.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;On top of the fact, we built an Analytic Model, &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;AM_ANOMALIES_PREDICTION&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; to use it in SAP Analytics Cloud.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_11-1762191470102.png" style="width: 897px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335580i4BDAA692CA949129/image-dimensions/897x350/is-moderation-mode/true?v=v2" width="897" height="350" role="button" title="inastasi_11-1762191470102.png" alt="inastasi_11-1762191470102.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_12-1762191470103.png" style="width: 882px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335578i13554A7F285DDB50/image-dimensions/882x410/is-moderation-mode/true?v=v2" width="882" height="410" role="button" title="inastasi_12-1762191470103.png" alt="inastasi_12-1762191470103.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;Finally, &lt;/SPAN&gt;&lt;SPAN&gt;Analytic model was used in the Story builder to present to end user records which are identified by the model as potentially anomalous (anomaly attribute is equal to -1).&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inastasi_14-1762191470104.png" style="width: 884px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335582i4438A3CE3321C508/image-dimensions/884x391/is-moderation-mode/true?v=v2" width="884" height="391" role="button" title="inastasi_14-1762191470104.png" alt="inastasi_14-1762191470104.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN class=""&gt;​&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-bdc-enhance-a-bw-data-product-with-sap-databricks-part-ii/ba-p/14259490"/>
    <published>2025-11-03T18:50:26.975000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/sap-datasphere-news-in-october/ba-p/14258069</id>
    <title>SAP Datasphere News in October</title>
    <updated>2025-11-04T06:30:00.036000+01:00</updated>
    <author>
      <name>kpsauer</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/14110</uri>
    </author>
    <content>&lt;P&gt;&lt;STRONG&gt;SAP Datasphere News in October&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Another month with a great new release delivery &lt;span class="lia-unicode-emoji" title=":sparkles:"&gt;✨&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Learn more in my short top features video in October2025 on YouTube &lt;span class="lia-unicode-emoji" title=":television:"&gt;📺&lt;/span&gt; also including an outlook at SAP TechEd session this year.&lt;/P&gt;&lt;P&gt;In addition, explore the latest updates in our community news blogs and more.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="kpsauer_0-1761924309739.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334798i6A01E6965243EEBC/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="kpsauer_0-1761924309739.png" alt="kpsauer_0-1761924309739.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-1763840577"&gt;My top features in October&lt;/H2&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H3 id="toc-hId-1696409791"&gt;Impact of Changes in the Source Objects of an Analytic Model&lt;/H3&gt;&lt;P&gt;Changing, saving, and re-deploying a data builder object again will trigger a status change of any analytic model that uses that object. The analytic model status will change to either “Changes to Deploy” or “Design Time Error”, depending on which change has been made (compatible or incompatible changes). This gives you better transparency in change control handling of your models.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H3 id="toc-hId-1499896286"&gt;Converting a Data Flow Into a Replication Flow and/or Transformation Flow&lt;/H3&gt;&lt;P&gt;When your data flow is eligible to be converted into a transformation flow or a replication flow coupled with a transformation flow, a banner will be displayed in the data flow editor. Using a replication flow and/or a transformation flow will allow you to benefit from improved connectivity, access to delta load processing, and enhanced resiliency and scalability.&lt;BR /&gt;&lt;span class="lia-unicode-emoji" title=":right_arrow:"&gt;➡️&lt;/span&gt;&lt;SPAN&gt;&amp;nbsp; &amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;&lt;A href="https://help.sap.com/docs/PRODUCTS/d4185d7d9a634d06a5459c214792c67e/34ae0a2ea6e94483b19f632a2843d56d.html?version=cloud&amp;amp;locale=en-US" target="_blank" rel="noopener noreferrer"&gt;More information&lt;/A&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H3 id="toc-hId-1303382781"&gt;Technical User to Manage Tasks via CLI&lt;/H3&gt;&lt;P&gt;You can now use OAuth clients with the technical user purpose to manage tasks via the command line interface based on the roles and spaces you assign to the client.&lt;BR /&gt;&lt;span class="lia-unicode-emoji" title=":right_arrow:"&gt;➡️&lt;/span&gt;&lt;SPAN&gt;&amp;nbsp; &amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;&lt;A href="https://help.sap.com/docs/PRODUCTS/d4185d7d9a634d06a5459c214792c67e/2b26a31f197444dea314495bc0008eae.html?version=cloud&amp;amp;locale=en-US" target="_blank" rel="noopener noreferrer"&gt;More information&lt;/A&gt;&lt;/SPAN&gt;&lt;SPAN&gt;.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;div class="video-embed-center video-embed"&gt;&lt;iframe class="embedly-embed" src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F-E6Ah1ZGCfA%3Ffeature%3Doembed&amp;amp;display_name=YouTube&amp;amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D-E6Ah1ZGCfA&amp;amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F-E6Ah1ZGCfA%2Fhqdefault.jpg&amp;amp;type=text%2Fhtml&amp;amp;schema=youtube" width="200" height="112" scrolling="no" title="SAP Datasphere: Top New Features | October 2025" frameborder="0" allow="autoplay; fullscreen; encrypted-media; picture-in-picture;" allowfullscreen="true"&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-977786557"&gt;&lt;SPAN&gt;SAP TechEd 2025&lt;/SPAN&gt;&lt;/H2&gt;&lt;P&gt;&lt;SPAN&gt;There are many sessions about SAP Business Data Cloud and also SAP Datasphere at SAP TechEd 2025. Some are part of the virtual TechEd and others like for instance the onsite hands-on sessions are exclusive for people attending in Berlin.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="SAP BDC @Japan night__.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334802iA58A71DDA358FF90/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="SAP BDC @Japan night__.gif" alt="SAP BDC @Japan night__.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-781273052"&gt;My favorite blogs in October&lt;/H2&gt;&lt;P&gt;My colleague &lt;a href="https://community.sap.com/t5/user/viewprofilepage/user-id/14553"&gt;@Max_Gander&lt;/a&gt; wrote two nice blogs about the new feature with live versions in planning, which I think is a one of the best features I have seen all next to seamless planning itself:&lt;BR /&gt;&lt;SPAN&gt;&lt;span class="lia-unicode-emoji" title=":right_arrow:"&gt;➡️&lt;/span&gt;&amp;nbsp; &amp;nbsp;&lt;/SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/live-versions-from-an-sql-source-for-sap-analytics-cloud-planning/ba-p/14245675" target="_blank"&gt;Live Versions from an SQL Source for SAP Analytics Cloud Planning&lt;/A&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;span class="lia-unicode-emoji" title=":right_arrow:"&gt;➡️&lt;/span&gt;&amp;nbsp; &amp;nbsp;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/unlocking-the-next-chapter-of-seamless-planning-in-sap-business-data-cloud/ba-p/14243864" target="_blank"&gt;Unlocking the Next Chapter of Seamless Planning in SAP Business Data Cloud with Live Versions&lt;/A&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;People Intelligence is one of the latest applications offered as part of SAP Business Data Cloud.&amp;nbsp;&lt;a href="https://community.sap.com/t5/user/viewprofilepage/user-id/3843"&gt;@blberger&lt;/a&gt; has written a informative blog about its general availability and offers many useful links to further details about this offering:&lt;BR /&gt;&lt;SPAN&gt;&lt;span class="lia-unicode-emoji" title=":right_arrow:"&gt;➡️&lt;/span&gt;&amp;nbsp; &amp;nbsp;&lt;/SPAN&gt;&lt;A href="https://community.sap.com/t5/human-capital-management-blog-posts-by-sap/maximize-the-value-of-hr-data-introducing-people-intelligence/ba-p/14252396" target="_blank"&gt;Maximize the value of HR data – Introducing People Intelligence&lt;/A&gt;&lt;BR /&gt;&lt;a href="https://community.sap.com/t5/user/viewprofilepage/user-id/14765"&gt;@VenkatPS&lt;/a&gt;&lt;SPAN&gt;is looking at the same topic from a use case perspective and explains the problems which can be solved using this application:&lt;BR /&gt;&lt;/SPAN&gt;&lt;SPAN&gt;&lt;span class="lia-unicode-emoji" title=":right_arrow:"&gt;➡️&lt;/span&gt;&amp;nbsp; &amp;nbsp;&lt;/SPAN&gt;&lt;A href="https://community.sap.com/t5/delivery-blog-posts/people-intelligence-in-sap-business-data-cloud-is-generally-available/ba-p/14240825" target="_blank"&gt;People Intelligence in SAP Business Data Cloud is Generally Available!&lt;/A&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&lt;a href="https://community.sap.com/t5/user/viewprofilepage/user-id/36635"&gt;@hozumi&lt;/a&gt;highlight another interesting capability we recently delivered as part of SAP BDC, which is called BDC Connect and it helps expand the zero copy delta sharing to external systems:&amp;nbsp;&lt;BR /&gt;&lt;/SPAN&gt;&lt;SPAN&gt;&lt;span class="lia-unicode-emoji" title=":right_arrow:"&gt;➡️&lt;/span&gt;&amp;nbsp; &amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-bdc-connect-sap-elevates-bidirectional-zero-copy-data-sharing-with-game/ba-p/14244067" target="_blank"&gt;SAP BDC Connect: SAP elevates bidirectional zero-copy data sharing with game-changing partnerships&lt;/A&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Keep on brining these valuable blogs &lt;a href="https://community.sap.com/t5/user/viewprofilepage/user-id/1410810"&gt;@FernandaFroelich&lt;/a&gt;. On the one hand she extend her blog series on SAP Databricks by looking more detailed on user and roles with Databricks:&lt;BR /&gt;&lt;/SPAN&gt;&lt;SPAN&gt;&lt;span class="lia-unicode-emoji" title=":right_arrow:"&gt;➡️&lt;/span&gt;&amp;nbsp; &amp;nbsp;&lt;/SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/get-started-with-sap-databricks-users-and-roles/ba-p/14231027" target="_blank"&gt;Get started with SAP Databricks: Users and roles&lt;/A&gt;&lt;BR /&gt;On the other hand, she helps you to better understand the consumption monitoring in SAP BDC:&lt;BR /&gt;&lt;SPAN&gt;&lt;span class="lia-unicode-emoji" title=":right_arrow:"&gt;➡️&lt;/span&gt;&amp;nbsp; &amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/how-to-monitor-consumption-for-sap-business-data-cloud-bdc-and-its/ba-p/14257413" target="_blank"&gt;How to monitor consumption for SAP Business Data Cloud (BDC) and its components?&lt;/A&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-584759547"&gt;More blogs about SAP BDC and Datasphere to check out &lt;span class="lia-unicode-emoji" title=":backhand_index_pointing_down:"&gt;👇&lt;/span&gt;&lt;/H2&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-members/integration-between-sap-cpi-and-sap-datasphere-jdbc-connection/ba-p/14256236" target="_blank"&gt;Integration Between SAP CPI and SAP DataSphere (JDBC Connection)&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/bw-data-product-generator-to-databricks-enterprise-capability/ba-p/14251192" target="_blank"&gt;BW Data Product Generator to Databricks Enterprise Capability&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/how-to-monitor-consumption-for-sap-business-data-cloud-bdc-and-its/ba-p/14257413" target="_blank"&gt;How to monitor consumption for SAP Business Data Cloud (BDC) and its components?&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/planning-your-transition-paths-to-sap-business-data-cloud-data-architecture/ba-p/14257176" target="_blank"&gt;Planning your transition paths to SAP Business Data Cloud (data architecture miniseries)&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/leading-sap-business-warehouse-modernization-as-a-solution-architect-data/ba-p/14256656" target="_blank"&gt;Leading SAP Business Warehouse modernization as a Solution Architect (data architecture miniseries)&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-members/from-sap-datasphere-to-a-local-llm-llama-3-1-hands-on-tutorial/ba-p/14253357" target="_blank"&gt;From SAP Datasphere to a Local LLM (Llama 3.1) — Hands-On Tutorial&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-members/converting-fsv-financial-statement-version-into-a-flattened-structure-using/ba-p/14253164" target="_blank"&gt;Converting FSV (Financial Statement Version) into a Flattened Structure using Python&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-members/building-bridges-between-sap-and-databricks-a-dream-team-for-your-data-and/ba-p/14255167" target="_blank"&gt;Building Bridges Between SAP and Databricks: A Dream Team for Your Data and AI-Powered Analytics&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/unpacking-sap-business-data-cloud-for-solution-architects-data-architecture/ba-p/14255350" target="_blank"&gt;Unpacking SAP Business Data Cloud for Solution Architects (data architecture miniseries)&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/sap-maxattention-blog-posts/sap-maxattention-summit-2026-shaping-the-future-of-enterprise-innovation/ba-p/14252333" target="_blank"&gt;SAP MaxAttention Summit 2026: Shaping the Future of Enterprise Innovation&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/human-capital-management-blog-posts-by-sap/maximize-the-value-of-hr-data-introducing-people-intelligence/ba-p/14252396" target="_blank"&gt;Maximize the value of HR data – Introducing People Intelligence&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-teched-sessions-in-berlin-and-sydney-2025-by-kp-sauer/ba-p/14251763" target="_blank"&gt;SAP TechEd Sessions in Berlin and Sydney 2025 by KP Sauer&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/human-capital-management-blog-posts-by-members/people-intelligence-for-successfactors-what-it-is-and-why-it-matters/ba-p/14252869" target="_blank"&gt;People Intelligence for SuccessFactors – What It Is and Why It Matters&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-members/launch-your-data-science-platform-with-sap-business-data-cloud/ba-p/14250546" target="_blank"&gt;Launch your Data Science Platform with SAP Business Data Cloud&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/developer-news/sap-developer-news-23rd-october-2025/ba-p/14251875" target="_blank"&gt;SAP Developer News 23rd October, 2025&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/join-the-data-and-analytics-value-map-on-sap-community/ba-p/14249416" target="_blank"&gt;Join the Data and Analytics Value Map on SAP Community&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-teched-virtual-business-data-cloud-session-recommendations/ba-p/14248738" target="_blank"&gt;SAP TechEd Virtual Business Data Cloud Session Recommendations&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/introducing-my-metrics-your-personalized-view-of-metrics-in-sap-analytics/ba-p/14241430" target="_blank"&gt;Introducing My Metrics: Your Personalized view of Metrics in SAP Analytics Cloud&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/what-s-new-on-the-sap-analytics-cloud-homepage-2025-q4-qrc-enhancements/ba-p/14234040" target="_blank"&gt;What’s New on the SAP Analytics Cloud Homepage: 2025 Q4 QRC Enhancements!&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sneak-peek-in-to-sap-analytics-cloud-release-for-q4-2025/ba-p/14249322" target="_blank"&gt;Sneak Peek in to SAP Analytics Cloud release for Q4 2025&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/introducing-horizon-themes-and-usability-enhancements-in-sap-analytics/ba-p/14230129" target="_blank"&gt;Introducing Horizon Themes and Usability Enhancements in SAP Analytics Cloud&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-bdc-connect-sap-elevates-bidirectional-zero-copy-data-sharing-with-game/ba-p/14244067" target="_blank"&gt;SAP BDC Connect: SAP elevates bidirectional zero-copy data sharing with game-changing partnerships&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/crm-and-cx-blog-posts-by-sap/implementing-sap-emarsys-loyalty-management-part-2-technical-deep-dive/ba-p/14247872" target="_blank"&gt;Implementing SAP Emarsys Loyalty Management – Part 2 (Technical Deep Dive)&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-members/re-wire-your-existing-sap-datasphere-tenant-from-sap-btp-to-sap-bdc/ba-p/14247423" target="_blank"&gt;Re-wire your existing SAP Datasphere Tenant from SAP BTP to SAP BDC&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-teched-berlin-2025-sap-business-data-cloud-data-products-master-data/ba-p/14246937" target="_blank"&gt;SAP TechEd Berlin 2025: SAP Business Data Cloud: Data products, master data, governance&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-business-data-cloud-%E3%82%AB%E3%82%B9%E3%82%BF%E3%83%A0%E3%83%87%E3%83%BC%E3%82%BF%E3%83%97%E3%83%AD%E3%83%80%E3%82%AF%E3%83%88%E3%81%AE%E4%BD%9C%E6%88%90/ba-p/14235138" target="_blank"&gt;SAP Business Data Cloud: カスタムデータプロダクトの作成&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/delivery-blog-posts/people-intelligence-in-sap-business-data-cloud-is-generally-available/ba-p/14240825" target="_blank"&gt;People Intelligence in SAP Business Data Cloud is Generally Available!&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-members/parameterized-task-chains-what-you-can-and-can-t-do/ba-p/14245534" target="_blank"&gt;Parameterized Task Chains: What You Can and Can’t Do&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/live-versions-from-an-sql-source-for-sap-analytics-cloud-planning/ba-p/14245675" target="_blank"&gt;Live Versions from an SQL Source for SAP Analytics Cloud Planning&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-members/sac-seamless-planning-model-design-principles-compared-to-sap-bw/ba-p/14242687" target="_blank"&gt;SAC seamless planning: Model design principles - compared to SAP BW&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-bdc-enhance-a-bw-data-product-with-sap-databricks/ba-p/14242606" target="_blank"&gt;SAP BDC - Enhance a BW Data product with SAP Databricks&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/unlocking-the-next-chapter-of-seamless-planning-in-sap-business-data-cloud/ba-p/14243864" target="_blank"&gt;Unlocking the Next Chapter of Seamless Planning in SAP Business Data Cloud with Live Versions&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-members/building-ai-ready-life-sciences-enterprise-with-sap-business-data-cloud/ba-p/14242504" target="_blank"&gt;Building AI Ready Life Sciences Enterprise with SAP Business Data Cloud&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/key-planning-amp-analytics-components-of-sap-business-data-cloud-bdc/ba-p/14238066" target="_blank"&gt;Key Planning &amp;amp; Analytics components of SAP Business Data Cloud (BDC)&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/data-and-analytics-blog-posts/sap-business-data-cloud-create-a-custom-data-product-based-on-a-replication/ba-p/14238555" target="_blank"&gt;SAP Business Data Cloud - Create a custom Data Product based on a replication Flow&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-members/step-by-step-implementing-generative-ai-with-sap-hana-cloud/ba-p/14231548" target="_blank"&gt;Step-by-Step: Implementing Generative AI with SAP HANA Cloud&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-members/how-to-connect-to-sap-ibp-from-sap-datasphere-using-odata/ba-p/14233734" target="_blank"&gt;How to Connect to SAP IBP from SAP Datasphere using OData&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/provisioning-sap-business-data-cloud-connect-and-enrolling-with-existing/ba-p/14237512" target="_blank"&gt;Provisioning SAP Business Data Cloud Connect and Enrolling with Existing Databricks&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/from-foundation-to-future-master-data-management-as-the-fuel-for-gen-ai/ba-p/14237459" target="_blank"&gt;From Foundation to Future: Master Data Management as the Fuel for Gen AI&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/what-s-new-in-sap-btp-q3-2025/ba-p/14235530" target="_blank"&gt;What's New in SAP BTP - Q3 2025&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/financial-management-blog-posts-by-sap/extend-existing-planning-scenarios-of-sap-bpc-bw-ip-in-sap-bdc/ba-p/14234247" target="_blank"&gt;Extend existing planning scenarios of SAP BPC/ BW-IP in SAP BDC&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/product-and-customer-updates/exciting-news-people-intelligence-is-now-generally-available/ba-p/14234867" target="_blank"&gt;Exciting News: People Intelligence is Now Generally Available!&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-btp-customer-onboarding-newsletter-october-2025/ba-p/14233774" target="_blank"&gt;SAP BTP Customer Onboarding Newsletter October 2025&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/what-s-new-in-sap-analytics-cloud-modeling-extensions-amp-integration-qrc4/ba-p/14208685" target="_blank"&gt;What's New in SAP Analytics Cloud Modeling Extensions &amp;amp; Integration QRC4 2025 Release&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/get-started-with-sap-databricks-users-and-roles/ba-p/14231027" target="_blank"&gt;Get started with SAP Databricks: Users and roles&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/sap-maxattention-blog-posts/from-strategy-to-impact-tqm-multiplier-week-2025-highlights/ba-p/14230628" target="_blank"&gt;From Strategy to Impact: TQM Multiplier Week 2025 Highlights&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-data-and-analytics-forum-18-19-11-2025/ba-p/14232575" target="_blank"&gt;SAP Data and Analytics Forum 18.-19.11.2025&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/enterprise-resource-planning-blog-posts-by-sap/sap-enterprise-support-academy-newsletter-october-2025/ba-p/14232476" target="_blank"&gt;SAP Enterprise Support Academy Newsletter October 2025&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/enterprise-architecture-blog-posts/fast-track-your-transition-to-sap-business-data-cloud-%EF%B8%8F/ba-p/14232169" target="_blank"&gt;Fast Track Your Transition to SAP Business Data Cloud!&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="kpsauer_0-1761924614923.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334799i9C6F5F225AE081E3/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="kpsauer_0-1761924614923.png" alt="kpsauer_0-1761924614923.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Find more information and related blog posts on the&amp;nbsp;&lt;SPAN&gt;&lt;A href="https://pages.community.sap.com/topics/datasphere" target="_blank" rel="noopener noreferrer"&gt;topic page for SAP Datasphere&lt;/A&gt;&lt;/SPAN&gt;. You will find further product information on our Community with various subpages about &lt;SPAN&gt;&lt;A href="https://pages.community.sap.com/topics/datasphere/business-content" target="_blank" rel="noopener noreferrer"&gt;Business Content&lt;/A&gt;&lt;/SPAN&gt;, the&amp;nbsp;&lt;SPAN&gt;&lt;A href="https://pages.community.sap.com/topics/datasphere/bw-bridge" target="_blank" rel="noopener noreferrer"&gt;SAP BW Bridge&lt;/A&gt;&lt;/SPAN&gt; as well as content for&amp;nbsp;&lt;SPAN&gt;&lt;A href="https://pages.community.sap.com/topics/datasphere/best-practices-troubleshooting" target="_blank" rel="noopener noreferrer"&gt;Best Practices &amp;amp; Troubleshooting&lt;/A&gt;&lt;/SPAN&gt;. Also check out the new &lt;SPAN&gt;&lt;A href="https://help.sap.com/docs/SUPPORT_CONTENT/datasphere/4181116697.html?locale=en-US" target="_blank" rel="noopener noreferrer"&gt;support content for SAP Datasphere&lt;/A&gt;&lt;/SPAN&gt; on SAP Help for troubleshooting and analysis guides, how-to guides, technical details, and more.&lt;/P&gt;&lt;P&gt;Find out how to unleash the power of your business data with SAP’s free learning content on &lt;SPAN&gt;&lt;A href="https://learning.sap.com/learning-journey/explore-sap-datasphere?source=social-meta-prdteng-ExploreSAPDatasphere" target="_blank" rel="noopener noreferrer"&gt;SAP Datasphere&lt;/A&gt;&lt;/SPAN&gt;. It’s designed to help you enrich your data projects, simplify the data landscape, and make the most out of your investment. Check out even more role-based learning resources and opportunities to get certified in one place on &lt;SPAN&gt;&lt;A href="https://learning.sap.com/?url_id=text-sapcommunity-prdteng" target="_blank" rel="noopener noreferrer"&gt;&amp;nbsp;SAP Learning site.&lt;/A&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-datasphere-news-in-october/ba-p/14258069"/>
    <published>2025-11-04T06:30:00.036000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/sap-datasphere-export-data-of-analyticalmodel-via-odata-url-amp-oauth/ba-p/14256993</id>
    <title>SAP Datasphere : Export Data of AnalyticalModel via Odata URL &amp; Oauth Client of type Technical User</title>
    <updated>2025-11-05T07:58:30.289000+01:00</updated>
    <author>
      <name>vikasparmar88</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1528256</uri>
    </author>
    <content>&lt;P&gt;In this blog, I have explained how to export data from an analytical model into CSV file securely using an OData URL and a technical user OAuth client.&lt;/P&gt;&lt;P&gt;Step - 1) Create an Oauth Client with Purpose as Technical User and select required roles. get client ID and secret and save it.&amp;nbsp;&lt;A href="https://help.sap.com/docs/SAP_DATASPHERE/9f804b8efa8043539289f42f372c4862/88b13468fc3c4ebd972bcb8faa6cafbf.html" target="_self" rel="noopener noreferrer"&gt;How to Guide&lt;/A&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Oauth.png" style="width: 347px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334244iFF68CE80547515B7/image-dimensions/347x368/is-moderation-mode/true?v=v2" width="347" height="368" role="button" title="Oauth.png" alt="Oauth.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Step - 2) Create Odata_Tech_User_OauthClient.py file with below code&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# --- IMPORTS ---
import requests              # For making HTTP requests to token and OData endpoints
import pandas as pd          # For handling tabular data from the OData response
import os                    # For file path resolution and environment variable access
from dotenv import load_dotenv  # For loading credentials from a .env file

# --- LOAD ENV VARIABLES ---
load_dotenv()                # Load environment variables from .env file into the runtime

# --- CONFIG: Read credentials and endpoints from environment ---
odata_url = os.getenv("ODATA_URL")             # OData service endpoint
token_url = os.getenv("TOKEN_URL")             # OAuth token endpoint
client_id = os.getenv("CLIENT_ID")             # OAuth client ID
client_secret = os.getenv("CLIENT_SECRET")     # OAuth client secret

# --- GET TOKEN: Request access token using client credentials ---
token_payload = {
    "grant_type": "client_credentials",        # OAuth flow type
    "client_id": client_id,                    # Injected from .env
    "client_secret": client_secret             # Injected from .env
}
token_resp = requests.post(token_url, data=token_payload)  # POST request to token endpoint
token_resp.raise_for_status()                 # Raise error if token request fails
access_token = token_resp.json()["access_token"]  # Extract access token from response

# --- CALL ODATA SERVICE: Fetch data using bearer token ---
headers = {"Authorization": f"Bearer {access_token}"}  # Auth header with token
response = requests.get(odata_url, headers=headers)    # GET request to OData endpoint
response.raise_for_status()                            # Raise error if data fetch fails

# --- PARSE RESULTS: Convert JSON payload to DataFrame ---
data = response.json()["value"]         # Extract 'value' list from OData response
df = pd.DataFrame(data)                 # Convert list of records to pandas DataFrame

# --- DISPLAY OR EXPORT: Show preview and optionally save to CSV ---
print()
print("🔍 Displaying top rows for quick inspection:")
print()
print(df.head())                        # Display top rows for quick inspection
print()

# --- Extract view name from OData URL ---
view_name = odata_url.rstrip("/").split("/")[-1]  # Gets 'AM_EXPORT' from the URL

# --- Construct filename ---
filename = f"{view_name}.csv"

# --- Display and save ---
df.to_csv(filename, index=False)
print(f"📁 Exported data Saved to: {os.path.abspath(filename)}")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Step - 3) Create .env file with all values of variables.&lt;/P&gt;&lt;P&gt;ODATA_URL : Copy the Odata link from analytical model&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="odata.png" style="width: 630px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334255i61C52D8A5777AD69/image-dimensions/630x162/is-moderation-mode/true?v=v2" width="630" height="162" role="button" title="odata.png" alt="odata.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="odata.png" style="width: 517px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334257iF036F0A98768B92D/image-dimensions/517x297/is-moderation-mode/true?v=v2" width="517" height="297" role="button" title="odata.png" alt="odata.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;TOKEN_URL : get it from Datasphere -&amp;gt; System -&amp;gt; App Integration page&lt;/P&gt;&lt;P&gt;CLIENT ID &amp;amp; Secret : Get it from Step-1&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Variables.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334246iBF8AA617FEDF18B0/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Variables.png" alt="Variables.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Step-&amp;nbsp; 4) Create .bat file with blow code&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt; off

chcp 65001 &amp;gt;nul

REM ───────────────────────────────────────────────
REM 🚀 ODATA TECH USER OAUTH CLIENT EXECUTION SCRIPT
REM ───────────────────────────────────────────────


echo.
echo ==============================================
echo 🔄 Starting OData export process...
echo ==============================================

REM --- Navigate to script directory ---
cd /d "%~dp0"

REM --- Run the Python script ---
echo 🐍 Running Python script: Odata_Tech_User_OauthClient.py
python Odata_Tech_User_OauthClient.py

REM --- Completion message ---
echo.
echo ✅ Script execution completed.
echo ==============================================

REM --- Keep window open ---
pause&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Keep all files in same directory&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Folder.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334250i25167CB60103AFB0/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Folder.png" alt="Folder.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;!--  StartFragment   --&gt;&lt;/P&gt;&lt;P&gt;Once everything is set up, just double-click the .bat&amp;nbsp;file to run the process. It will execute the Python script and, once finished, generate a .csv file name exactly same name as the analytical model name. As part of the execution, the first five rows of data will also be displayed on screen for quick preview&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="output.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334261i1BE9F41A80C0F984/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="output.png" alt="output.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Folder.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334248iA75EC24BCB3F6067/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Folder.png" alt="Folder.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Thanks&lt;/P&gt;&lt;P&gt;Vikas Parmar&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/sap-datasphere-export-data-of-analyticalmodel-via-odata-url-amp-oauth/ba-p/14256993"/>
    <published>2025-11-05T07:58:30.289000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/evolution-of-data-and-analytics-with-sap-business-data-cloud/ba-p/14260476</id>
    <title>Evolution of Data and Analytics with SAP Business Data Cloud</title>
    <updated>2025-11-06T08:37:22.657000+01:00</updated>
    <author>
      <name>DHANALAKSHMI_NH</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/2236818</uri>
    </author>
    <content>&lt;P&gt;&lt;STRONG&gt;Introduction to Business Data Cloud:&lt;/STRONG&gt;&lt;BR /&gt;&lt;BR /&gt;SAP Business Data Cloud was introduced on&amp;nbsp;February 13 2025 as Software-as-a-Service (SaaS) solution fully managed by SAP.&amp;nbsp;SAP Business Data Cloud is a data &amp;amp; analytics platform that harmonizes all data from SAP and non-SAP sources into a unified semantic layer of trusted data that governs, connects and share data securely across systems or cloud. By integrating&amp;nbsp;powerful functionality of SAP Datasphere, SAP Analytics Cloud, and SAP Databricks and&amp;nbsp;SAP Business Warehouse.&amp;nbsp;It empowers businesses to break data silos to power advanced AI and&amp;nbsp;analytics.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Why was Business data cloud introduced?&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;To get&amp;nbsp;&lt;STRONG&gt;easier&amp;nbsp;access to SAP data&lt;/STRONG&gt;&amp;nbsp;without the need for complex data preparation steps such as extraction, building semantics, transformation and loading data for analytics.&lt;/LI&gt;&lt;LI&gt;To get&lt;STRONG&gt; access to&amp;nbsp;all&amp;nbsp;SAP data from all Line of Business (LoB)&lt;/STRONG&gt;&amp;nbsp;applications, such as SAP S/4HANA, SAP SuccessFactors and SAP Ariba without the need of 3rd party connector &amp;amp; tool.&lt;/LI&gt;&lt;LI&gt;To get&amp;nbsp;&lt;STRONG&gt;a unified&amp;nbsp;business view&lt;/STRONG&gt;&amp;nbsp;of the data regardless of which SAP application it originated from without making harmonization difficult and holds its&amp;nbsp;original business context without any change.&lt;/LI&gt;&lt;LI&gt;To&lt;STRONG&gt;&amp;nbsp;accelerate AI with trusted and secure&amp;nbsp;data&lt;/STRONG&gt;&amp;nbsp;that they can use to build reliable AI applications to accelerate innovation and insights with AI.&lt;/LI&gt;&lt;LI&gt;To get&lt;STRONG&gt;&amp;nbsp;faster access to&amp;nbsp;SAP data insights&amp;nbsp;&lt;/STRONG&gt;by removing data preparation and dashboard building steps.&lt;/LI&gt;&lt;LI&gt;To get&lt;STRONG&gt;&amp;nbsp;a Simplified Landscape&lt;/STRONG&gt;&amp;nbsp;by reducing multiple tools, silos and complex tasks of manual data preparation, development and analytics.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Key Features &amp;amp; Capabilities of Business Data Cloud:&amp;nbsp;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;SAP Business Data Cloud Key Innovation of out-of-the-box components&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Intelligent Applications&lt;/STRONG&gt;:&amp;nbsp;Intelligent Applications are predefined, SAP-managed dashboards for analytics based on underlying Data Products and models.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Data Products&lt;/STRONG&gt;: Create and share governed, reusable data products that deliver consistent, high-quality business information across teams and systems.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;BDC Cockpit&lt;/STRONG&gt;: Centrally manage your data products, monitor usage, and enforce governance policies, all from one intuitive interface.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;SAP Business Data Cloud Integrated Software components:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;SAP Datasphere:&lt;/STRONG&gt;&amp;nbsp;to provide the data integration and modelling.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;SAP Analytics Cloud&lt;/STRONG&gt;: to provide the analysis and planning.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;SAP Databricks&lt;/STRONG&gt;: to provide AI and machine learning enhancements.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;SAP BW/BW4HANA&lt;/STRONG&gt;: can be used as a data source for datasphere &amp;amp; databricks.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;SAP Business Data Cloud is built on SAP Business Technology Platform:&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="BTP.png" style="width: 993px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/336429i82075B466C11F998/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="BTP.png" alt="BTP.png" /&gt;&lt;/span&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Unified Customer Landscape&amp;nbsp;&lt;/STRONG&gt;used to connect the SAP Business Data Cloud components.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;SAP HANA Cloud&lt;/STRONG&gt;&amp;nbsp;provides the data storage layer.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;SAP Security and Compliance&lt;/STRONG&gt;&amp;nbsp;to manage access and permissions and governs regulatory compliance.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;SAP Development &amp;amp; Integration&lt;/STRONG&gt;&amp;nbsp;to provide developer tools for modelling &amp;amp; data Integration&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;SAP AI &amp;amp; Analytics&lt;/STRONG&gt;&amp;nbsp;to provide advanced analytics and AI powered insights.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;SAP Business Data Cloud Architecture:&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="SAP_BDC_Architecture.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/336432i64C2492DD9A371CF/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="SAP_BDC_Architecture.png" alt="SAP_BDC_Architecture.png" /&gt;&lt;/span&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Source System layer:&amp;nbsp;&lt;/STRONG&gt;It harmonizes all data from SAP and non-SAP sources, into a unified semantic layer of trusted data, to power advanced analytics and to build AI applications.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Foundation Service Layer: &lt;/STRONG&gt;This layer replicates and harmonizes data from the source systems to produce curated, governed&amp;nbsp;Data Products&amp;nbsp;that retain their original business context.&amp;nbsp;Data products are grouped by&amp;nbsp;Data Packages&amp;nbsp;to make it easy to discover them and to activate them.&amp;nbsp;This data is stored in a hyper scaler environment using SAP HANA Cloud and Data Lake files.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Business Data Fabric Layer:&amp;nbsp;&lt;/STRONG&gt;It combines the strengths of SAP Datasphere, SAP Analytics Cloud (SAC), and SAP Business Warehouse (BW) capabilities, enhanced by the Databricks platform to unify an organization with SAP and Non – SAP and third-party data into a single, trusted foundation model.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;SAP Datasphere&amp;nbsp;&lt;/STRONG&gt;is the technological foundation for the business data fabric within BDC.&amp;nbsp;It integrates, transforms, and enriches data from SAP and non-SAP sources into powerful semantic models. Data can be replicated into&amp;nbsp;Datasphere&amp;nbsp;for faster performance or accessed virtually from its source, allowing for flexible management of a hybrid data landscape.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;SAP Databricks&amp;nbsp;&lt;/STRONG&gt;is an Advanced AI and machine learning technology.&amp;nbsp;It provides a pro-code environment for data engineers and AI developers to build, train, and deploy custom AI and ML models using the rich, semantically aware data from SAP applications.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;SAP Business Warehouse (BW) or BW/4HANA:&lt;/STRONG&gt; For customers with existing on-premises SAP Business Warehouse (BW) or BW/4HANA systems, this component provides a modernization path within the Business Data Fabric.&amp;nbsp;A&amp;nbsp;Data Product Generator&amp;nbsp;can be used to convert BW Info Provider data into cloud-ready data products, which can then be consumed by&amp;nbsp;Datasphere&amp;nbsp;for analytics or by&amp;nbsp;Databricks&amp;nbsp;for AI/ML.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;SAP Business Data Cockpit&lt;/STRONG&gt;:&amp;nbsp;SAP Business Data Cloud cockpit allows you to browse and find pre delivered Intelligent Applications and data products, install Intelligent Applications, and share data products with SAP Datasphere &amp;amp; Databricks to enable analytics &amp;amp; enrich&amp;nbsp;with AI and machine learning capabilities.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Intelligent Application Layer:&lt;/STRONG&gt;&amp;nbsp;It’s a unified semantic layer of trusted data, to power advanced AI &amp;amp; Analytics.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;SAP Analytical Cloud:&amp;nbsp;&lt;/STRONG&gt;Using SAP Analytics Cloud, business users can run ad-hoc analysis, explore the provided data with AI-driven chat capabilities, or act on their insights with AI-driven suggestions, bridging the gap between analytics and their end-to-end business processes.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Intelligent Application:&amp;nbsp;&lt;/STRONG&gt;SAP Business Data Cloud Intelligent Applications&amp;nbsp;are used to provide the required dashboard in out-of-the-box reporting scenarios&amp;nbsp;with its advanced visualization and planning functions&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Joule:&amp;nbsp;&lt;/STRONG&gt;Joule uses natural language processing to interact, access insights, and perform tasks across SAP and third-party applications. It uses "agents" and "skills" to automate multi-step workflows and provide contextual assistance.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;How SAP Business Data Cloud work as one domain model:&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Scenario 1: Out-of-the-Box model with Datasphere&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 1:&lt;/STRONG&gt; The first step is to identify the right intelligent application for your business scenario, In SAP Business Data Cloud cockpit search for available intelligent application and install the one that meets your requirement.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 2:&lt;/STRONG&gt;&amp;nbsp;When intelligent application installation is initiated the relevant business data that has already been bundled&amp;nbsp;into a package in your business application is accessed and&amp;nbsp;replicated to the Foundation Services of SAP Business Data Cloud.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 3:&lt;/STRONG&gt;&amp;nbsp;Inside the Foundation Services, the replicated business&amp;nbsp;data is harmonized with other data packages from different&amp;nbsp;business applications such as SAP SuccessFactors the data&amp;nbsp;is transformed and enriched to become a data product.&amp;nbsp;The data is now ready to be used as the basis for analysis.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 4:&lt;/STRONG&gt; In&amp;nbsp;SAP Datasphere all artifacts necessary for the analysis&amp;nbsp;are now automatically generated on top of the Data Product.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 5:&lt;/STRONG&gt; Visual representations of the insights are&amp;nbsp;automatically generated and made available in the intelligent application dashboard.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="Out of the box.png" style="width: 569px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/336435i5752C60D5B26CB23/image-dimensions/569x315/is-moderation-mode/true?v=v2" width="569" height="315" role="button" title="Out of the box.png" alt="Out of the box.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;STRONG&gt;Scenario 2:&amp;nbsp; AI and machine learning enhancement by Databricks&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 1:&lt;/STRONG&gt; The first step is to identify the right intelligent application for your business scenario, In SAP Business Data Cloud cockpit search for available intelligent application and install the one that meets your requirement.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 2:&amp;nbsp;&lt;/STRONG&gt;When intelligent application installation is initiated&amp;nbsp;the relevant business data that has already been bundled&amp;nbsp;into a package in your business application is accessed and&amp;nbsp;replicated to the Foundation Services of SAP Business Data Cloud.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 3:&lt;/STRONG&gt;&amp;nbsp;Inside the Foundation Services, the replicated business&amp;nbsp;data is harmonized with other data packages from different&amp;nbsp;business applications such as SAP SuccessFactors the data&amp;nbsp;is transformed and enriched to become a data product.&amp;nbsp;The data is now ready to be used as the basis for analysis.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 4:&lt;/STRONG&gt; In&amp;nbsp;SAP Datasphere all artifacts necessary for the analysis&amp;nbsp;are now automatically generated on top of the Data Product.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 5:&lt;/STRONG&gt; Visual representations of the insights are&amp;nbsp;automatically generated and made available in the intelligent application dashboard.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 6:&lt;/STRONG&gt; Integration of SAP Databricks&lt;BR /&gt;&lt;BR /&gt;It is possible to extend the results shown in the dashboard with machine learning and AI based Insights, to do so use the SAP business data cockpit to share data product with databricks,&lt;/P&gt;&lt;P&gt;In SAP Databricks AI and Machine learning techniques are used to generate the desired result, this results then shared back to the data model and extend your dashboard with AI and Machine learning insights.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="Intergation databricks.png" style="width: 575px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/336439i264D112E2B941334/image-dimensions/575x324/is-moderation-mode/true?v=v2" width="575" height="324" role="button" title="Intergation databricks.png" alt="Intergation databricks.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Scenario 2:&amp;nbsp;&amp;nbsp;:&amp;nbsp;Integration of SAP BW &amp;amp; SAP Analytical Cloud&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 1:&lt;/STRONG&gt; The first step is to identify the right intelligent application for your business scenario, In SAP BDC cockpit search for available intelligent application and install the one that meets your requirement.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 2: &lt;/STRONG&gt;When intelligent application installation is initiated&amp;nbsp;the relevant business data that has already been bundled&amp;nbsp;into a package in your business application is accessed and&amp;nbsp;replicated to the Foundation Services of SAP Business Data Cloud.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 3:&lt;/STRONG&gt;&amp;nbsp;Inside the Foundation Services, the replicated business&amp;nbsp;data is harmonized with other data packages from different&amp;nbsp;business applications such as SAP SuccessFactors the data&amp;nbsp;is transformed and enriched to become a data product.&amp;nbsp;The data is now ready to be used as the basis for analysis.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 4:&lt;/STRONG&gt; In&amp;nbsp;SAP Datasphere all artifacts necessary for the analysis&amp;nbsp;are now automatically generated on top of the Data Product.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 5:&amp;nbsp;&lt;/STRONG&gt;This is where the integration of your&amp;nbsp;third-party data comes into play.&amp;nbsp;The advanced integration and data modelling capabilities of SAP&amp;nbsp;Datasphere allow you to integrate your third-party data into the out-of-the-box data model.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 6:&lt;/STRONG&gt; Integration of SAP BW&lt;/P&gt;&lt;P&gt;It is also possible to integrate your&amp;nbsp;existing objects from SAP BW/4HANA into SAP Business Data Cloud.&amp;nbsp;With a dedicated data provisioning tool, you can&amp;nbsp;onboard your SAP BW objects into SAP Datasphere&amp;nbsp;and combine them with the existing data model.&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;Step 7:&lt;/STRONG&gt; Integration of SAP Analytical Cloud&amp;nbsp;&lt;/P&gt;&lt;P&gt;SAP Business Data Cloud provides a wide range of consumption&amp;nbsp;options on top of your data model. Use the AI-based Just Ask feature in SAP Analytics Cloud.&amp;nbsp;The power of its natural language processing allows you to request information from your data&amp;nbsp;model simply by typing a question about your data. Answers are instantly displayed in the most appropriate format.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="Integration BW &amp;amp; SAC.png" style="width: 518px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/336441i2AC8460F6CF8FEE3/image-dimensions/518x301/is-moderation-mode/true?v=v2" width="518" height="301" role="button" title="Integration BW &amp;amp; SAC.png" alt="Integration BW &amp;amp; SAC.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Role of Source system &amp;amp; Data product in BDC:&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="Data product.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/336442iB6AF5920AD8A607F/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Data product.png" alt="Data product.png" /&gt;&lt;/span&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Role of Datasphere in BDC:&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="Datasphere.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/336445i090B4C7F2964E626/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Datasphere.png" alt="Datasphere.png" /&gt;&lt;/span&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&amp;nbsp;&lt;STRONG&gt;Role of Databricks in BDC:&lt;BR /&gt;&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="Databricks.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/336446i11C5DC952AAAFE08/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Databricks.png" alt="Databricks.png" /&gt;&lt;/span&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;STRONG&gt;Role of BW in BDC:&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="BW.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/336447iCC1DAAA991EE2D64/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="BW.png" alt="BW.png" /&gt;&lt;/span&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;STRONG&gt;Role of SAC in BDC:&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&lt;SPAN&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="SAC.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/336448iB09BACB8E5911F1C/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="SAC.png" alt="SAC.png" /&gt;&lt;/span&gt;&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;STRONG&gt;Role of Intelligent Application Dashboard &amp;amp; Joule in BDC:&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="Joule.jpg" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/336449iB3F60AC97EBF31E1/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Joule.jpg" alt="Joule.jpg" /&gt;&lt;/span&gt;&lt;/STRONG&gt;&amp;nbsp;&lt;BR /&gt;&lt;STRONG&gt;Use Case of Business Data Cloud:&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;SAP Business Data Cloud can handle many use-cases including:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Data Integration &amp;amp; Modelling&amp;nbsp;&lt;/LI&gt;&lt;LI&gt;Data Governance and Compliance&amp;nbsp;&lt;/LI&gt;&lt;LI&gt;Development of AI and machine learning models&lt;/LI&gt;&lt;LI&gt;Data Warehousing&lt;/LI&gt;&lt;LI&gt;Planning and BI&amp;nbsp;&lt;/LI&gt;&lt;LI&gt;Development of intelligent applications to support all business processes.&lt;/LI&gt;&lt;LI&gt;End-to-end data lifecycle management by SAP&amp;nbsp;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;SAP Business Data Cloud reaches all personas of the organization.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Conclusion:&amp;nbsp;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Data from all SAP applications is combined and made available to the businesses under a single domain model.&amp;nbsp;With SAP Business Data Cloud, SAP has reached the next level of data and analytics evolution to tackle challenges and&amp;nbsp;enables businesses to seamlessly integrate, harmonize, and analyse data from multiple sources, transforming raw data into valuable insights with advanced AI and analytics.&amp;nbsp;&lt;/P&gt;&lt;P&gt;By adopting the latest 2025 features, organizations can accelerate analytics, improve decision-making, and prepare for AI-driven innovation.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Reference:&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Learning Journey:&amp;nbsp;&lt;A href="https://learning.sap.com/learning-journeys/exploring-sap-business-data-cloud" target="_blank" rel="noopener noreferrer"&gt;https://learning.sap.com/learning-journeys/exploring-sap-business-data-cloud&lt;/A&gt;&lt;BR /&gt;Live Session :&amp;nbsp;&lt;A href="https://learning.sap.com/live-sessions/overview-of-sap-business-data-cloud" target="_blank" rel="noopener noreferrer"&gt;https://learning.sap.com/live-sessions/overview-of-sap-business-data-cloud&lt;/A&gt;&lt;BR /&gt;SAP Help Document :&amp;nbsp;&lt;A href="https://help.sap.com/docs/business-data-cloud" target="_blank" rel="noopener noreferrer"&gt;https://help.sap.com/docs/business-data-cloud&lt;/A&gt;&lt;BR /&gt;GenAI Assistance : Assistance in&amp;nbsp;drafting the structure&amp;nbsp;of blog.&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/evolution-of-data-and-analytics-with-sap-business-data-cloud/ba-p/14260476"/>
    <published>2025-11-06T08:37:22.657000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/integration-blog-posts/data-space-integration-package/ba-p/14262165</id>
    <title>Data Space Integration Package</title>
    <updated>2025-11-06T14:27:22.842000+01:00</updated>
    <author>
      <name>Monisha_Sharma</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/2071981</uri>
    </author>
    <content>&lt;P&gt;&lt;SPAN&gt;&lt;STRONG&gt;Exciting News: Introducing the new data space integration package!&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;We are pleased to announce the successful release of our new add-on SKU:&amp;nbsp;&lt;STRONG&gt;data space integration package&lt;/STRONG&gt;! Officially available on the price list as of October 20th, this milestone reflects our commitment to advancing data space adoption and delivering greater value to our customers.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;What does it include?&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;The &lt;STRONG&gt;data space integration package&lt;/STRONG&gt; is designed to enable organizations to securely and efficiently connect supply chain partners within data spaces so that they can exchange data in a secure and sovereign way – a key requirement for modern digital supply chains. It contains two powerful components:&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;SPAN&gt;data space integration, &lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;a connector that facilitates smooth, secure and self-sovereign data exchange in data spaces.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;SPAN&gt;decentralized identity verification, &lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;a wallet that allows businesses to efficiently share trustworthy, verifiable data.&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Highlights:&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;SPAN&gt;Key application areas&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;: Manufacturing-X, Catena-X, Factory-X, Semiconductor-X, Chem-X and supply chain networks in other data spaces.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;SPAN&gt;Competitive Pricing&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;:&lt;STRONG&gt;&amp;nbsp;&lt;/STRONG&gt;The pricing has been carefully structured to ensure competitiveness, while maintaining SAP’s high standard of quality. This is initially available exclusively as a single add-on to the Integration Suite Starter Edition, with planned expansion to multi-connector support for all Integration Suite editions starting Q2 2026 (based on our product roadmap).&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;SPAN&gt;Benefits&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;: The package facilitates data-driven collaboration to optimize production, enhance supply chain visibility and drive innovation, and promotes interoperability amongst partner applications.&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Why Is This Important?&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;The Data Space Integration Package represents SAP’s ongoing dedication to simplifying data collaboration and helping customers unlock new business models. With global supply chains growing more complex, establishing secure, sovereign, and verifiable data exchange is more critical than ever. &lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Looking Ahead&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;We are excited to see how our customers leverage the &lt;STRONG&gt;data space integration package&lt;/STRONG&gt; to streamline data exchange and reshape secure collaboration across their partner networks.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;For more information about pricing and availability, or to learn how you can get started,&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;A href="mailto:Manufacturing-X@sap.com" target="_blank" rel="nofollow noopener noreferrer"&gt;&lt;STRONG&gt;contact&lt;/STRONG&gt; &lt;STRONG&gt;us&lt;/STRONG&gt;&lt;/A&gt;.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;Stay tuned for more updates on SAP Integration Suite’s data space capabilities!&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/integration-blog-posts/data-space-integration-package/ba-p/14262165"/>
    <published>2025-11-06T14:27:22.842000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/unleashing-the-power-of-trusted-data-sap-2025-teched-announces-enhanced/ba-p/14263266</id>
    <title>Unleashing the Power of Trusted Data: SAP 2025 TechEd Announces Enhanced Data Product Capabilities</title>
    <updated>2025-11-07T19:15:55.796000+01:00</updated>
    <author>
      <name>Corrie</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/164966</uri>
    </author>
    <content>&lt;P&gt;&lt;A href="https://www.sap.com/events/teched/virtual.html" target="_blank" rel="noopener noreferrer"&gt;SAP TechEd 2025&lt;/A&gt; has unveiled advancements in &lt;A href="https://www.sap.com/products/data-cloud.html" target="_blank" rel="noopener noreferrer"&gt;SAP Business Data Cloud&lt;/A&gt;, and they're set to transform how data stewards and data owners manage and utilize data. These innovations empower developers and data teams to create, share, and manage custom data products, propelling analytics, AI, and operational use cases to new heights.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Data Products: The Future of Business Data&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Data products from SAP are designed to convert curated and certified data, metadata, and business semantics from SAP sources into reusable, governed assets. They are fundamental components of SAP Business Data Cloud and its suite of intelligent applications, offering a seamless blend of governance and utility.&lt;/P&gt;&lt;P&gt;SAP's latest enhancements enable the creation of custom data products - across SAP and third-party sources - that can be shared across various analytics, AI, and operational applications. This interoperability ensures that organizations can leverage their data more effectively, driving smarter decisions and optimizing workflows.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Introducing the Data Product Studio&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;One of the most exciting announcements from SAP TechEd is the new Data Product Studio (general availability planned for H1 2026). This cutting-edge capability within SAP Business Data Cloud provides developers and data teams with a comprehensive visual workspace to model, configure, and manage custom data products.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="Data Product Studio.jpg" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/337419iF67ABBC26C8C106C/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Data Product Studio.jpg" alt="Data Product Studio.jpg" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;SPAN&gt;Here's what the Data Product Studio will bring to the table:&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Visual Configuration&lt;/STRONG&gt;: Craft custom data products using robust metadata from the data catalog.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Schema Definition&lt;/STRONG&gt;: Define schemas, lineage, and transformation logic to ensure data integrity.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Versioning and Governance&lt;/STRONG&gt;: Manage versioning and enforce governance standards through a shared domain catalog, ensuring data reliability and compliance.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;SQL-Based Transformations&lt;/STRONG&gt;: Utilize familiar SQL-based transformations to enrich and customize data products effectively.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Full Lifecycle Management&lt;/STRONG&gt;: Oversee the entire lifecycle of data products within a structured environment, maintaining governance and performance.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Why This Matters to Data Stewards and Owners&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;For data stewards and owners, these advancements mean enhanced control over data assets, ensuring they are trustworthy, scalable, and reusable across the organization. The Data Product Studio accelerates the process of data modeling and transformation, making it simpler to align data management with business objectives.&lt;/P&gt;&lt;P&gt;SAP's latest innovations in SAP Business Data Cloud and the introduction of the Data Product Studio mark a significant leap forward in data management. By providing a structured and governance-focused environment, SAP enables businesses to unlock the full potential of their data. Stay ahead of the curve by embracing these advancements and elevating your data stewardship practices.&lt;/P&gt;&lt;P&gt;Looking to learn more about SAP Business Data Cloud...&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Visit &lt;A href="https://www.sap.com/products/data-cloud.html" target="_blank" rel="noopener noreferrer"&gt;SAP Business Data Cloud&lt;/A&gt; on sap.com&lt;/LI&gt;&lt;LI&gt;Watch SAP Virtual TechEd Sessions replays:&lt;UL&gt;&lt;LI&gt;&lt;A href="https://www.sap.com/events/teched/virtual/flow/sap/tev25/catalog-virtual/page/catalog/session/1751961499331001rRV1" target="_blank" rel="noopener noreferrer"&gt;SAP Business Data Cloud: Strategy, architecture, and vision (ST107v)&lt;/A&gt;&amp;nbsp;&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://www.sap.com/events/teched/virtual/flow/sap/tev25/catalog-virtual/page/catalog/session/1751961468799001rUtP" target="_blank" rel="noopener noreferrer"&gt;SAP Business Data Cloud solution road map (DA806v)&lt;/A&gt;&amp;nbsp;&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;LI&gt;View the &lt;A href="https://www.sap.com/topics/innovation-guide/h2#sap-business-data-cloud-enhancements-advance-access-to-trusted-semantically-rich-data-products" target="_blank" rel="noopener noreferrer"&gt;H2 2025 SAP Innovation Guide&lt;/A&gt;&lt;/LI&gt;&lt;/UL&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/unleashing-the-power-of-trusted-data-sap-2025-teched-announces-enhanced/ba-p/14263266"/>
    <published>2025-11-07T19:15:55.796000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/source-data-extraction-for-sap-signavio-process-intelligence-using-sap/ba-p/14030384</id>
    <title>Source Data Extraction for SAP Signavio Process Intelligence using SAP Datasphere Replication Flows</title>
    <updated>2025-11-11T05:11:34.143000+01:00</updated>
    <author>
      <name>andreas_krompholz</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/229511</uri>
    </author>
    <content>&lt;P&gt;&lt;SPAN&gt;SAP Datasphere has recently introduced the new connection type "SAP Signavio," enabling Replication Flows to replicate data directly to SAP Signavio Process Intelligence. This allows CDS Views and ABAP tables from SAP ERP systems to be replicated in a manner similar to any other replication use case in Datasphere. &lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;On the SAP Signavio Process Intelligence side, the data will be pushed by the replication flow into the object store of the customer's Signavio workspace. A replication flow can then be mapped to a source data, providing the data from the corresponding ABAP tables and CDS Views to the Signavio ETL Pipeline to generate the process mining event log.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="High-Level component overview" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/335320iB3F835B8504C21CE/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Replication Flow - Blog.png" alt="High-Level component overview" /&gt;&lt;span class="lia-inline-image-caption" onclick="event.preventDefault();"&gt;High-Level component overview&lt;/span&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;The diagram illustrates the complete data flow:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Left&lt;/STRONG&gt;: Source systems (S/4HANA Public Cloud for CDS Views; S/4HANA/ECC with SLT for ABAP tables)&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Center&lt;/STRONG&gt;:&amp;nbsp;SAP Datasphere orchestrates the replication flow&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Right:&lt;/STRONG&gt;&amp;nbsp;SAP Signavio Process Intelligence receives data as Source Data objects&lt;/LI&gt;&lt;LI&gt;The dashed vertical line represents the boundary between customer landscape and internet connectivity via SAP Cloud Connector&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;We demonstrate now how to create a replication flow for:&lt;BR /&gt;(a) the customer object (CDS View i_customer) in S/4HANA Public Cloud Edition.&lt;BR /&gt;(b) the sales order object (ABAP tables VBAK and VBAP) in S/4HANA or SAP ECC&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Prerequisite:&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;We have a user and space in the SAP Datasphere service on BTP and have the authorization to create a new connection and to create and execute replication flows. Note that we use Replication in the pass-through option and don't require a SAP HANA as staging area within SAP Datasphere (see &lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/replication-flow-blog-series-part-1-overview/ba-p/13581472" target="_self"&gt;Replication Flow Blog series&lt;/A&gt;).&lt;BR /&gt;We have noted down the Datasphere Tenant ID (System =&amp;gt; About =&amp;gt; Tenant).&lt;/LI&gt;&lt;LI&gt;For the replication of CDS views we have configured the integration scenario SAP_COM_0532 in our S/4 HANA Public Cloud Edition&amp;nbsp;(see blog &lt;A href="https://community.sap.com/t5/enterprise-architecture-blog-posts/s-4hana-public-cloud-integration-with-sap-datasphere/ba-p/13699159" target="_self"&gt;S/4HANA Public Cloud Integration with SAP Dataspehere&lt;/A&gt;&amp;nbsp;)&lt;/LI&gt;&lt;LI&gt;For the replication of ABAP tables we have configured the SAP Landscape Transformation component (SLT) in our S/4HANA on-premise or use a standalone SLT system connected to our S/4 HANA system. We have enabled connectivity to the ABAP landscape via SAP Cloud Connector to SAP Datasphere (see &lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/replicating-table-data-from-an-sap-ecc-system-with-sap-datasphere-using/ba-p/13579998" target="_self"&gt;Replicating table data from an SAP ECC system with SAP Datasphere using Replication Flows&lt;/A&gt;)&lt;/LI&gt;&lt;LI&gt;In SAP Signavio we have access to a workspace subscribed to Process Intelligence and have the authorization to perform Process Data Pipeline steps&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Connectivity Handshake Signavio &amp;lt;-&amp;gt; Datasphere:&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;To establish a secure connection between SAP Datasphere and SAP Signavio, an OAuth Flow is required.&amp;nbsp; To do so, we start with creating an OAuth Client in our Datasphere tenant. Navigate to System =&amp;gt; Administration and choose tab App Integration. When clicking on "Add an OAuth Client" you create your OAuth Client for Signavio. Give it a name, choose the purpose "Technical User", enter a name for the technical user to be created and assign the appropriate roles (refer to the &lt;A href="https://help.sap.com/docs/SAP_DATASPHERE/9f804b8efa8043539289f42f372c4862/9bde7712a1ef47a18a292484284e2c0a.html?locale=en-US" target="_self" rel="noopener noreferrer"&gt;documentation&lt;/A&gt;&amp;nbsp;to assign the requires scoped integrator role).&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_0-1762446651275.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/336971iDAB551513CDE93C0/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_0-1762446651275.png" alt="andreas_krompholz_0-1762446651275.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Once the system has created the OAuth Client, it provides you with the OAuth credentials. Make sure to copy the credentials and note it down, you need it in the next step.&lt;/P&gt;&lt;P&gt;We now switch to Signavio and use the created OAuth Client to establish a secure connection. Create a "SAP Datasphere Replication Flow" connection in Signavio Process Intelligence. There is also a "SAP Datasphere" connection which relates to read the data from the SAP HANA instance in SAP Datasphere. Mind, this is not the integration flow we are looking into here and make sure to select the one with replication flow.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-left" image-alt="andreas_krompholz_0-1740759056297.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/231741i8D0FF7E29E372499/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_0-1740759056297.png" alt="andreas_krompholz_0-1740759056297.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;In the next screen we enter the OAuth Client Credentials we have noted down earlier:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_0-1762447413755.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/336984i456582D73EBB19DA/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_0-1762447413755.png" alt="andreas_krompholz_0-1762447413755.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;OAuth Client ID: Client ID you have noted down in the previous step&lt;/LI&gt;&lt;LI&gt;OAuth Client Secret: secret you have noted down in the previous step&lt;/LI&gt;&lt;LI&gt;Service Root:&amp;nbsp;&lt;SPAN&gt;URL root of the&amp;nbsp;Datasphere&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;App Integration&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;page where you create the OAuth client&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;OAuth Access Token URL: Token URL you find in the overview of the OAuth Clients in SAP Datasphere (ends with&amp;nbsp;&lt;SPAN&gt;/oauth/token&lt;/SPAN&gt;)&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;After we have created the connection in Signavio we have now established the OAuth connectivity handshake between Signavio and Datasphere and can work on a secure connection.&lt;/P&gt;&lt;P&gt;Let's now create the corresponding Signavio connection in Datasphere.&lt;/P&gt;&lt;P&gt;We logon to our SAP Datasphere instance and select our space where we create the connections and replication flows.&lt;BR /&gt;In the left bar menu we choose Connections and create a new connection of type Signavio.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_3-1740759811819.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/231753i938882B4074FE07C/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_3-1740759811819.png" alt="andreas_krompholz_3-1740759811819.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;In the following screen we enter the connection URL of the respective Signavio landscape. In our case it is EU which is the URL &lt;A href="https://api-mtls.eu.signavio.cloud.sap" target="_blank" rel="noopener nofollow noreferrer"&gt;https://api-mtls.eu.signavio.cloud.sap&lt;/A&gt;. You find the right API-Gateway endpoints in the &lt;A href="https://help.sap.com/docs/signavio-process-intelligence/user-guide/regions-ip-addresses-and-urls?locale=en-US" target="_self" rel="noopener noreferrer"&gt;SAP Signavio documentation&lt;/A&gt; for regions, IP addresses and URLs.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_0-1740760287909.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/231755i256CB245980B4570/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_0-1740760287909.png" alt="andreas_krompholz_0-1740760287909.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;After entering a name for the connection we can save and validate the connection:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_1-1740760449740.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/231756iF413131A645CD46B/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_1-1740760449740.png" alt="andreas_krompholz_1-1740760449740.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;The Datasphere tenant is automatically mapped to the corresponding Signavio connection where we had entered the respective Datasphere tenant ID. We can not map this Datasphere tenant to another Signavio workspace unless we delete the connection in Signavio.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Replicate customer information&lt;/STRONG&gt; (CDS View i_customer)&lt;/P&gt;&lt;P&gt;After establishing the Signavio connection we create a connection to the SAP S/4 HANA system to replicate the CDS View i_customer. We create a new connection of type S/4HANA Public Cloud:&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_1-1740761242263.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/231758i0E1B58FFC8FD4492/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_1-1740761242263.png" alt="andreas_krompholz_1-1740761242263.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;We use certificate or user/password from the Communication Arrangement for &lt;A href="https://help.sap.com/docs/SAP_DATASPHERE/9f804b8efa8043539289f42f372c4862/abb159e027184c98a54fc1b2a88dd3f5.html?locale=en-US" target="_self" rel="noopener noreferrer"&gt;Communication Scenario 0532&lt;/A&gt;.&lt;/P&gt;&lt;P&gt;Once created and validated we can continue to configure our Replication Flow for the CDS View i_customer. Choose Data Builder and new Replication Flow:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_2-1740761673688.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/231759i06F3BECCA7EDD2EE/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_2-1740761673688.png" alt="andreas_krompholz_2-1740761673688.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;We perform the following steps to configure the source of Replication Flow:&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;Select your S/4HANA Public Cloud as source connection&lt;/LI&gt;&lt;LI&gt;Choose CDS View Extraction as source container&lt;/LI&gt;&lt;LI&gt;Select CDS View I_CUSTOMER&amp;nbsp;&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_3-1740761896137.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/231760i88F0B54BFE457BF3/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_3-1740761896137.png" alt="andreas_krompholz_3-1740761896137.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;We configure now:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Load Type: Initial and Delta&lt;/LI&gt;&lt;LI&gt;Our recently created Signavio Connection as target&lt;/LI&gt;&lt;LI&gt;Content-Type "Template Type" to allow ABAP Types transformation such as Date/Time in Signavio&lt;/LI&gt;&lt;LI&gt;the name of the Replication Flow&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_4-1740762172075.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/231761i4E5B4A341AF9AE8B/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_4-1740762172075.png" alt="andreas_krompholz_4-1740762172075.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Once we have saved and deployed the Replication Flow we run it. In the Integration monitor we check the status of the replication:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_0-1740762663108.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/231766iD91C25B5041F0B5C/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_0-1740762663108.png" alt="andreas_krompholz_0-1740762663108.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Once initial load is finished the delta handling will regularly replicate the changes to Signavio.&lt;/P&gt;&lt;P&gt;Let's now switch back to SAP Signavio Process Intelligence and create a corresponding source data.&amp;nbsp;&lt;BR /&gt;We crate a new source data for our new replication flow RF_Customer and assign the earlier created connection to it.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_0-1740999962178.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/232410i11B825636C4C7E94/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_0-1740999962178.png" alt="andreas_krompholz_0-1740999962178.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Once created we can sync the data which results in processing the data provided by the replication flow from SAP Datasphere. In particular, the initial load and possible deltas will be merged and we get the replica of the ABAP source data in Signavio. It may take some minutes depending on the size of the CDS views.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_1-1741000318768.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/232412i8A911F1A0B36748B/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_1-1741000318768.png" alt="andreas_krompholz_1-1741000318768.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Mind, that whenever the Replication Flows send delta information a new sync is required to update the source data.&lt;/P&gt;&lt;P&gt;To validate the replication or use the data we have to create a Process Data Pipeline and create a SQL script. We create a new Process Data Pipeline based on a blank template and choose our source data RF_Customer.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_2-1741000584977.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/232413iCA28CC4266E05985/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_2-1741000584977.png" alt="andreas_krompholz_2-1741000584977.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;We create a new Process Data Model with business object&amp;nbsp;&lt;EM&gt;Customer&lt;/EM&gt; and open the SQL Editor for a new event collector.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_0-1741008005521.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/232462i83C277FECFE2A490/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_0-1741008005521.png" alt="andreas_krompholz_0-1741008005521.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;We can now use the preview option to validate the extraction, e.g. showing Top100 entries, count the replicated entities, etc.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Replicate sales order information&lt;/STRONG&gt; (ABAP tables VBAK and VBAP)&lt;BR /&gt;In the previous chapter we have successfully replicated the data from a CDS View. If we have an ABAP table to be replicated we have to leverage the SAP Landscape Transformation Server (SLT) integration to Datasphere.&lt;/P&gt;&lt;P&gt;Assuming we have our SLT component configured in our ABAP system (or use a dedicated SLT server) we can create a new mass transfer ID in SLT. We create it using transaction code ltrc.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_0-1741008776509.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/232464i04903E679C524F30/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_0-1741008776509.png" alt="andreas_krompholz_0-1741008776509.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Click on Create Configuration and enter the following information in the subsequent screens:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Configuration Name and Description&lt;/LI&gt;&lt;LI&gt;RFC Destination and (optionally) Read from Single Client flag&lt;/LI&gt;&lt;LI&gt;SAP Data Intelligence (Replication Management Service) under Other in Target System&lt;/LI&gt;&lt;LI&gt;Job options (we may start with 1 job first)&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;We have finished our SLT preparation step. All following replication configurations (tables, fields, filters, etc.) will now be configured in the Replication Flow in Datasphere which will be then pushed down to your SLT configuration. Let's switch to our Datasphere tenant and create a new connection to our SLT instance.&amp;nbsp;&lt;/P&gt;&lt;P&gt;When creating a new connection choose SAP ABAP as system type:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Protocol: RFC&lt;/LI&gt;&lt;LI&gt;Connection Type: Application Server (or Message Server if you prefer)&lt;/LI&gt;&lt;LI&gt;Application Server, System Number and Client&lt;/LI&gt;&lt;LI&gt;Choose Cloud Connector to allow on-premise connectivity&lt;/LI&gt;&lt;LI&gt;Authentication Type: for user/password choose your ABAP user for SLT with appropriate roles&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;We can save and validate our new connection.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_0-1741106711528.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/232946i3603CE4C621771C7/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_0-1741106711528.png" alt="andreas_krompholz_0-1741106711528.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Once connection is established, we create a new Replication Flow for our ABAP table replication. Create the replication flow similar to the CDS View, but use SLT as source container:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Business Name: RF_SalesOrder&lt;/LI&gt;&lt;LI&gt;Source Connection: our SLT connection&lt;/LI&gt;&lt;LI&gt;Source Container: "SLT - SAP LT Replication Server"&lt;UL&gt;&lt;LI&gt;our mass transfer ID from the ABAP system&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;LI&gt;Source Objects: ABAP Tables VBAK and VBAP&amp;nbsp;&lt;/LI&gt;&lt;LI&gt;Load Type: Initial and Delta&lt;/LI&gt;&lt;LI&gt;Content Type: Template Type&lt;/LI&gt;&lt;LI&gt;(optional): add a projection to filter the data or reduce the number of fields&lt;/LI&gt;&lt;LI&gt;Target Connection: your Signavio connection (same as used for CDS View replication)&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_1-1741107242133.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/232948i9CD83E3232BB77F1/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_1-1741107242133.png" alt="andreas_krompholz_1-1741107242133.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Optionally you may add filters or field projections which will be pushed down to SLT and reduces the data volume to be transferred.&lt;/P&gt;&lt;P&gt;Save, Deploy and run the replication flow. To monitor the transfer you can choose Data Integration Monitor in Datasphere and the monitoring tool in SLT. Data will be gathered by SLT and Datasphere can then pull the data to Datasphere and directly forward it to Signavio.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_0-1741339126908.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/234351i2DB28E60CED87E64/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_0-1741339126908.png" alt="andreas_krompholz_0-1741339126908.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;You see the replication status of each table and some metrics on the replication run, e.g. 886.757 entries from VBAK had bee initially replicated in 5:12 minutes using 9 partitions, 3 delta updates have been already executed.&lt;/P&gt;&lt;P&gt;Once data is replicated the Replication flow will show status "Active (retrying objects)" as we have chosen Initial and Delta Mode. The Replication Flow continuously checks for updates in the ABAP Tables and replicate them to Signavio.&lt;/P&gt;&lt;P&gt;Let's switch to Signavio and create a corresponding source data for our replication flow for the sales order ABAP Table.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_0-1741185324748.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/233377iAE3A346494378A4C/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_0-1741185324748.png" alt="andreas_krompholz_0-1741185324748.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;We have selected the same Datasphere connection as we have used for the CDS Views, and the RF_SalesOrder replication flow.&lt;/P&gt;&lt;P&gt;In the following screen we sync the replication flow.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_1-1741339746747.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/234356i826AC3C32887A0F1/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_1-1741339746747.png" alt="andreas_krompholz_1-1741339746747.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Once the sync is being processed we can see the tables VBAK and VBAP alongside with the sync log.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_0-1741340270266.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/234361i77AA4CD483016306/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_0-1741340270266.png" alt="andreas_krompholz_0-1741340270266.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;With having the source data prepared for further processing we can now create a Process Data Pipeline to generate the process mining event log.&lt;/P&gt;&lt;P&gt;In our pipeline we crate a new business object SalesOrder and create the SQL scripts for case and event collectors. As an example we create the event collector "create sales order".&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="andreas_krompholz_1-1741340951319.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/234369i601C4E68CD53B10A/image-size/large?v=v2&amp;amp;px=999" role="button" title="andreas_krompholz_1-1741340951319.png" alt="andreas_krompholz_1-1741340951319.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;We've demonstrated how to leverage SAP Datasphere to replicate and synchronize data from source systems using Replication Flow. Give this method a try and share your results and experiences in the comments below!&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/source-data-extraction-for-sap-signavio-process-intelligence-using-sap/ba-p/14030384"/>
    <published>2025-11-11T05:11:34.143000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/artificial-intelligence-blogs-posts/understanding-shadow-ai-and-its-impact-on-data-security/ba-p/14245143</id>
    <title>Understanding Shadow AI and Its Impact on Data Security</title>
    <updated>2025-11-13T05:29:17.062000+01:00</updated>
    <author>
      <name>Nicolas_BacaStorni</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/2194642</uri>
    </author>
    <content>&lt;P&gt;&lt;SPAN&gt;Hi community! This is my second post here, and I wanted to share a topic that keeps coming up in conversations with many IT leaders lately: &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;Shadow AI.&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;We’ve all noticed how quickly AI tools have found their way into our daily routines. It often starts with small things: someone uses ChatGPT to summarize a meeting, another person drafts a report with Gemini, or a colleague asks an AI tool to polish a sales email. Most of the time, these actions come from curiosity or a genuine wish to save time and work better.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;But sometimes, we might be introducing risk when using these tools without IT oversight.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;A recent example that caught my attention came from the legal world. Some in-house lawyers at large companies &lt;/SPAN&gt;&lt;A href="https://www.corporatecomplianceinsights.com/shadow-ai-crisis-brewing-gc-office/" target="_blank" rel="noopener nofollow noreferrer"&gt;&lt;SPAN&gt;were using public AI tools&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt; to draft contracts or analyze documents. One case described a lawyer uploading confidential merger files into a personal account just to meet a deadline. It sounds extreme, but it happens more often than we think...&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Of course, people don’t intend to break the rules or do something dangerous. They just want to get things done faster. Still, when you use an AI tool without reading its data policies first, you might be exposing sensitive information or allowing it to be used for unintended purposes. Once that data leaves the company’s ecosystem, no one can fully control where it goes or how it’s handled.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;This is what many are calling &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;Shadow AI.&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; It’s the evolution of &lt;/SPAN&gt;&lt;A href="https://inclusioncloud.com/insights/blog/shadow-it-innovation-or-risk/" target="_blank" rel="noopener nofollow noreferrer"&gt;&lt;SPAN&gt;Shadow IT&lt;/SPAN&gt;&lt;/A&gt;&lt;STRONG&gt;&lt;SPAN&gt;.&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; Years ago, employees installed their own software or used unapproved SaaS apps to get their work done. Now, it’s large language models quietly spreading across teams without IT even knowing.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Why does it happen? Usually, as I mentioned, it’s simply because people want to be more efficient. When official tools are too limited or slow, they look for alternatives that help them.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;That’s why I believe the solution isn’t to block AI, but to &lt;/SPAN&gt;&lt;STRONG&gt;&lt;SPAN&gt;guide its use.&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; When enterprises make AI available inside secure environments like SAP, people can use it safely and productively at the same time.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Imagine a legal department using AI built into their SAP environment to review contracts or summarize long documents. The team saves time, but everything stays within the company’s governance and data protection rules. The same idea applies to finance teams running forecasts or HR teams preparing reports.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;AI can help... as long as it happens in a trusted space.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Shadow AI isn’t about people doing something wrong. It’s about people trying to do their best with the tools they have. The challenge for leaders is to give them the right environment to expermient.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;And that’s the beauty of it: SAP gives you a safe space to experiment with AI through its own suite of tools - Joule, BTP, and Datasphere.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Thanks for reading! I’d love to know if you’ve seen similar situations in your teams, or how your organization is balancing innovation and data protection.&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/artificial-intelligence-blogs-posts/understanding-shadow-ai-and-its-impact-on-data-security/ba-p/14245143"/>
    <published>2025-11-13T05:29:17.062000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/sap-business-data-cloud-bdc-a-beginner-s-guide-part-1/ba-p/14263260</id>
    <title>SAP Business Data Cloud (BDC): A Beginner's Guide - Part 1</title>
    <updated>2025-11-13T06:48:36.414000+01:00</updated>
    <author>
      <name>Akhila_Ajith_Kumar_Unnithan</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/2057415</uri>
    </author>
    <content>&lt;P&gt;&lt;STRONG&gt;Introduction&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Let's understand what is SAP Business Data Cloud (BDC). SAP Business Data Cloud (BDC) is a unified data platform which means a single environment that harmonizes information from all SAP and non-SAP sources into a trusted semantic layer. BDC can be the organization's single source of truth which provides a consistent, governed layer of data that everyone&amp;nbsp;can use, whether it’s for analytics, planning, or building AI Applications.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Akhila_AjithKumarUnnithan79_0-1762536460599.png" style="width: 939px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/337417i61C3ADFCC3682B71/image-dimensions/939x372/is-moderation-mode/true?v=v2" width="939" height="372" role="button" title="Akhila_AjithKumarUnnithan79_0-1762536460599.png" alt="Akhila_AjithKumarUnnithan79_0-1762536460599.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;But the bigger topic is why do we even need something like SAP Business Data Cloud?&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Why do we need Business Data Cloud?&amp;nbsp;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Today every organization is talking about being ‘data-driven.’ We all want to make decisions based on real insights rather than gut feeling.&amp;nbsp; But in reality, its not that easy. the truth is data is everywhere. But it's rarely in one place or&amp;nbsp;in one consistent format. Businesses have information scattered across multiple systems . Like some in SAP applications like S/4HANA or SuccessFactors, some in non-SAP platforms, spreadsheets, or even external sources like partners and IoT devices. So when companies try to bring that data together for analytics or AI, they face a few very real challenges.&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;&amp;nbsp;How do you build a single, complete data foundation that supports modern AI and machine learning models?&lt;/LI&gt;&lt;LI&gt;&amp;nbsp;How do you trust your data? how do you ensure quality, consistency, and integrity so that the decisions you make are actually reliable?&lt;/LI&gt;&lt;LI&gt;&amp;nbsp;How do you extract real business value from this massive, mixed pool of structured and unstructured data?&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;SAP recognized that these are the exact pain points stopping companies from truly using data to their advantage. And that’s what led to the creation of SAP Business Data Cloud (BDC).&lt;/P&gt;&lt;P&gt;One of the major innovations behind BDC is how it eliminates data fragmentation. Instead of moving data between systems and losing context along the way, BDC integrates data in real time while preserving its business meaning. For example, if the same business entity, say a customer exists in multiple SAP systems, BDC brings together under a unified definition using the One Domain Model. So business users don't have to worry about where the data came from or how it's structured. They simply work with consistent, harmonized business entities that are ready for analytics, reporting, or Artificial Intelligence (AI).&amp;nbsp;&lt;/P&gt;&lt;P&gt;The best part is SAP fully manages the entire lifecycle. BDC is delivered as a Software-as-a-Service solution built on the SAP Business Technology Platform (BTP).&amp;nbsp; Because of that all the integration, replication, and governance are handled by SAP.&amp;nbsp;In the past, customers had to manually extract and prepare SAP data before it was usable for analytics. That process was slow, expensive, and error-prone. With BDC, that’s all automated&amp;nbsp;so organizations can focus their time and resources on building innovative AI models, automating insights, or improving business performance instead of just wrangling data.&lt;/P&gt;&lt;P&gt;Let's take a practical scenario,&amp;nbsp;imagine a large enterprise with data coming from ERP, HR, procurement, and expense systems. Traditionally, every system had its own data units, and teams had to spend weeks just preparing the data before running any kind of analysis.&amp;nbsp;Now, with SAP Business Data Cloud, all of that data&amp;nbsp;whether it’s from SAP S/4HANA, SuccessFactors, Ariba, Concur, or even non-SAP systems can be harmonized and made available in one place instantly.&amp;nbsp;That gives the business a 360-degree view across operations, employees, customers, and suppliers. The platform supports a wide range of use cases from data warehousing and BI, to AI and machine learning, planning, governance, and even intelligent application development.&amp;nbsp;Basically, it’s an end-to-end solution that connects data and intelligence across the entire organization. The real value, however, is in the trust and governance that come built-in. Because BDC leverages SAP’s domain expertise and semantic understanding of business data, it ensures that what you see in your reports and AI models actually reflects reality&amp;nbsp;not inconsistent or duplicate information.&lt;/P&gt;&lt;P&gt;So to sum it up, SAP Business Data Cloud (BDC)&amp;nbsp;transforms how organizations manage and use their data.&amp;nbsp;It addresses the three biggest challenges of the data-driven world:&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;&amp;nbsp;Building a single, reliable foundation for AI and analytics&lt;/LI&gt;&lt;LI&gt;&amp;nbsp;Ensuring data trust and quality&lt;/LI&gt;&lt;LI&gt;&amp;nbsp;Unlocking the full value of structured and unstructured data across the enterprise&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;It's like a bridge that connects all the data sources, both internal and external into one governed, business-ready environment.&amp;nbsp;In other words, it takes us from data chaos to clarity. It helps organizations move beyond just having data, to truly using it to drive growth, innovation, and intelligent decision-making.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Key Components of SAP Business Data Cloud&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Let's see how BDC is built and how&amp;nbsp;the key components that make it work and how they fit together.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Akhila_Ajith_Kumar_Unnithan_0-1762942944935.png" style="width: 861px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/339295iFEAF41681ABC9229/image-dimensions/861x414/is-moderation-mode/true?v=v2" width="861" height="414" role="button" title="Akhila_Ajith_Kumar_Unnithan_0-1762942944935.png" alt="Akhila_Ajith_Kumar_Unnithan_0-1762942944935.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;One of the most interesting things about SAP Business Data Cloud (BDC) is that it doesn't start from scratch. It combines some of SAP's most proven existing technologies with brand new innovations designed for the data and AI era.&lt;/P&gt;&lt;P&gt;Let's understand the existing SAP solutions that form the foundation.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;EM&gt;&lt;STRONG&gt;SAP Datasphere&lt;/STRONG&gt;&lt;STRONG&gt;&amp;nbsp;-&lt;/STRONG&gt;&lt;/EM&gt; This is one of the core data integration and modeling layer of BDC.&amp;nbsp; It is like a brain that organises, connects, and models all the business data.&amp;nbsp;Datasphere allows you to define data models, map relationships, and create semantic layers that business and technical users can both understand. It’s the part of the platform that ensures data is structured, contextualized, and ready to use for analytics and AI.&lt;/P&gt;&lt;P&gt;&lt;EM&gt;&lt;STRONG&gt;SAP Analytics Cloud -&amp;nbsp;&lt;/STRONG&gt;&lt;/EM&gt;This is the analysis and visualization layer. It’s where users actually interact with data through dashboards, reports, and planning models. SAC is tightly integrated with BDC.&amp;nbsp; It provides both out-of-the-box analytical stories and flexible tools for ad-hoc analysis. It even supports AI-driven features like predictive insights and chat-based data exploration, helping users move seamlessly from insight to action.&lt;/P&gt;&lt;P&gt;&lt;EM&gt;&lt;STRONG&gt;SAP BW or BW/4HANA -&amp;nbsp;&amp;nbsp;&lt;/STRONG&gt;&lt;/EM&gt;Many customers already have large investments in BW, and SAP recognized that value. So instead of replacing it, BDC integrates it. BW data objects can be technically onboarded into the Business Data Cloud, as data products and reuse them in new AI and analytics scenarios. SAP plans to support BW until 2030, but the idea is to give customers a smooth modernization path toward BW/4HANA and cloud-native analytics.&lt;EM&gt;&lt;STRONG&gt;&lt;BR /&gt;&lt;/STRONG&gt;&lt;/EM&gt;&lt;/P&gt;&lt;P&gt;Now let's understand the new components introduced specifically for SAP Business Data Cloud.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;EM&gt;&lt;STRONG&gt;SAP Databricks -&amp;nbsp;&lt;/STRONG&gt;&lt;/EM&gt;This is one of the biggest innovations. It brings advanced AI and machine learning capabilities directly into the SAP ecosystem. Databricks has long been a leader in large-scale data processing and pro-code data science, and SAP has now embedded a tailored version inside BDC.&lt;BR /&gt;This integration allows data scientists to access and extend data products directly. For example, they can take a data product from SAP Business Data Cloud, enrich it with machine learning models in Databricks, and then push the enhanced insights right back into the BDC catalog or vice versa.&lt;/P&gt;&lt;P&gt;&lt;EM&gt;&lt;STRONG&gt;SAP Business Data Cloud Foundation Services -&amp;nbsp;&lt;/STRONG&gt;&lt;/EM&gt;This layer manages the data lifecycle from ingestion and harmonization to transformation and enrichment. When data comes from your source systems (for example, SAP S/4HANA Cloud or SAP SuccessFactors) the Foundation Services replicate it into the BDC environment, which runs on SAP HANA Cloud and hyper-scaler infrastructure. From there, it’s automatically cleansed, harmonized, and turned into something called a data product which a packaged, ready-to-use set of business data.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Conclusion&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Now we have covered the Introduction, Why do we need Business Data Cloud, and Key Components of SAP Business Data Cloud. I hope you have got a basic understanding about SAP Business Data Cloud. This is the first part of the blog series.&lt;/P&gt;&lt;P&gt;In the upcoming parts, we will cover the architecture of SAP Business Data Cloud, and understand the main components of BDC in detail.&lt;/P&gt;&lt;P&gt;If you have any questions or queries, please post them as comments. I will try to answer the maximum as per my knowledge.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/sap-business-data-cloud-bdc-a-beginner-s-guide-part-1/ba-p/14263260"/>
    <published>2025-11-13T06:48:36.414000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/teched-2025-in-berlin-recap-on-planning-in-sap-business-data-cloud/ba-p/14268314</id>
    <title>TechEd 2025 in Berlin - Recap on planning in SAP Business Data Cloud</title>
    <updated>2025-11-13T18:00:00.026000+01:00</updated>
    <author>
      <name>Max_Gander</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/14553</uri>
    </author>
    <content>&lt;P&gt;It has already been a week since I boarded my train home from Berlin after an exciting TechEd 2025.&amp;nbsp;&lt;SPAN&gt;Coming back from such an event always feels a bit like returning to reality — all the nicer to look back on the sessions I had the honor of hosting and the great conversations with customers and partners in the meeting center, at the demo booth, and beyond.&lt;/SPAN&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;In case you missed TechEd, don't worry. We have got you covered.&amp;nbsp;&lt;/P&gt;&lt;H5 id="toc-hId--2142952318"&gt;Virtual Sessions:&lt;/H5&gt;&lt;P&gt;Together with&amp;nbsp;&lt;a href="https://community.sap.com/t5/user/viewprofilepage/user-id/183681"&gt;@jie_deng&lt;/a&gt;&amp;nbsp;&amp;nbsp;and&amp;nbsp;&lt;a href="https://community.sap.com/t5/user/viewprofilepage/user-id/72985"&gt;@matthiaskraemer&lt;/a&gt;, I hosted session &lt;EM&gt;DA207 -&amp;nbsp;Planning and analytics: Innovations powering SAP Business Data Cloud&lt;/EM&gt; which is available on-demand on the &lt;A href="https://www.sap.com/events/teched/virtual/flow/sap/tev25/catalog-virtual/page/catalog/session/1755678688968001xkoa?token_type=bearer&amp;amp;client_id=08cbc601-1128-43a8-9587-150cadca5f52" target="_self" rel="noopener noreferrer"&gt;TechEd virtual website&lt;/A&gt;&amp;nbsp;(registration required). We had a full house, lots of interesting questions and loads of live demos (and no demo system let us down&amp;nbsp;&lt;span class="lia-unicode-emoji" title=":folded_hands:"&gt;🙏&lt;/span&gt;). We discussed the overall strategy of SAP Analytics Cloud in the context of SAP Business Data Cloud. On the planning side, we gave an outlook on our priorities for 2026 and highlighted recent innovations in seamless planning and GenAI. We could also showcase great new features like MyMetrics, Horizon theme support and provided a labs preview at how we want to further empower the story designer in SAP Analytics Cloud.&amp;nbsp;&lt;/P&gt;&lt;P&gt;Besides, I recommend these sessions (again, registration is required):&lt;/P&gt;&lt;TABLE border="1" width="56.25000127269703%"&gt;&lt;TBODY&gt;&lt;TR&gt;&lt;TD width="33.794466403162055%"&gt;&lt;STRONG&gt;Session ID &amp;amp; Name&lt;/STRONG&gt;&lt;/TD&gt;&lt;TD width="32.608695652173914%"&gt;&lt;STRONG&gt;Speakers&lt;/STRONG&gt;&lt;/TD&gt;&lt;TD width="33.201581027667984%"&gt;&lt;STRONG&gt;Link&lt;/STRONG&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="33.794466403162055%"&gt;&lt;SPAN&gt;ST107v | SAP Business Data Cloud: Strategy, architecture, and vision&lt;/SPAN&gt;&lt;/TD&gt;&lt;TD width="32.608695652173914%"&gt;&lt;SPAN&gt;&lt;STRONG&gt;Torsten Ammon&lt;/STRONG&gt;&lt;BR /&gt;SVP - Head of Business Data Cloud Core&amp;nbsp;&lt;BR /&gt;&lt;/SPAN&gt;&lt;STRONG&gt;&lt;STRONG&gt;Prakash Nanduri&lt;BR /&gt;&lt;/STRONG&gt;&lt;/STRONG&gt;Global Head of Product Management BDC&amp;amp;I&lt;STRONG&gt;&lt;BR /&gt;&lt;/STRONG&gt;&lt;/TD&gt;&lt;TD width="33.201581027667984%"&gt;&lt;A href="https://www.sap.com/events/teched/virtual/flow/sap/tev25/catalog-virtual/page/catalog/session/1751961499331001rRV1?token_type=bearer&amp;amp;client_id=08cbc601-1128-43a8-9587-150cadca5f52" target="_self" rel="noopener noreferrer"&gt;Link&lt;/A&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="33.794466403162055%"&gt;&lt;SPAN&gt;DA806v | SAP Business Data Cloud solution road map&lt;/SPAN&gt;&lt;/TD&gt;&lt;TD width="32.608695652173914%"&gt;&lt;SPAN&gt;&lt;STRONG&gt;Torsten Ammon&lt;/STRONG&gt;&lt;BR /&gt;SVP - Head of Business Data Cloud Core&amp;nbsp;&lt;BR /&gt;&lt;/SPAN&gt;&lt;STRONG&gt;Kristian Rümmelin&lt;/STRONG&gt;&lt;BR /&gt;&lt;SPAN&gt;SAP Business Data Cloud - Head of Product Management Planning &amp;amp; Analytics,&lt;/SPAN&gt;&lt;/TD&gt;&lt;TD width="33.201581027667984%"&gt;&lt;A href="https://www.sap.com/events/teched/virtual/flow/sap/tev25/catalog-virtual/page/catalog/session/1751961468799001rUtP?token_type=bearer&amp;amp;client_id=08cbc601-1128-43a8-9587-150cadca5f52" target="_self" rel="noopener noreferrer"&gt;Link&lt;/A&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="33.794466403162055%"&gt;&lt;SPAN&gt;DA203v | SAP Business Data Cloud: The path forward for SAP Business Warehouse&lt;/SPAN&gt;&lt;/TD&gt;&lt;TD width="32.608695652173914%"&gt;&lt;STRONG&gt;Dominik Kurz&lt;BR /&gt;&lt;/STRONG&gt;Product Expert&amp;nbsp;&lt;SPAN&gt;SAP BW/4HANA &amp;amp; SAP BW Bridge&lt;/SPAN&gt;&lt;BR /&gt;&lt;STRONG&gt;Klaus-Peter Sauer&lt;BR /&gt;&lt;/STRONG&gt;Product Management SAP Business Data Cloud&lt;/TD&gt;&lt;TD width="33.201581027667984%"&gt;&amp;nbsp;&lt;A href="https://www.sap.com/events/teched/virtual/flow/sap/tev25/catalog-virtual/page/catalog/session/1754495722564001nPF0?token_type=bearer&amp;amp;client_id=08cbc601-1128-43a8-9587-150cadca5f52" target="_self" rel="noopener noreferrer"&gt;Link&lt;/A&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;/TBODY&gt;&lt;/TABLE&gt;&lt;H5 id="toc-hId-1955501473"&gt;Hands-on&lt;/H5&gt;&lt;P&gt;Our hands-on workshop &lt;EM&gt;DA267 - Seamless planning for xP&amp;amp;A in SAP Business Data Cloud (SAP BDC)&lt;/EM&gt; was another highlight. Preparing such sessions is a big task but it always is worth the effort. Luckily, I was supported by&amp;nbsp;&lt;a href="https://community.sap.com/t5/user/viewprofilepage/user-id/183681"&gt;@jie_deng&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href="https://community.sap.com/t5/user/viewprofilepage/user-id/70553"&gt;@Matthew_Shaw&lt;/a&gt;&amp;nbsp;who supported the preparation, the testing, and the session itself. The new live version feature for seamless planning was front and center in the workshop. Participants learned how to:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Build a seamless planning model from a view that is part of content shipped with&amp;nbsp;&lt;SPAN&gt;Working Capital Insights Intelligent Application&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;Expose plan data in SAP Datasphere and bring it together with actual data in an analytic model&lt;/LI&gt;&lt;LI&gt;Extend the model with xP&amp;amp;A data and leverage the live version for calculations&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;The exercise and the tenants are still available! &lt;A href="https://github.com/SAP-samples/teched2025-DA267" target="_self" rel="nofollow noopener noreferrer"&gt;Check it out!&lt;/A&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Exercise flow" style="width: 805px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/340232iCBBAB8EAA0D121F1/image-dimensions/805x351/is-moderation-mode/true?v=v2" width="805" height="351" role="button" title="Flow.png" alt="Exercise flow" /&gt;&lt;span class="lia-inline-image-caption" onclick="event.preventDefault();"&gt;Exercise flow&lt;/span&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Thanks to everyone for joining TechEd and for your interest in our content! Looking forward to 2026 already.&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/teched-2025-in-berlin-recap-on-planning-in-sap-business-data-cloud/ba-p/14268314"/>
    <published>2025-11-13T18:00:00.026000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/is-sap-business-data-cloud-the-answer-to-your-ai-ambitions/ba-p/14268363</id>
    <title>Is SAP Business Data Cloud the Answer to Your AI Ambitions?</title>
    <updated>2025-11-13T18:17:06.369000+01:00</updated>
    <author>
      <name>MIKE210</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1952764</uri>
    </author>
    <content>&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Photo by pcess609 in iStock" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/340279i5A753BBBD858286D/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="istockphoto-1488294044-2048x2048.jpg" alt="Photo by pcess609 in iStock" /&gt;&lt;span class="lia-inline-image-caption" onclick="event.preventDefault();"&gt;Photo by pcess609 in iStock&lt;/span&gt;&lt;/span&gt;&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;This blog shows SAP customers how to apply the Fast-Track model to accelerate AI integration and technology transformation, aligning strategy, people, and processes with measurable outcomes while using SAP Business Data Cloud (BDC) to fast‑forward execution. BDC provides a governed, unified data foundation across SAP and non‑SAP systems, ready‑to‑use data products and Intelligent Applications, and open integrations that let teams move from data to decisions quickly without turning analytics into an IT project. With clear migration paths from BW, embedded governance and metadata, and a lifecycle focus on quick wins, continuous improvement, and renewal, the approach helps organizations de‑risk adoption, scale AI use cases, and turn innovation into sustained business value.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;Many firms struggle with a common problem: great new technologies often fail to deliver value because data is fragmented, governance is weak, and users aren’t equipped to exploit the tools. My 3 years research study calls this the gap between technology innovation &amp;amp; adoption and reveals insight is that technology adoption and AI integration is not just a technology issue; it’s a lifecycle challenge that involves strategy, leadership, change management, and ongoing measurement. The Fast-Track framework developed in the research emphasizes that data readiness and governance are central to accelerating adoption and sustaining value. SAP Business Data Cloud answers this challenge with a single, managed platform that harmonizes SAP and non-SAP data, provides ready made analytics and AI capabilities, and offers a lightweight path from data to insight without turning a data project into an IT project.&lt;/P&gt;&lt;H2 id="toc-hId-1764766975"&gt;&amp;nbsp;&lt;/H2&gt;&lt;H2 id="toc-hId-1568253470"&gt;&lt;STRONG&gt;&lt;U&gt;Fast-Track Model&lt;/U&gt;&lt;/STRONG&gt;&lt;/H2&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;The Robust Technology Adoption and AI Integration (Fast-Track) is a dual-perspective framework that guides technology vendors and users through complementary lifecycles vendors from pre-sales and contracting to implementation, post-adoption support, and renewal; Businesses from vision and strategy to planning, implementation, feedback-driven improvement, and expansion while diagnosing barriers and enablers across organizational, personal, and external dimensions. Grounded in established models (TAM, DIT, TTF, UTAUT), it emphasizes usefulness, ease of use, task fit, social influence, and facilitating conditions alongside industry standards, total cost of ownership, and vendor sup&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;port as critical adoption drivers. The model operationalizes success through strategic vision, comprehensive planning, stakeholder and change management, leadership buy-in, internal capability alignment, clear measurement mechanisms, and rigorous risk mitigation. It highlights perceived value, perceived quality, customer experience, CRM, and incentives tempered by switching costs as central to satisfaction and contract retention. AI strategy is treated as a core enabler, requiring alignment to business goals, robust data and infrastructure, and a culture of continuous learning. Above all, Fast-Track advocates continuous engagement and iterative improvement across the lifecycle to sustain fit with evolving needs, maximize technology value, and improve renewal rates and competitive advantage. If you'd like to explore further, my full research article with reproach is&amp;nbsp;&lt;A href="https://sap-my.sharepoint.com/:b:/p/mike_popal/EWOViZXA_oNNlQ5_Pg1UgpkBw5wG65O0nkKOcKH4NCnfdg?e=WXW90w" target="_blank" rel="noopener nofollow noreferrer"&gt;available here&lt;/A&gt;. Please don't hesitate to contact me if you wish to discuss the research findings.&lt;/P&gt;&lt;H2 id="toc-hId-1371739965"&gt;&amp;nbsp;&lt;/H2&gt;&lt;H2 id="toc-hId-1175226460"&gt;&lt;STRONG&gt;&lt;U&gt;How SAP Business Data Cluod fast forward the AI Integration?&lt;/U&gt;&lt;/STRONG&gt;&lt;/H2&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;A critical starting point is a single, trusted data foundation. Most organizations wrestle with multiple data sources, inconsistent definitions, and siloed analytics. SAP BDC creates a unified data domain where business users work with harmonized definitions of key entities for example, a “sales order” data product that includes customer, date, material, and amount. This eliminates the “where is the data from?” headache, reduces rework, and makes analytics faster and more reliable. The platform uses a common semantic layer so end users don’t have to chase data provenance across systems. Metadata and semantics travel with the data, giving business analysts confidence in what they see.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;Equally important is a built-in data lifecycle with governance. BDC manages data replication, transformation, and governance end to end. BDC Foundation Services handle the extraction, cleansing, and enrichment of data coming from SAP and non-SAP sources, while SAP Datasphere provides the cognitive layer to create, manage, and reuse data models. Standards such as Delta Sharing and the ORD protocol allow data producers to share data safely and efficiently with AI/ML workspaces and analytics tools. In practice, this reduces data movement friction and speeds time to insight often the biggest bottleneck in analytics initiatives.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;To accelerate outcomes, BDC brings ready-to-use analytics and AI accelerators. SAP Datasphere serves as the data modeling backbone, while SAP Analytics Cloud delivers dashboards and planning templates. SAP Databricks sits alongside to bring advanced machine learning and AI capabilities into the same data fabric, so data scientists can build models that leverage enterprise data with minimum friction. A key advantage for adoption is the availability of Intelligent Applications, pre-built, ready-to-consume analytics and AI artifacts that can be installed with just a few clicks. This lowers the barrier for business users to start generating value quickly, rather than waiting for a bespoke development cycle.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;What makes this solution even more attactive is the framework for organizations with legacy systems such as data warehouses. There is a clear development path from BW to modern analytics. Firms running SAP BW/4HANA can choose from practical options: lift-and-shift to Datasphere, hybrid models, or progressive migration via BW Bridge. In short, companies can move at their own pace, preserving their previouse investments while unlocking modern capabilities.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;The platform’s data architecture is designed for the new AI era. BDC supports high volume data, low friction analytics, and AI ready data products that are reusable, richly described with metadata, and delivered in data packages for easy discovery and activation. This modular approach makes it easier to expand the data ecosystem without starting from scratch on every project. The combination of a Delta Lake foundation and Delta Sharing provides a scalable, secure way to share data across teams and with external partners a critical capability for cross-organizational AI and planning efforts.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-978712955"&gt;How SAP Business Data Cloud relates to my research model so called Fast-Track?&lt;/H2&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;SAP BDC connects directly to the Fast-Track adoption model that anchors the my PhD study. In the Vision and Strategy phase of my research model, BDC enables a concrete data strategy aligned with business goals. Formations and spaces in Datasphere allow teams to organize data projects around line-of-business needs, turning the data program from a luxury into a governance driven growth engine. In the Adoption Planning and Enablement phase, Intelligent Applications and out-of-the-box analytics reduce time-to-value and provide tangible “first wins” for stakeholders. The simplicity of installing Intelligent Applications lowers the risk that business users view analytics as an IT-only project.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;Post-adoption, continuous improvement is essential. The platform’s governance, metadata, and data-quality controls ensure ongoing trust, while feedback loops and measurable outcomes KPIs, ROI, and adoption metrics can be embedded in a Success Measurement Mechanism (SMM) to track value realization and guide renewal decisions. In the Expansion and Renewal phase, as data products and intelligent apps proliferate, organizations can scale coverage across more lines of business, bring in external data, and extend AI use cases, all while preserving governance, security, and compliance.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;Real-world impact makes the case even clearer. Working Capital Insights is an Intelligent Application that aggregates data from S/4HANA, SuccessFactors, and external sources to offer a unified view of liquidity, accounts receivable, accounts payable, and inventory health. Its dashboards reveal cash gaps, forecasted working capital needs, and optimization opportunities delivered with minimal data wrangling. People Intelligence provides a workforce analytics suite built on the data fabric, combining SAP SuccessFactors data with other HR and performance sources to surface insights on skills gaps, workforce movement, and pay equity. These capabilities help governance and planning teams align talent with strategy. On the compliance front, centralized cataloging and lineage support data protection and regulatory adherence, simplifying audits and reducing risk as AI initiatives expand.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-782199450"&gt;From Pilot to Platform: Strategy for Leaders&lt;/H2&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;For leaders considering a BDC pilot, a few principles can make the difference between momentum and stagnation. Start with a clear business problem: identify the decision that will be improved by data-driven insight, translate it into a data product, and select a concrete Intelligent Application that can be installed quickly. Plan for governance and change management upfront; data quality, access controls, and steward roles matter as you scale, and Foundation Services alongside Datasphere governance features can help avoid common pitfalls. Think end-to-end, not end-of-project, by embracing my research model “Fast-Track” model’s emphasis on continuous improvement and value realization. That means planning for ongoing KPI tracking, user feedback, and feature enhancements as your data fabric grows. Finally, take a data-ecosystem view. BDC works best at the center of a landscape that includes non-SAP sources, AI/ML tooling, and BI planning. With ORD and Delta Sharing, collaboration across partners, vendors, and internal teams becomes practical without duplicating data.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;Executives can use a simple mental checklist to gauge readiness and fit. Do we have a trusted, unified data foundation? If not, BDC can harmonize data across SAP and non-SAP sources into a single domain model. Are we ready to governance, not just use data? Foundation Services and Datasphere provide governance, security, and lineage for scalable analytics. Can we realize value quickly? Intelligent Applications and ready-to-use data products deliver near-term wins and accelerate executive buy-in. Are we prepared to expand and renew? A modular, scalable data fabric supports ongoing growth, wider AI adoption, and contract retention through continuous value delivery.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&amp;nbsp;&lt;/P&gt;&lt;H3 id="toc-hId-714768664"&gt;Which innovative technology to leaverage?&lt;/H3&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;Technology innovation without adoption is a lost opportunity. SAP Business Data Cloud offers a practical, Enterprise-level roadmap to close the gap between what technology is &amp;nbsp;avialable in the market to leaverage and what’s actually used with the organization. By standardizing data definitions, streamlining data flows, and delivering ready-to-consume analytics and AI, BDC helps organizations move from data to decisions faster, with less risk and more confidence. If your organization is serious about turning technology innovation into sustained competitive advantage, a well-planned BDC journey anchored by a clear business problem, strong governance, and a structured adoption lifecycle could be your fastest route to measurable, lasting value.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&amp;nbsp;&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;Find more similar article on my linked in page:&lt;/P&gt;&lt;TABLE&gt;&lt;TBODY&gt;&lt;TR&gt;&lt;TD width="258"&gt;&amp;nbsp;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;&amp;nbsp;&lt;/TD&gt;&lt;TD&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="MIKE210_0-1763053881657.jpeg" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/340264i3A180A8F1E9F1D58/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="MIKE210_0-1763053881657.jpeg" alt="MIKE210_0-1763053881657.jpeg" /&gt;&lt;/span&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;/TBODY&gt;&lt;/TABLE&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;@&lt;A href="https://www.linkedin.com/in/mi4po/" target="_blank" rel="noopener nofollow noreferrer"&gt;https://www.linkedin.com/in/mi4po/&lt;/A&gt;&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/is-sap-business-data-cloud-the-answer-to-your-ai-ambitions/ba-p/14268363"/>
    <published>2025-11-13T18:17:06.369000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/understanding-seamless-planning/ba-p/14273093</id>
    <title>Understanding Seamless Planning</title>
    <updated>2025-11-19T22:00:45.821000+01:00</updated>
    <author>
      <name>hkaur</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/83605</uri>
    </author>
    <content>&lt;P&gt;&lt;STRONG&gt;Introduction to Seamless Planning&amp;nbsp;&amp;nbsp;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Seamless Planning is a new integration paradigm between SAP Analytics Cloud (SAC) and SAP Datasphere that unifies planning logic with enterprise-grade data storage and governance. It allows SAC to remain the planning experience and calculation engine while Datasphere becomes the authoritative and governed persistence layer for plan data and master data. This means planning teams can reuse planning data across analytics, data transformation pipelines, and operational applications—without duplicating or exporting data outside of SAC.&amp;nbsp;Modelers can build SAC planning models that physically store their transactional data and public dimensions inside Datasphere. They also have control over whether planning artifacts from SAC are exposed in Datasphere for reuse across Spaces or downstream analytic models.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Technical Architecture&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;EM&gt;Model Metadata &amp;amp; Planning Logic&lt;/EM&gt;: Planning calculations, version management, and data actions remain defined inside SAC.&lt;/LI&gt;&lt;LI&gt;&lt;EM&gt;Fact Data &amp;amp; Dimensions Stored in Datasphere&lt;/EM&gt;: The planning model’s fact tables and public dimensions are persisted in Datasphere to enable governed reuse and enterprise distribution.&lt;/LI&gt;&lt;LI&gt;&lt;EM&gt;Data Exposure from SAC to Datasphere: &lt;/EM&gt;SAC can expose selected planning objects as read-only Datasphere artifacts, enabling modelers to use these objects in Data Builder or transform them into Analytical Models.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Key Benefits&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;EM&gt;Unified Planning Data:&lt;/EM&gt; Centralized storage of plan facts and dimensions in Datasphere ensures consistency across systems and workflows.&lt;/LI&gt;&lt;LI&gt;&lt;EM&gt;Direct Persistence to Datasphere:&lt;/EM&gt; Changes made in SAC planning flows are instantly reflected in Datasphere fact tables—no manual exports required.&lt;/LI&gt;&lt;LI&gt;&lt;EM&gt;Optimized SAC Resource Usage:&lt;/EM&gt; Offloading data storage to Datasphere reduces SAC’s memory and storage footprint.&lt;/LI&gt;&lt;LI&gt;&lt;EM&gt;Enterprise Reusability &amp;amp; Governance&lt;/EM&gt;: Datasphere’s modeling and transformation extend to planning data, enabling secure and scalable reuse in the form of analytic models.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;This approach eliminates data silos, reduces duplication, and empowers organizations to operationalize planning data across their digital landscape—while preserving SAC’s rich planning experience at the front end.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Prerequisites for Seamless Planning&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Before configuring or deploying planning models from SAC to Datasphere, ensure the following system prerequisites are met.&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;&lt;EM&gt;SAC Tenant Must Be Running on SAP HANA Cloud&lt;/EM&gt;&lt;UL&gt;&lt;LI&gt;SAC must be provisioned on SAP HANA Cloud infrastructure.&lt;/LI&gt;&lt;LI&gt;To verify: Navigate to System → About in SAC and confirm the HANA Cloud version is listed.&lt;/LI&gt;&lt;LI&gt;This ensures compatibility with Datasphere’s storage and modeling architecture.&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&amp;nbsp;2.&amp;nbsp;&amp;nbsp;&lt;EM&gt;Tenant Co-Location and 1:1 Linkage&lt;/EM&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Both SAC and Datasphere tenants must reside in the same SAP data center region.&lt;/LI&gt;&lt;LI&gt;A 1:1 tenant relationship is required—each SAC tenant must be linked to a single Datasphere tenant.&lt;/LI&gt;&lt;LI&gt;This linkage enables SAC to persist planning data directly into Datasphere spaces.&lt;/LI&gt;&lt;LI&gt;This can be done in System à Administration à Tenant Links in both SAP Analytics Cloud and SAP Datasphere.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="hkaur_1-1763585241956.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/342824i0BA7BFD6BD8C529B/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="hkaur_1-1763585241956.png" alt="hkaur_1-1763585241956.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;EM&gt;3. Consistent Identity Provider (IdP) Configuration&lt;/EM&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;It is strongly recommended that both tenants use the same SAML-based Identity Provider (IdP).&lt;/LI&gt;&lt;LI&gt;This ensures consistent user identity mapping across SAC and Datasphere.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;EM&gt;4. System Owner Credentials on Both Tenants&lt;/EM&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;The tenant linkage process requires authentication using a system owner account on both SAC and Datasphere.&lt;/LI&gt;&lt;LI&gt;This account must have administrative privileges to authorize cross-tenant integration.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;EM&gt;5. User Role Assignment in Datasphere Space&lt;/EM&gt;&lt;/P&gt;&lt;OL&gt;&lt;UL&gt;&lt;LI&gt;SAC users who need to create, edit, or expose planning models must be granted appropriate space-level roles in Datasphere.&lt;/LI&gt;&lt;LI&gt;Without these roles, SAC will not be able to write data or expose model objects to the designated Datasphere space.&lt;/LI&gt;&lt;/UL&gt;&lt;/OL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="hkaur_2-1763585429580.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/342825iE573B6F496F807BC/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="hkaur_2-1763585429580.png" alt="hkaur_2-1763585429580.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;If any of the above conditions are not met, then Datasphere will not appear as a selectable data storage location in SAC. The &lt;A href="https://me.sap.com/notes/3515100" target="_blank" rel="noopener noreferrer"&gt;&amp;nbsp;SAP Knowledge Base Article 3515100&lt;/A&gt; highlights certain errors along with their corresponding workarounds.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Workflow for Seamless Planning&lt;/STRONG&gt;&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;Create a New Model in SAC&lt;STRONG&gt;:&lt;/STRONG&gt; While creating a new Model in SAC, there is an option available to specify the Data Storage Location as a Space within SAP Datasphere.&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="hkaur_3-1763585457335.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/342826i7ED2677A27AC66B3/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="hkaur_3-1763585457335.png" alt="hkaur_3-1763585457335.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;EM&gt;2. Configure Dimensions and Measures:&lt;/EM&gt; &amp;nbsp;The model created with Datasphere persistence layer is created as a Planning Measure based model by default and the data storage location is Datasphere Space as shown in model details.&lt;/P&gt;&lt;P&gt;&lt;EM&gt;3. Expose to Datasphere:&lt;/EM&gt; There is also an additional option in the Model Details to expose the underlying fact table in the chosen Datasphere space. &lt;EM&gt;What gets created in Datasphere: &lt;/EM&gt;When a SAC planning model is configured to store data in Datasphere:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="hkaur_4-1763585500428.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/342827iEC11E95E4FCF4D72/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="hkaur_4-1763585500428.png" alt="hkaur_4-1763585500428.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;EM&gt;4. What gets created in Datasphere:&amp;nbsp;&lt;/EM&gt;When a SAC planning model is configured to store data in Datasphere:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;A Fact object (read-only) becomes visible in the Datasphere Space.&lt;/LI&gt;&lt;LI&gt;A corresponding physical table (sap.sac.&amp;lt;GUID&amp;gt;) is created to store transactional planning data.&lt;/LI&gt;&lt;LI&gt;Any public dimensions used in the model are created in Datasphere as dimension tables and remain shareable across Space.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="hkaur_5-1763585533182.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/342828iE713F5C915428392/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="hkaur_5-1763585533182.png" alt="hkaur_5-1763585533182.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;EM&gt;5. Execute Planning scenarios in SAC&lt;/EM&gt;&lt;STRONG&gt;:&lt;/STRONG&gt; With the new model now exposed in Datasphere, key planning scenarios can continue to be executed within SAC exactly as before. Seamless Planning does not alter the way SAC planning processes run; it simply extends the option to persist and integrate data through Datasphere while maintaining the familiar SAC planning experience.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;EM&gt;Create and Execute a Data Action&lt;/EM&gt;: Develop a Data Action in SAC to update planning data. Once executed, publish the updated data back to Datasphere to ensure synchronization across platforms.&lt;/LI&gt;&lt;LI&gt;&lt;EM&gt;Conduct Actual vs Plan Analysis&lt;/EM&gt;: Utilize SAC's analytical capabilities to compare actual performance against planned figures. This enables variance analysis and supports informed decision-making.&lt;/LI&gt;&lt;LI&gt;&lt;EM&gt;Extend Modelling in Datasphere&lt;/EM&gt;: Build custom calculations directly within SAP Datasphere using the exposed model. Create a dedicated view and Analytical Model on top of these calculations, then visualize the results seamlessly in SAC for enhanced planning insights.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Important Considerations&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;SAP is actively working to reduce the space dependency in future releases, for latest updates please refer the &lt;A href="https://roadmaps.sap.com/board?q=seamless%20planning&amp;amp;range=CURRENT-LAST#Q4%202025" target="_blank" rel="noopener noreferrer"&gt;SAP Roadmap Explorer&lt;/A&gt; and the &lt;A href="https://help.sap.com/docs/SAP_ANALYTICS_CLOUD/00f68c2e08b941f081002fd3691d86a7/6d81dcce234b417e8afb8450abab785e.html" target="_blank" rel="noopener noreferrer"&gt;help guide&lt;/A&gt;.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;EM&gt;Cross-Model Planning Constraints:&lt;/EM&gt; Currently all models involved in cross-model planning operations—such as a data action, cross-model copy step must be in the same Datasphere space.&lt;/LI&gt;&lt;LI&gt;&lt;EM&gt;Shared Public Dimensions&lt;/EM&gt;: &amp;nbsp;Sharing public dimensions across multiple models is only supported when both the models and the associated public dimensions are deployed within the same SAP Datasphere space. The same requirement applies to currency rate tables, which must reside in the same space to be referenced across models.&lt;/LI&gt;&lt;LI&gt;&lt;EM&gt;Hierarchies:&lt;/EM&gt; &amp;nbsp;Hierarchies defined in SAP Analytics Cloud are not exposed to SAP Datasphere. If hierarchical structures are required for modeling or reporting, Datasphere modelers must reconstruct these hierarchies natively within Datasphere, ensuring alignment with the underlying data model and semantic layer.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Seamless Planning in BDC&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;SAP is introducing a unified enterprise planning paradigm through SAP Business Data Cloud (BDC). This represents the next stage in the evolution of Seamless Planning, where planning activities are connected, real-time, and driven by trusted operational data—eliminating the traditional “export, transform, and import” workflow. Because SAC and Datasphere are core components of BDC, seamless planning is highly relevant in this context and establishes the foundation for deep integration of planning data within the BDC environment. Looking ahead, BDC has the potential to further enrich seamless planning by:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Supporting planning-enabled intelligent applications that combine analytics with actionable planning workflows.&lt;/LI&gt;&lt;LI&gt;Extending planning architectures to include platforms such as SAP Databricks.&lt;/LI&gt;&lt;LI&gt;Enabling the consumption of data products directly within planning processes, ensuring governed, reusable, and scalable planning assets.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;To understand Seamless Planning in context of Business Data Cloud, please refer to this &lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/unlocking-the-next-chapter-of-seamless-planning-in-sap-business-data-cloud/ba-p/14243864" target="_blank"&gt;blog&lt;/A&gt;.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/understanding-seamless-planning/ba-p/14273093"/>
    <published>2025-11-19T22:00:45.821000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/sap-rpt-1-context-model-vs-training-classical-models-the-models-battle/ba-p/14268507</id>
    <title>SAP RPT-1 Context Model vs. Training Classical Models: The Models Battle (Python Hands-on)</title>
    <updated>2025-11-20T07:50:27.670000+01:00</updated>
    <author>
      <name>nicolasestevan</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1198632</uri>
    </author>
    <content>&lt;H2 id="toc-hId-1764768715"&gt;&lt;span class="lia-unicode-emoji" title=":collision:"&gt;💥&lt;/span&gt;The Models Battle&lt;/H2&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_5-1763206328497.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341535i2A2C9A98D24BF43B/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_5-1763206328497.png" alt="nicolasestevan_5-1763206328497.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Predictive modeling is becoming a built-in capability across SAP, improving how teams handle forecasting, pricing, and planning. &lt;STRONG&gt;Many SAP professionals, however, aren’t machine-learning specialists&lt;/STRONG&gt;, and traditional models often demand extensive setup, tuning, and repeated training, which slows down new ideas.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;SAP RPT-1&lt;/STRONG&gt; offers a simpler path. It’s a pretrained model from SAP, also available in an OSS version, that lets developers and consultants produce predictions with far less technical effort, no deep ML background required.&lt;/P&gt;&lt;P&gt;I've explored SAP RPT-1 hands-on, comparing it with traditional regressors using Python and a real public vehicles price dataset.&amp;nbsp;&lt;/P&gt;&lt;BLOCKQUOTE&gt;&lt;P&gt;&lt;STRONG&gt;Goal:&lt;/STRONG&gt; To see (as a non Data Scientist) how &lt;STRONG&gt;SAP RPT-1&lt;/STRONG&gt; behaves in practice, what advantages and limits it shows, and when it could make sense in a predictive scenario.&lt;/P&gt;&lt;/BLOCKQUOTE&gt;&lt;P&gt;Usually for real-world scenario, the right approach would be consume the SAP RPT-1 though the available and simplified API, but for studies proposal and fair comparision over othe traditional ML models, the &lt;STRONG&gt;OSS&lt;/STRONG&gt; fits perfectly for it:&lt;/P&gt;&lt;HR /&gt;&lt;H2 id="toc-hId-1568255210"&gt;&lt;span class="lia-unicode-emoji" title=":thinking_face:"&gt;🤔&lt;/span&gt;&amp;nbsp;SAP RPT-1 vs Traditional Machine Learning - Core Differences&lt;/H2&gt;&lt;P&gt;Before diving into the code, let’s quickly revisit how&lt;STRONG&gt; traditional ML&lt;/STRONG&gt; models work:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Training-based models like Random Forest, LightGBM, and Linear Regression learn patterns directly from data.&amp;nbsp;&lt;/LI&gt;&lt;LI&gt;They require hundreds or thousands of examples to tune their internal parameters.&lt;/LI&gt;&lt;LI&gt;Their performance depends heavily on data quantity and quality.&lt;/LI&gt;&lt;LI&gt;The more relevant examples they see, the smarter they get.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;On the other hand, &lt;STRONG&gt;SAP RPT-1 f&lt;/STRONG&gt;ollows a different philosophy. It’s part of the RPT (Representational Predictive Transformer) family, pretrained on a wide variety of business and contextual data. This means:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;You don’t "train" it in the traditional sense. Instead, it uses context embeddings to predict outcomes.&lt;/LI&gt;&lt;LI&gt;It can be used immediately, even with smaller datasets.&lt;/LI&gt;&lt;LI&gt;The OSS version allows developers to experiment directly in Python.&lt;/LI&gt;&lt;LI&gt;No special SAP backend required.&lt;/LI&gt;&lt;/UL&gt;&lt;BLOCKQUOTE&gt;&lt;P&gt;&lt;STRONG&gt;Outcome:&lt;/STRONG&gt; Traditional ML models learn from high amount of data. SAP RPT-1 already knows how to deal with small context amount of data.&lt;/P&gt;&lt;/BLOCKQUOTE&gt;&lt;HR /&gt;&lt;H2 id="toc-hId-1371741705"&gt;&lt;span class="lia-unicode-emoji" title=":desktop_computer:"&gt;🖥&lt;/span&gt;&amp;nbsp;The Experiment - Setup &amp;amp; Dataset&amp;nbsp;&lt;/H2&gt;&lt;div class="lia-spoiler-container"&gt;&lt;a class="lia-spoiler-link" href="#" rel="nofollow noopener noreferrer"&gt;Spoiler&lt;/a&gt;&lt;noscript&gt; (Highlight to read)&lt;/noscript&gt;&lt;div class="lia-spoiler-border"&gt;&lt;div class="lia-spoiler-content"&gt;Don't worry on "playing puzzles" copying + pasting code below. The full version is available for download at end!&lt;/div&gt;&lt;noscript&gt;&lt;div class="lia-spoiler-noscript-container"&gt;&lt;div class="lia-spoiler-noscript-content"&gt;Don't worry on "playing puzzles" copying + pasting code below. The full version is available for download at end!&lt;/div&gt;&lt;/div&gt;&lt;/noscript&gt;&lt;/div&gt;&lt;/div&gt;&lt;P&gt;To make this comparison tangible, I built a simple yet realistic Python experiment to predict vehicle selling prices using a public dataset containing car attributes like make, model, year, transmission, and mileage.&lt;/P&gt;&lt;P&gt;Why vehicle pricing? Because it’s an intuitive example where both traditional machine learning and pretrained AI models can be applie, and it helps visualize how prediction quality evolves as the sample size grows.&lt;/P&gt;&lt;P&gt;This entire analysis runs on a local Python environment&amp;nbsp;with the following stack:&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import os
import gc
import warnings
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LinearRegression
from sap_rpt_oss import SAP_RPT_OSS_Regressor
import lightgbm as lgb&lt;/code&gt;&lt;/pre&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;pandas&lt;/STRONG&gt; and &lt;STRONG&gt;numpy&lt;/STRONG&gt; for data manipulation&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;scikit-learn&lt;/STRONG&gt; for classical ML regressors (R&lt;STRONG&gt;andom Forest, Linear Regression&lt;/STRONG&gt;)&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;LightGBM&lt;/STRONG&gt; for gradient &lt;STRONG&gt;boosting&lt;/STRONG&gt; comparison&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;sap_rpt_oss&lt;/STRONG&gt; — the open-source Python version of &lt;STRONG&gt;SAP’s RPT-1 model&lt;/STRONG&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;matplotlib&lt;/STRONG&gt; for all &lt;STRONG&gt;visualizations&lt;/STRONG&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;BLOCKQUOTE&gt;&lt;P&gt;&lt;STRONG&gt;SAP RPT-1 OSS &lt;/STRONG&gt;can be downloaded installed following official Hugging Face:&amp;nbsp;&lt;A title="https://huggingface.co/SAP/sap-rpt-1-oss?library=sap-rpt-1-oss" href="https://huggingface.co/SAP/sap-rpt-1-oss?library=sap-rpt-1-oss" target="_blank" rel="noopener nofollow noreferrer"&gt;https://huggingface.co/SAP/sap-rpt-1-oss?library=sap-rpt-1-oss&lt;/A&gt;&amp;nbsp;. Python can be installed with executable download on Windows, or via &lt;STRONG&gt;Home Brew&lt;/STRONG&gt; for Mac and &lt;STRONG&gt;apt&lt;/STRONG&gt; commands for Linux. Libraries dependencies can be downloaded with &lt;STRONG&gt;pip&lt;/STRONG&gt; commands. Googling it may not be a road blocker.&lt;/P&gt;&lt;/BLOCKQUOTE&gt;&lt;P&gt;We use a sample&amp;nbsp;vehicle sales dataset. The complete file is about to 88Mb but for such experiment a restricted sample of 20k as it's more than enough to prove our the concept, still it's faster and consuming less computing resources.&lt;/P&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;TABLE border="1" width="498px"&gt;&lt;TBODY&gt;&lt;TR&gt;&lt;TD&gt;&lt;STRONG&gt;Feature&lt;/STRONG&gt;&lt;/TD&gt;&lt;TD&gt;&lt;STRONG&gt;Description&lt;/STRONG&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;year&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Vehicle model year&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;make&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Brand (e.g., Toyota, Ford, BMW)&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;model&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Specific model name&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;body&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Type (SUV, Sedan, etc.)&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;transmission&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Gear type&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;odometer&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Vehicle mileage&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;color&lt;/CODE&gt;, &lt;CODE&gt;interior&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Visual attributes&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;sellingprice&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;The target variable to predict&lt;/TD&gt;&lt;/TR&gt;&lt;/TBODY&gt;&lt;/TABLE&gt;&lt;P&gt;&lt;STRONG&gt;&lt;span class="lia-unicode-emoji" title=":bar_chart:"&gt;📊&lt;/span&gt;&amp;nbsp;Dataset Download:&lt;/STRONG&gt;&amp;nbsp;&lt;A title="https://www.kaggle.com/datasets/syedanwarafridi/vehicle-sales-data?resource=download" href="https://www.kaggle.com/datasets/syedanwarafridi/vehicle-sales-data?resource=download" target="_blank" rel="noopener nofollow noreferrer"&gt;https://www.kaggle.com/datasets/syedanwarafridi/vehicle-sales-data?resource=download&lt;/A&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;The dataset is loaded and preprocessed in a few simple steps:&lt;/P&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;df = pd.read_csv("car_prices.csv").sample(n=20000, random_state=42)

# Fill missing values for categorical columns
fill_defaults = {
    'make': 'Other', 'model': 'Other', 'color': 'Other',
    'interior': 'Unknown', 'body': 'Unknown', 'transmission': 'Unknown'
}
for col, val in fill_defaults.items():
    df[col] = df[col].fillna(val)

X = df[["year", "make", "model", "body", "transmission", "odometer", "color", "interior"]]
y = df["sellingprice"]&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;At this point, the stage is set:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;The data is clean.&lt;/LI&gt;&lt;LI&gt;The environment is ready.&lt;/LI&gt;&lt;LI&gt;All models, traditionals and SAP RPT-1, are ready to be tested under identical conditions.&lt;/LI&gt;&lt;/UL&gt;&lt;HR /&gt;&lt;H2 id="toc-hId-1175228200"&gt;&lt;span class="lia-unicode-emoji" title=":robot_face:"&gt;🤖&lt;/span&gt;&amp;nbsp;Training the Models - Three different ones&lt;/H2&gt;&lt;P&gt;With the dataset ready, the &lt;STRONG&gt;next step&lt;/STRONG&gt; is to run each model under the same conditions: &lt;STRONG&gt;same features, same target, same train/test split and same random seed&lt;/STRONG&gt;. This ensures the comparison is fair and repeatable.&lt;/P&gt;&lt;P&gt;We evaluate prediction performance using &lt;STRONG&gt;R² (coefficient of determination)&lt;/STRONG&gt;, which indicates how much of the price variation the model can explain (1.0 = perfect prediction).&lt;/P&gt;&lt;HR /&gt;&lt;H3 id="toc-hId-1107797414"&gt;Training Model #1 - Random Forest&lt;/H3&gt;&lt;P&gt;Random Forest is often the first model used in tabular ML. It works by creating &lt;STRONG&gt;many decision trees&lt;/STRONG&gt; and averaging their predictions. Before training, categorical variables need to be &lt;STRONG&gt;label-encoded&lt;/STRONG&gt; into numbers, a common requirement for classical ML models:&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;def train_random_forest(X, y):
    X = X.copy()
    cat_cols = ["make", "model", "body", "transmission", "color", "interior"]
    le = LabelEncoder()

    for col in cat_cols:
        X[col] = le.fit_transform(X[col].astype(str).fillna("Unknown"))

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=default_test_size, random_state=42
    )

    model = RandomForestRegressor(
        n_estimators=150, max_depth=20, random_state=42, n_jobs=-1
    )

    try:
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        r2 = r2_score(y_test, preds)
    except Exception as e:
        preds, r2 = np.zeros_like(y_test), 0

    return [preds, r2, y_test]&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-911283909"&gt;Up to 50 rows:&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_3-1763206176248.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341502i82216AA724092E03/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_3-1763206176248.png" alt="nicolasestevan_3-1763206176248.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId-714770404"&gt;Up to 7067 rows:&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_8-1763206511155.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341538iF2A25E0C0EBE0612/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_8-1763206511155.png" alt="nicolasestevan_8-1763206511155.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId-518256899"&gt;Live view&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="RandomForest_20251115_092355.gif" style="width: 960px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341551i3A2C874AFAF47388/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="RandomForest_20251115_092355.gif" alt="RandomForest_20251115_092355.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;HR /&gt;&lt;H3 id="toc-hId-321743394"&gt;Training Model #2 - LightGBM&lt;/H3&gt;&lt;P&gt;LightGBM is one of the most powerful models for tabular data. Unlike Random Forest (many independent trees), LightGBM builds trees &lt;STRONG&gt;sequentially&lt;/STRONG&gt;, each correcting the errors of the previous one. It supports categorical features natively, which simplifies preprocessing.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;def train_lightgbm(X, y):
    X = X.copy()
    cat_cols = ["make", "model", "body", "transmission", "color", "interior"]
    for col in cat_cols:
        X[col] = X[col].astype(str).fillna("Unknown").astype("category")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=default_test_size, random_state=42
    )

    model = lgb.LGBMRegressor(
        n_estimators=500, learning_rate=0.05, num_leaves=31,
        subsample=0.8, colsample_bytree=0.8, random_state=42
    )

    try:
        model.fit(X_train, y_train, categorical_feature=cat_cols)
        preds = model.predict(X_test)
        r2 = r2_score(y_test, preds)
    except Exception:
        preds, r2 = np.zeros_like(y_test), 0

    return [preds, r2, y_test]&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-125229889"&gt;Up to 50 rows:&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_2-1763205951324.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341474i1AAB214E2D01C2B2/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_2-1763205951324.png" alt="nicolasestevan_2-1763205951324.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId--146514985"&gt;Up to 7067 rows:&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_7-1763206474860.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341537i0ACD453B96C87ADF/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_7-1763206474860.png" alt="nicolasestevan_7-1763206474860.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId--343028490"&gt;Live view&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="LightGBM_20251115_092355.gif" style="width: 960px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341552i30BC4DE94C4988F6/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="LightGBM_20251115_092355.gif" alt="LightGBM_20251115_092355.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;HR /&gt;&lt;H3 id="toc-hId--539541995"&gt;Training Model #3 - Linear Regression&lt;/H3&gt;&lt;P&gt;Not fancy and even not complex, Linear Regression provides a baseline that shows:&amp;nbsp;&lt;SPAN&gt;“If the relationship between attributes and price is roughly linear, how well can a simple model perform?”&lt;/SPAN&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;def train_linear_model(X, y):
    X = X.copy()
    cat_cols = ["make", "model", "body", "transmission", "color", "interior"]
    for col in cat_cols:
        X[col] = LabelEncoder().fit_transform(X[col].astype(str).fillna("Unknown"))

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=default_test_size, random_state=42
    )

    model = LinearRegression()
    X_train = X_train.fillna(X_train.mean(numeric_only=True))
    X_test = X_test.fillna(X_test.mean(numeric_only=True))

    try:
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        r2 = r2_score(y_test, preds)
    except Exception:
        preds, r2 = np.zeros_like(y_test), 0

    return [preds, r2, y_test]&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId--736055500"&gt;&lt;STRONG&gt;Up to 50 rows:&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_1-1763205857765.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341472i81AFB2D0BE770F90/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_1-1763205857765.png" alt="nicolasestevan_1-1763205857765.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId--932569005"&gt;&lt;STRONG&gt;Up to 7067 rows:&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_6-1763206428099.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341536iC708165AEAE11D46/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_6-1763206428099.png" alt="nicolasestevan_6-1763206428099.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId--1129082510"&gt;Live view&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="LinearModel_20251115_092355.gif" style="width: 960px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341553i0849B4C842A417EE/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="LinearModel_20251115_092355.gif" alt="LinearModel_20251115_092355.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H2 id="toc-hId--1032193008"&gt;&lt;span class="lia-unicode-emoji" title=":chequered_flag:"&gt;🏁&lt;/span&gt;&amp;nbsp;&lt;SPAN&gt;SAP RPT-1 OSS: Context Model&lt;/SPAN&gt;&lt;/H2&gt;&lt;P&gt;This is where things get interesting. SAP RPT-1 does &lt;STRONG&gt;not&lt;/STRONG&gt; rely on learning patterns from the dataset. Instead, it uses a pretrained transformer architecture to infer relationships directly through &lt;STRONG&gt;context embeddings&lt;/STRONG&gt;. Lean and simple, "for non-Data Science PhD":&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;def train_sap_rpt1(X, y):
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=default_test_size, random_state=42
    )

    model = SAP_RPT_OSS_Regressor(max_context_size=8192, bagging=8)
    model.fit(X_train, y_train)

    preds = model.predict(X_test)
    r2 = r2_score(y_test, preds)

    return [preds, r2, y_test]&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId--1522109520"&gt;&lt;STRONG&gt;Up to 50 rows:&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_0-1763205729558.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341471i4AC7007DCA5A0F76/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_0-1763205729558.png" alt="nicolasestevan_0-1763205729558.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId--1718623025"&gt;&lt;STRONG&gt;Up to 2055 rows:&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_4-1763206228416.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341505i9ADE9D2D2B38C363/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_4-1763206228416.png" alt="nicolasestevan_4-1763206228416.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId--1915136530"&gt;Live view&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="SAP_RPT1_20251115_092355.gif" style="width: 960px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341566i0BE0E0D666836951/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="SAP_RPT1_20251115_092355.gif" alt="SAP_RPT1_20251115_092355.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;HR /&gt;&lt;H2 id="toc-hId--1650063337"&gt;&lt;STRONG&gt;&lt;span class="lia-unicode-emoji" title=":magnifying_glass_tilted_right:"&gt;🔎&lt;/span&gt;&amp;nbsp;Running Experiments at Multiple Sample Sizes&lt;/STRONG&gt;&lt;/H2&gt;&lt;P&gt;This section breaks down how the iterative experiment loop works, why the SAP RPT-1 OSS model has a max-context limit, and how performance changes as we scale up the dataset. By running the same models across several sample sizes, we can see where traditional ML shines, where RPT-1 stays competitive, and how both behave as the data grows.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;sample_sizes = np.linspace(50, len(X), 200, dtype=int)
results, max_r2_rpt1, max_sample_rpt1 = [], 0, 0

for n in sample_sizes:
    idx = np.random.choice(len(X), n, replace=False)
    X_sample, y_sample = X.iloc[idx], y.iloc[idx]


    # SAP RPT-1 OSS (limited sample size)
    if n &amp;lt;= rpt1_limit:
        rpt_res = train_sap_rpt1(X_sample, y_sample)
        fn = plot_predictions(rpt_res[2], rpt_res[0], rpt_res[1], "SAP_RPT1", n)
        video_frames["SAP_RPT1"].append(fn)
        r2_rpt1 = rpt_res[1]
        max_r2_rpt1 = max(max_r2_rpt1, r2_rpt1)
    else:
        r2_rpt1 = max_r2_rpt1
        if max_sample_rpt1 == 0:
            max_sample_rpt1 = n

    # Train and plot models
    rf_res = train_random_forest(X_sample, y_sample)
    fn = plot_predictions(rf_res[2], rf_res[0], rf_res[1], "RandomForest", n)
    video_frames["RandomForest"].append(fn)

    lgb_res = train_lightgbm(X_sample, y_sample)
    fn = plot_predictions(lgb_res[2], lgb_res[0], lgb_res[1], "LightGBM", n)
    video_frames["LightGBM"].append(fn)

    lin_res = train_linear_model(X_sample, y_sample)
    fn = plot_predictions(lin_res[2], lin_res[0], lin_res[1], "LinearModel", n)
    video_frames["LinearModel"].append(fn)

    results.append((n, rf_res[1], r2_rpt1, lgb_res[1], lin_res[1]))

    # Early stop if traditional model reaches SAP RPT-1
    if rf_res[1] &amp;gt;= max_r2_rpt1 or lgb_res[1] &amp;gt;= max_r2_rpt1 or lin_res[1] &amp;gt;= max_r2_rpt1:
        break
    gc.collect()&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;This loop compares SAP RPT-1 OSS with traditional ML models as sample sizes increase. Each iteration randomly selects a subset of the data and trains all models on the same slice for a fair comparison. SAP RPT-1 can only run up to its max-context limit, so once the sample size exceeds that threshold, it stops retraining and simply carries forward its best R². The traditional models continue training at every step. The loop ends early when any traditional model matches or surpasses RPT-1’s best score, making the experiment efficient while showing how performance evolves as data grows.&lt;/P&gt;&lt;HR /&gt;&lt;H2 id="toc-hId--1846576842"&gt;&lt;STRONG&gt;&lt;span class="lia-unicode-emoji" title=":end_arrow:"&gt;🔚&lt;/span&gt;&amp;nbsp;Conclusion and Final Thoughts&lt;/STRONG&gt;&lt;/H2&gt;&lt;P&gt;&amp;nbsp;SAP RPT-1 OSS stands out because it performs well with small datasets, requires minimal code, and can generate useful predictions with just an API call and a bit of context. This makes it ideal for jump-starting predictive use cases early on, delivering fast business value without a full ML pipeline. Traditional models, however, still shine when projects mature, data grows, and fine-tuned control becomes important. It’s not about choosing one over the other, but understanding where each approach brings the most value.&lt;/P&gt;&lt;TABLE border="1" width="100%"&gt;&lt;TBODY&gt;&lt;TR&gt;&lt;TD&gt;&lt;STRONG&gt;&amp;nbsp;&lt;/STRONG&gt;&lt;STRONG&gt;Aspect&amp;nbsp;&lt;/STRONG&gt;&lt;/TD&gt;&lt;TD&gt;&lt;STRONG&gt;SAP RPT-1 OSS&amp;nbsp;&lt;/STRONG&gt;&lt;/TD&gt;&lt;TD&gt;&lt;STRONG&gt;Traditional ML (RF, LGBM, Linear)&lt;/STRONG&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Data Requirements&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;Low (performs well with small samples)&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Medium/High (performance scales with data&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Setup Effort&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;Minimal (API call + context)&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Higher (preprocessing, encoding, tuning)&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Training Process&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;None (pretrained context model)&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Full training pipeline required&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Speed to Insights&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;Very fast&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Moderate to slow&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Best Use Case&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;Early-stage predictive cases, quick baselines&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Mature pipelines, high control and customization&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Flexibility&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;Limited tuning / plug-and-play&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Highly customizable&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Business Value&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;Immediate, fast, accessible&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Strong when optimized and scaled&lt;/TD&gt;&lt;/TR&gt;&lt;/TBODY&gt;&lt;/TABLE&gt;&lt;P&gt;This experiment highlights a simple truth: &lt;STRONG&gt;SAP RPT-1 isn’t here to replace traditional ML, it jump-starts it.&amp;nbsp;&lt;/STRONG&gt;With a pretrained, context-driven approach, RPT-1 delivers fast, reliable insights with very little data and almost no setup. Traditional models still excel in mature, data-rich scenarios, but RPT-1 shines as a rapid accelerator and early-value generator inside SAP landscapes.&lt;/P&gt;&lt;HR /&gt;&lt;H3 id="toc-hId-1958473942"&gt;&lt;STRONG&gt;&lt;span class="lia-unicode-emoji" title=":speech_balloon:"&gt;💬&lt;/span&gt;Open for Exchange&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;If you're testing RPT-1, exploring predictive cases, or want the full code, feel free to reach out.&lt;BR /&gt;&lt;STRONG&gt;Happy to connect, compare experiences, and push this topic forward together.&lt;/STRONG&gt;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/sap-rpt-1-context-model-vs-training-classical-models-the-models-battle/ba-p/14268507"/>
    <published>2025-11-20T07:50:27.670000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/link-sac-and-sap-dsp-tenants-via-formation-to-schedule-a-sac-story-based-on/ba-p/14272432</id>
    <title>Link SAC and SAP DSP tenants via Formation to Schedule a SAC Story Based on SAP DSP Live Connection</title>
    <updated>2025-11-20T11:21:03.095000+01:00</updated>
    <author>
      <name>rajniashok</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/597458</uri>
    </author>
    <content>&lt;H2 id="create-a-formation-to-integrate-with-sap-analytics-cloud" id="toc-hId-1765512617"&gt;&lt;SPAN&gt;Create a Formation to Integrate with&amp;nbsp;&lt;SPAN class=""&gt;SAP Analytics Cloud&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;/H2&gt;&lt;DIV class=""&gt;&lt;H3 id="prerequisites" id="toc-hId-1698081831"&gt;Prerequisites&lt;/H3&gt;&lt;/DIV&gt;&lt;UL class=""&gt;&lt;LI&gt;&lt;P class=""&gt;You are an SAP global account administrator, or you are a system landscape administrator.&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;&lt;P class=""&gt;&lt;SPAN class=""&gt;SAP Analytics Cloud&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;can run on Hana 2.0 or HANA Cloud.&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;&lt;P class=""&gt;Both&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;SAP Analytics Cloud&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;and&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;SAP Datasphere&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;tenants must be located in the same data center and deployed in the same landscape. SAP data centers are not in the scope.&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;&lt;P class=""&gt;Both&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;SAP Analytics Cloud&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;and&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;SAP Datasphere&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;tenants must use the same identity provider (IdP) and the same subject name identifier.&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;&lt;P class=""&gt;Ensure the relevant&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;SAP Analytics Cloud&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;and&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;SAP Datasphere&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;tenants or systems in your landscape are available to be integrated in a formation.&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;H3 id="context" id="toc-hId-1501568326"&gt;Context&lt;/H3&gt;&lt;/DIV&gt;A formation is created in SAP BTP cockpit in the&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;System Landscape&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;section of your global account. The formation identifies the systems that can connect with your&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;SAP Analytics Cloud&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;tenant.&lt;P class=""&gt;Formations enable the automated integration between a set of your provisioned SAP systems.&lt;/P&gt;&lt;/DIV&gt;&lt;DIV class=""&gt;&lt;H3 id="procedure" id="toc-hId-1305054821"&gt;Procedure&lt;/H3&gt;&lt;/DIV&gt;&lt;OL class=""&gt;&lt;LI&gt;&lt;SPAN class=""&gt;In the SAP BTP cockpit&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;System Landscape&lt;/SPAN&gt;&amp;nbsp;&amp;nbsp;&lt;SPAN class=""&gt;Systems&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;page, browse the systems in your customer system landscape.&lt;/SPAN&gt;&lt;DIV class=""&gt;Every&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;SAP Analytics Cloud&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;system and&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;SAP Datasphere&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;system you are going to include in this formation must not be apart of another formation. The system landscape features auto-discovered systems in the list.&lt;UL class=""&gt;&lt;LI&gt;&lt;DIV class=""&gt;&lt;STRONG&gt;Note&lt;/STRONG&gt;&lt;/DIV&gt;&lt;P&gt;if a given system is missing on the&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;Systems&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;page, it may be associated with a different customer ID on the SAP BTP global account you're working in. In this case you need to add the system manually, and then register it. For more information, see&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;A class="" title="https://help.sap.com/docs/btp/sap-business-technology-platform/registering-sap-system?version=Cloud" href="https://help.sap.com/docs/btp/sap-business-technology-platform/registering-sap-system?version=Cloud" target="_blank" rel="noopener noreferrer"&gt;Adding, Registering, and Deregistering Systems&lt;/A&gt;.&lt;/P&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;/DIV&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN class=""&gt;Go to&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;System Landscape&lt;/SPAN&gt;&amp;nbsp;&amp;nbsp;&lt;SPAN class=""&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;Formations&lt;/SPAN&gt;&lt;/SPAN&gt;. Select&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;Create Formation&lt;/SPAN&gt;.&lt;/SPAN&gt;&lt;OL class=""&gt;&lt;LI&gt;&lt;SPAN class=""&gt;In the&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;General Information&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;step, enter a descriptive formation name.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN class=""&gt;In the&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;Formation Type&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;drop-down menu, select&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;Integration with SAP Analytics Cloud&lt;/SPAN&gt;.&lt;/SPAN&gt;&lt;/LI&gt;&lt;/OL&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN class=""&gt;Select&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;Next Step&lt;/SPAN&gt;.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN class=""&gt;In the&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;Include Systems&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;step, select the&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;SAP Datasphere&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;tenant and&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;SAP Analytics Cloud&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;tenant you want to link with.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN class=""&gt;Select&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;Next Step&lt;/SPAN&gt;.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN class=""&gt;Review your selections and create the formation.&lt;/SPAN&gt;&lt;/LI&gt;&lt;/OL&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;H3 id="results-1" id="toc-hId-1108541316"&gt;Results&lt;/H3&gt;&lt;/DIV&gt;The formation appears under your list of formations and a&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;Ready&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;success message is shown.&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;DIV class=""&gt;&lt;H2 id="delete-a-formation-in-sap-btp-cockpit" id="toc-hId-782945092"&gt;&lt;SPAN&gt;Delete a Formation in SAP BTP Cockpit&lt;/SPAN&gt;&lt;/H2&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;H3 id="context-1" id="toc-hId-715514306"&gt;Context&lt;/H3&gt;&lt;/DIV&gt;When you delete a formation, the following happens. Both&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;SAP Analytics Cloud&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;and&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;SAP Datasphere&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;tenants will be excluded from the formation. Then, the subaccount is unassigned. Finally, the SAP Analytics Cloud formation is removed from the&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;Formations&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;page completely.&lt;/DIV&gt;&lt;DIV class=""&gt;&lt;H3 id="procedure-1" id="toc-hId-519000801"&gt;Procedure&lt;/H3&gt;&lt;/DIV&gt;&lt;OL class=""&gt;&lt;LI&gt;&lt;SPAN class=""&gt;In the SAP BTP cockpit go to&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;&lt;SPAN class=""&gt;System Landscape&lt;/SPAN&gt;&amp;nbsp;&amp;nbsp;&lt;SPAN class=""&gt;Formations&lt;/SPAN&gt;&lt;/SPAN&gt;.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN class=""&gt;Select the formation of&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;SAP Analytics Cloud&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;that you want to remove.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN class=""&gt;Select&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class=""&gt;Delete&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;at the top-right of the formation.&lt;/SPAN&gt;&lt;/LI&gt;&lt;/OL&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;DIV class=""&gt;&lt;H2 id="recreation-of-formation" id="toc-hId-193404577"&gt;&lt;SPAN&gt;Recreation of Formation&lt;/SPAN&gt;&lt;/H2&gt;&lt;DIV class=""&gt;&lt;P class=""&gt;When formation is recreated the scheduling feature will be enabled again. The functionalities will continue to work as they were before the deletion. For scheduling, UCL formation will only provide enablement of tunneled connections.&lt;/P&gt;&lt;P class=""&gt;As a next step to create Tunnel connection and perform scheduling of SAC story based on DSP live connectivity please follow the help document.&amp;nbsp;&lt;/P&gt;&lt;P class=""&gt;&lt;A href="https://help.sap.com/docs/SAP_ANALYTICS_CLOUD/18850a0e13944f53aa8a8b7c094ea29e/1c4c615f5d7f4b78a5ae681759a55650.html?version=2025.23" target="_blank" rel="noopener noreferrer"&gt;https://help.sap.com/docs/SAP_ANALYTICS_CLOUD/18850a0e13944f53aa8a8b7c094ea29e/1c4c615f5d7f4b78a5ae681759a55650.html?version=2025.23&lt;/A&gt;&lt;/P&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/LI&gt;&lt;/UL&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/link-sac-and-sap-dsp-tenants-via-formation-to-schedule-a-sac-story-based-on/ba-p/14272432"/>
    <published>2025-11-20T11:21:03.095000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/scheduling-sap-analytics-cloud-stories-based-on-sap-datasphere-live/ba-p/14273401</id>
    <title>Scheduling SAP Analytics Cloud Stories Based on SAP Datasphere Live Connections</title>
    <updated>2025-11-20T11:30:24.964000+01:00</updated>
    <author>
      <name>pranav_kumar11</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/223642</uri>
    </author>
    <content>&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;Overview&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;(Available Starting QRC4 2025)&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Scheduled publications are an important feature in SAP Analytics Cloud (SAC), enabling organizations to distribute stories at regular intervals for offline access over email email delivery.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;As SAP Datasphere continues to be the strategic data foundation for SAP customers, many organizations rely extensively on live connections between SAC and Datasphere. Starting with SAP Analytics Cloud QRC4 2025, customers can now schedule publications for stories built on SAP Datasphere live connections.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;This capability is supported for:&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;SAP Analytics cloud and SAP Datasphere on BDC&amp;nbsp;Tenant &lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Standalone (non-BDC) SAP Analytics Cloud and SAP Datasphere Tenant&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;Prerequisites&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;To schedule a story based on a Datasphere live connection, the story must use an SAP Datasphere Tunnel Connection.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;&lt;SPAN&gt;Creating a Datasphere Tunnel Connection&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;A Tunnel Connection can be created only when SAP Analytics Cloud and SAP Datasphere tenants are “deep linked.” Deep linking ensures seamless authentication, metadata access, and cross-system operations.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Deep linking configurations:&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Option 1 — In BDC tenant where SAP Analytics Cloud and SAP Datasphere tenants are linked via BDC formation &lt;STRONG&gt;(Recommended)&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Option 2 — In SAP Analytics Cloud tenant and SAP Datasphere tenant (Non BDC) are linked via UCL formation.&amp;nbsp;&amp;nbsp;&lt;STRONG&gt;(Recommended)&lt;/STRONG&gt; - Refer to blog&amp;nbsp;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/link-sac-and-sap-dsp-tenants-via-formation-to-schedule-a-sac-story-based-on/ba-p/14272432" target="_self"&gt;Link SAC and SAP DSP tenants via Formation to Schedule a SAC Story Based on SAP DSP Live Connection&lt;/A&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;SPAN&gt;Option 3 — SAP Analytics Cloud and SAP Datasphere Tenant are linked for seamless planning as below&amp;nbsp;&lt;BR /&gt;&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="pranav_kumar11_0-1763631064482.png" style="width: 607px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/343057iB661B3E346566BB3/image-dimensions/607x350/is-moderation-mode/true?v=v2" width="607" height="350" role="button" title="pranav_kumar11_0-1763631064482.png" alt="pranav_kumar11_0-1763631064482.png" /&gt;&lt;/span&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Once any one of the above configurations is done, In SAP Analytics Cloud tenant, you can create a SAP Datasphere Tunnel connection using below step&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;From the side navigation, choose&amp;nbsp;&lt;STRONG&gt;Connections&lt;/STRONG&gt;&amp;nbsp;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Click Add + Connection&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Expand&amp;nbsp;&lt;STRONG&gt;Connect to Live Data&lt;/STRONG&gt;&amp;nbsp;and select&amp;nbsp;&lt;STRONG&gt;SAP Datasphere&lt;/STRONG&gt;.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;In the dialog, enter a name and description for your connection.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;From the&amp;nbsp;&lt;STRONG&gt;Connection Type&lt;/STRONG&gt;&amp;nbsp;drop-down list, select &lt;STRONG&gt;Tunnel&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;UL class="lia-list-style-type-square"&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;SPAN&gt;Tunnel&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; connection type is mandatory if you plan to use this Scheduled Publications using the Datasphere Live connection. &lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;&lt;STRONG&gt;For tunnel connections,&lt;/STRONG&gt;&amp;nbsp;ensure both SAC and Datasphere users use the same Identity Provider (IdP)&lt;/SPAN&gt;&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="pranav_kumar11_2-1763632678188.png" style="width: 449px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/343086i8672EE2CBDC6BC1B/image-dimensions/449x428/is-moderation-mode/true?v=v2" width="449" height="428" role="button" title="pranav_kumar11_2-1763632678188.png" alt="pranav_kumar11_2-1763632678188.png" /&gt;&lt;/span&gt;&lt;SPAN&gt;&lt;STRONG&gt;&lt;BR /&gt;&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;SPAN&gt;&lt;STRONG&gt;Note:&lt;/STRONG&gt; In QRC4.2025, you have an option to create new Tunnel connection, however, you will not be able to convert existing SAP Datasphere Direct connection to Tunnel connection. However, if you are on Fast track tenant and SAP Analytics Cloud version &lt;STRONG&gt;2025.23 &lt;/STRONG&gt;or beyond then you can edit Direct connection to Tunnel Connection. If you are on QRC tenant, then this will be available in &lt;STRONG&gt;QRC1.2026&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;&lt;SPAN&gt;Scheduling a Story Based on a Datasphere Live Connection&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Once the Tunnel Connection is created and used in the story, it can be scheduled like any other SAC story, with support for email distribution, CSV/Excel/PDF/PowerPoint output, recurrence, dynamic time filters, and personalization.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="pranav_kumar11_3-1763633354352.png" style="width: 358px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/343094i7E4DB081E309B70A/image-dimensions/358x516/is-moderation-mode/true?v=v2" width="358" height="516" role="button" title="pranav_kumar11_3-1763633354352.png" alt="pranav_kumar11_3-1763633354352.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;&lt;SPAN&gt;Conclusion&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Starting QRC4 2025, this enhancement supports Scheduling Publication of SAP Analytics Cloud story capabilities for customers using SAP Datasphere with SAP Analytics Cloud.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/scheduling-sap-analytics-cloud-stories-based-on-sap-datasphere-live/ba-p/14273401"/>
    <published>2025-11-20T11:30:24.964000+01:00</published>
  </entry>
</feed>
