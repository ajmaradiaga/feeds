<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>https://raw.githubusercontent.com/ajmaradiaga/feeds/main/scmt/topics/Python-blog-posts.xml</id>
  <title>SAP Community - Python</title>
  <updated>2026-01-10T12:11:50.863466+00:00</updated>
  <link href="https://community.sap.com/t5/c-khhcw49343/Python/pd-p/f220d74d-56e2-487e-8e6c-a8cb3def2378" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <subtitle>Python blog posts in SAP Community</subtitle>
  <entry>
    <id>https://community.sap.com/t5/artificial-intelligence-blogs-posts/hands-on-tutorial-sap-databricks/ba-p/14156999</id>
    <title>Hands-on Tutorial: SAP Databricks</title>
    <updated>2025-08-04T09:17:31.648000+02:00</updated>
    <author>
      <name>AndreasForster</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/14188</uri>
    </author>
    <content>&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="000 logos white.png" style="width: 584px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291532iF242B7265DB0E43A/image-size/large?v=v2&amp;amp;px=999" role="button" title="000 logos white.png" alt="000 logos white.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;With &lt;A href="https://www.sap.com/products/data-cloud/databricks.html" target="_blank" rel="noopener noreferrer"&gt;SAP Databricks&lt;/A&gt; we now have a dedicated environment for Data Scientists within the SAP Business Data Cloud. This blog gives a practical introduction to this bespoke Databricks edition by implementing a bare bones demand forecast.&amp;nbsp;SAP Databricks includes important functionality beyond what is explained in this entry-level tutorial., for example data sharing, experiment tracking or AutoML.&lt;BR /&gt;&lt;BR /&gt;The integration of SAP Databricks is adding a new option to the SAP Business Technology Platform (BTP), giving customers more choice when creating custom extensions or applications. Existing components on the BTP, which you might already be familiar with, remain very relevant and strategic, such as:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;A href="https://www.sap.com/suisse/products/artificial-intelligence/ai-core.html" target="_self" rel="noopener noreferrer"&gt;SAP AI Core&lt;/A&gt;, which provides for example access to a &lt;A href="https://me.sap.com/notes/3437766" target="_self" rel="noopener noreferrer"&gt;long list of Large Language Models&lt;/A&gt; through its &lt;A href="https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/generative-ai-hub-in-sap-ai-core-7db524ee75e74bf8b50c167951fe34a5" target="_self" rel="noopener noreferrer"&gt;Generative AI Hub.&lt;/A&gt;&amp;nbsp;Some of the models are even hosted on SAP's own physical infrastructure, giving increased security (currently Mistral and Aleph Alpha).&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://www.sap.com/products/artificial-intelligence/ai-foundation-os/document-ai.html" target="_self" rel="noopener noreferrer"&gt;SAP Document AI&lt;/A&gt;, which extracts information from documents such as PDFs, Excel, images, ...&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://www.sap.com/products/data-cloud/hana.html" target="_self" rel="noopener noreferrer"&gt;SAP HANA Cloud&lt;/A&gt; and &lt;A href="https://www.sap.com/products/data-cloud/datasphere.html" target="_self" rel="noopener noreferrer"&gt;SAP Datasphere&lt;/A&gt;, with their built-in multi-model capabilities, ie Machine Learning, embeddings generation for Text, Vector engine, Graph engine, Geospatial&amp;nbsp;&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://www.sap.com/products/artificial-intelligence/joule-studio.html" target="_self" rel="noopener noreferrer"&gt;SAP Build, Joule Studio&lt;/A&gt; for adding new skills to our digital assistant Joule, and the ability to create new AI agents on the roadmap. This &lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/how-sap-s-ai-agent-architecture-enables-unprecedented-automation-and/ba-p/14158296" target="_self"&gt;blog&lt;/A&gt;&amp;nbsp;(series) on SAP's AI Agent Architecture by our CTO and Chief AI Officer&amp;nbsp;Philipp Herzig&amp;nbsp;&lt;SPAN&gt;gives an excellent overview of the wider picture.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;SAP Databricks can also be used as development environment for the above components. &lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/unlocking-sap-ai-foundation-capabilities-in-sap-databricks-a-technical-deep/ba-p/14162430" target="_self"&gt;This blog&lt;/A&gt; by &lt;a href="https://community.sap.com/t5/user/viewprofilepage/user-id/434167"&gt;@san_tran&lt;/a&gt;&amp;nbsp;for instance shows a SAP Databricks project that integrates with Large Language Models on SAP AI Core.&amp;nbsp;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;And remember to check on the AI/ML functionality that has already been built into SAP standard applications, or what is in the pipeline to be released. Both released and planned functionality is shown in the &lt;A href="https://roadmaps.sap.com/board?FT=AI&amp;amp;FT=GEN_AI&amp;amp;range=FIRST-LAST" target="_self" rel="noopener noreferrer"&gt;Roadmap Explorer&lt;/A&gt;.&lt;/P&gt;&lt;P&gt;&lt;FONT color="#FF0000"&gt;Note: All data, code and images that are used in this blog can be downloaded from this repository: &lt;A href="https://github.com/SAP-samples/mee-samples/tree/main/Hands-on%20Tutorial%20SAP%20Databricks" target="_self" rel="nofollow noopener noreferrer"&gt;Hands-on Tutorial SAP Databricks&lt;/A&gt;. In that repository you find the individual files but also the whole project exported as Databricks Archive.&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;Before we get going, big thanks to Stojan Maleschlijski for all the great collaboration, including exploring the SAP Databricks capabilities together!&amp;nbsp;&lt;a href="https://community.sap.com/t5/user/viewprofilepage/user-id/39047"&gt;@stojanm&lt;/a&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="toc-hId-1606077867"&gt;Table of contents&lt;/H1&gt;&lt;UL&gt;&lt;LI&gt;&lt;A href="#background" target="_self" rel="nofollow noopener noreferrer"&gt;Use Case&amp;nbsp;&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="#architecture" target="_self" rel="nofollow noopener noreferrer"&gt;Architecture&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="#prerequisites" target="_self" rel="nofollow noopener noreferrer"&gt;Prerequisites&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="#uploaddata" target="_self" rel="nofollow noopener noreferrer"&gt;Upload data&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="#eda" target="_self" rel="nofollow noopener noreferrer"&gt;Explore the data&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="#forecast" target="_self" rel="nofollow noopener noreferrer"&gt;Create a forecast with SAP Databricks&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="#schedule" target="_self" rel="nofollow noopener noreferrer"&gt;Schedule a forecast with SAP Databricks&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="#goodtoknow" target="_self" rel="nofollow noopener noreferrer"&gt;Good to know&lt;/A&gt;&lt;UL&gt;&lt;LI&gt;Base file&lt;/LI&gt;&lt;LI&gt;Lineage &amp;amp; Table usage insights&lt;/LI&gt;&lt;LI&gt;Visuals&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="#summary" target="_self" rel="nofollow noopener noreferrer"&gt;Summary&lt;/A&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="background" id="toc-hId-1409564362"&gt;Use Case&lt;/H1&gt;&lt;P&gt;This blog aims to give a first introduction for carrying out a Machine Learning project on SAP Databricks. Trying to simulate a demand forecast, we will predict how many nights people spend in a hotel in Switzerland. We use some granular data that is kindly shared by the &lt;A href="https://opendata.swiss/de/dataset/hotellerie-ankunfte-und-logiernachte-der-geoffneten-betriebe-nach-jahr-monat-tourismusregion-un46/resource/dc676b65-69dc-437c-b0f0-4ef703be9ae2" target="_self" rel="nofollow noopener noreferrer"&gt;Swiss Statistics department&lt;/A&gt;.&lt;/P&gt;&lt;P&gt;A Data Scientist (you) creates a monthly forecast and sets up a schedule for the forecast to be updated every month.&amp;nbsp; This data would then become part of a business process, maybe you want to provide the data to business users in a dashboard.&amp;nbsp;This tutorial however focusses solely on creating and scheduling the forecast in SAP Databricks.&lt;/P&gt;&lt;P&gt;It's a simple time-series forecasting example, but the concept is still extremely relevant for so many different business requirements. Instead of forecasting how many visitors are staying overnight, you might need to forecast your sales quantities, your cash flow or even for topics that don't come immediately to mind. One customer for instance is using time-series forecasting to improve the data quality of external data that is loaded into the system. If a new value is outside the predicted range, there might be a data quality issue and IT can follow up, whether the data is correct or whether indeed a data quality issue cropped up.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="architecture" id="toc-hId-1213050857"&gt;Architecture&lt;/H1&gt;&lt;P&gt;In a &lt;STRONG&gt;productive&lt;/STRONG&gt; system a typical architecture and data flow would look like this.&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;A Data Product is pushed into the Business Data Cloud Object Store&lt;/LI&gt;&lt;LI&gt;This Data Product is shared with SAP Databricks&lt;/LI&gt;&lt;LI&gt;SAP Databricks creates and saves a forecast as DeltaTable into the Object Store&lt;/LI&gt;&lt;LI&gt;This table is registered as Custom Data Product in the Business Data Cloud&lt;/LI&gt;&lt;LI&gt;SAP Datasphere installs the Custom Data Product, making it accessible for further data modelling and visualisation in SAP Analytics Cloud&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="100 architecture prod.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291617iCD8EC574E5DB872A/image-size/large?v=v2&amp;amp;px=999" role="button" title="100 architecture prod.png" alt="100 architecture prod.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;The architecture of this hands-on exercise is focused purely on getting some familiarity with SAP Databricks. The only interface we will be using is SAP Databricks. We use it to upload the data and to go through the steps of a Machine Learning project.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="114 hands-on architecture.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291641i1B8E5C353594A19C/image-size/large?v=v2&amp;amp;px=999" role="button" title="114 hands-on architecture.png" alt="114 hands-on architecture.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="prerequisites" id="toc-hId-1016537352"&gt;Prerequisites&lt;/H1&gt;&lt;P&gt;Getting started is pretty easy.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;You will just need to have access to an instance of SAP Databricks. This could be the &lt;A href="https://www.sap.com/products/data-cloud/trial.html" target="_self" rel="noopener noreferrer"&gt;SAP Business Data Cloud trial&lt;/A&gt;.&lt;/LI&gt;&lt;LI&gt;Some familiarity with SQL and especially Python would be very useful.&amp;nbsp;&lt;/LI&gt;&lt;LI&gt;And maybe just some curiosity to try something new.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="uploaddata" id="toc-hId-820023847"&gt;Upload data&lt;/H1&gt;&lt;P&gt;Start by bringing the historic data, on which we want to train a model, into the system. Upload the dataset &lt;A href="https://github.com/SAP-samples/mee-samples/blob/main/Hands-on%20Tutorial%20SAP%20Databricks/OVERNIGHTSTAYS.csv" target="_self" rel="nofollow noopener noreferrer"&gt;OVERNIGHTSTAYS.csv&lt;/A&gt; with the Graphical User Interface of SAP Databricks. Follow the steps shown in this screencam. In case that you are working with the trial, then you need to change the Catalog drop-down as shown to "workspace".&lt;/P&gt;&lt;P&gt;&lt;EM&gt;HINT: In case the upload fails with the message "Table with same name already exists", then another user has already created the table in this shared environment. Due to access rights you won't be able to see that other user's table. Just change the table name&amp;nbsp; in your upload to something unique and remember to use this name when accessing the table further on.&lt;BR /&gt;&lt;/EM&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="010 data upload.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291462i85AFDC42927C8AE0/image-size/large?v=v2&amp;amp;px=999" role="button" title="010 data upload.gif" alt="010 data upload.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;The data is uploaded. it is showing in the Catalog as (Delta)Table. Physically it is stored in the Object Store. The table's Overview tab lists the column names and their data types:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="200 table.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291821i2442B47235562A8F/image-size/large?v=v2&amp;amp;px=999" role="button" title="200 table.png" alt="200 table.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Move to the table's "Sample Data" tab to see some of the uploaded rows.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="210 sample data.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291822i8812DF688B48DC5A/image-size/large?v=v2&amp;amp;px=999" role="button" title="210 sample data.png" alt="210 sample data.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;The first lines shows that in January 2022 there were 392.805 nights spent by Swiss residents in the area of Graubünden. In case you are not that familiar with Swiss geography, Graubünden is the largest Canton in Switzerland, it's in the Alps and includes beautiful places like St. Moritz and Davos.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="eda" id="toc-hId-623510342"&gt;Explore the data&amp;nbsp;&lt;/H1&gt;&lt;P&gt;You have two options to explore the data in SAP Databricks.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Either query the DeltaTable directly with SQL&lt;/LI&gt;&lt;LI&gt;Or use a Notebook, in which you can use both SQL or Python&lt;/LI&gt;&lt;/UL&gt;&lt;H2 id="toc-hId-556079556"&gt;&lt;FONT color="#000000"&gt;Data exploration with the SQL Editor&lt;/FONT&gt;&lt;/H2&gt;&lt;P&gt;Let's look closer into the data. You can start with some SQL statements. You can go into the SQL Editor and type in your own syntax. The screencam shows two simple examples. We learn for instance that we have data from January 2022 to May 2025 to work with.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;SELECT * from overnightstays&lt;/LI&gt;&lt;LI&gt;SELECT min(MONTH), max(month) from overnightstays&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="1 SQL Editor.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291823i07AAB646DC970612/image-size/large?v=v2&amp;amp;px=999" role="button" title="1 SQL Editor.gif" alt="1 SQL Editor.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Or describe what you are looking for and have SAP Databrick's write the SQL for you! Click that small red-ish star icon to toggle on the &lt;A href="https://www.databricks.com/product/databricks-assistant" target="_self" rel="nofollow noopener noreferrer"&gt;Databricks Assistant&lt;/A&gt;.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="230 assistant toggle.png" style="width: 793px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291825i58D63393E550F4C2/image-size/large?v=v2&amp;amp;px=999" role="button" title="230 assistant toggle.png" alt="230 assistant toggle.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Just try out what you are interested in. I was wondering for example:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;which regions are in overnightstays?&lt;/LI&gt;&lt;LI&gt;use table overnightstays to determine how many overnight stays were they by region, sort the results&lt;/LI&gt;&lt;LI&gt;use table overnightstays to summarise overnightstays by countryofresidence&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="2 sql assistant.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291827i03309ED58F155C9F/image-size/large?v=v2&amp;amp;px=999" role="button" title="2 sql assistant.gif" alt="2 sql assistant.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;It turns out that most overnight stays in Switzerland are by Swiss residents, followed by German residents. Maybe surprisingly residents from the United States are a close third.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="240 overnight by countryofresidence.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291828iAF52C2350418D6FE/image-size/large?v=v2&amp;amp;px=999" role="button" title="240 overnight by countryofresidence.png" alt="240 overnight by countryofresidence.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-359566051"&gt;&lt;FONT color="#000000"&gt;Data exploration with a Notebook&lt;/FONT&gt;&lt;/H2&gt;&lt;P&gt;&lt;FONT color="#000000"&gt;Now continue the data exploration in a Notebook. &lt;A href="https://docs.databricks.com/aws/en/notebooks/" target="_self" rel="nofollow noopener noreferrer"&gt;Databricks Notebooks&lt;/A&gt; are quite special in that they can contain more than one scripting language. SAP Databricks supports both SQL as well as Python. The Notebook created in this section can also be downloaded from the &lt;A href="https://github.com/SAP-samples/mee-samples/tree/main/Hands-on%20Tutorial%20SAP%20Databricks" target="_self" rel="nofollow noopener noreferrer"&gt;repository&lt;/A&gt;.&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT color="#000000"&gt;Begin by creating a folder in which we will save our Notebooks. Go into the "Workspace" section, which is where such files are kept together. Within your user's workspace create a folder called "Demand forecast".&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT color="#000000"&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="250 create folder.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291832iC5DC43480B5680ED/image-size/large?v=v2&amp;amp;px=999" role="button" title="250 create folder.gif" alt="250 create folder.gif" /&gt;&lt;/span&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;FONT color="#000000"&gt;In that new folder create a first Notebook as shown in the next screencam. Rename it to "010 Data exploration". And add a short header in Markdown at the top. Markdown is a common way to add comments and context to the code in a Notebook. Adding "#" character at the beginning of a line formats the text as top-level heading. Two&amp;nbsp;"#" characters make it a second-level heading, and so on. Without any&amp;nbsp;"#" character the following text is formatted just normally as plain text.&amp;nbsp;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT color="#000000"&gt;To add the Markdown, you need to change the cell's language selector to "Markdown" as shown in the screencam. Enter your text. When done, click outside the cell and the formatting kicks in.&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT color="#000000"&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="260 create notebook.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291833i095F807AD2D1B795/image-size/large?v=v2&amp;amp;px=999" role="button" title="260 create notebook.gif" alt="260 create notebook.gif" /&gt;&lt;/span&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Since the cells can also execute SQL code, add the most recent SQL statement from the SQL Editor.&lt;/P&gt;&lt;pre class="lia-code-sample language-sql"&gt;&lt;code&gt;SELECT
  COUNTRYOFRESIDENCE,
  SUM(OVERNIGHTSTAYS) AS total_overnightstays
FROM
  workspace.default.overnightstays
GROUP BY
  COUNTRYOFRESIDENCE
ORDER BY
  total_overnightstays DESC&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;FONT color="#000000"&gt;The screencam shows how a new cell can be added to the Notebook, by hovering with the mouse at the bottom of the cell above. Select "Code" as cell type. Change the cell's language selector to "SQL", paste the code and run it with the blue play button. You will see the same result as before in the "SQL Editor".&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT color="#000000"&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="270 notebook with sql.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291834iD2FD9AF968D6F360/image-size/large?v=v2&amp;amp;px=999" role="button" title="270 notebook with sql.gif" alt="270 notebook with sql.gif" /&gt;&lt;/span&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Let's switch to Python for the data exploration so that you have tried all the options. Create a new Code cell, its language selector might already be on "Python" by default. Begin by loading the data into a PySpark DataFrame using the table's fully qualified path.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;overnightstays_sdf = spark.read.table("workspace.default.overnightstays") &lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Aggregate the data by month and look at the results. The display command initially shows the data as table, but it also comes with a Graphical User interface to quickly create a plot. Follow the steps in the screencam to create a line chart, to see how the numbers have evolved over time. There is a clear pattern, that especially in July and August the numbers are at their highest.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import pyspark.sql.functions as F
display(overnightstays_sdf.groupBy("month").agg(F.sum("overnightstays").alias("overnightstays")))&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="280 display chart.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292292iF1C7C59E9B75F071/image-size/large?v=v2&amp;amp;px=999" role="button" title="280 display chart.gif" alt="280 display chart.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Now explore, whether we can visually get a feel for an overall trend in the data, whether the numbers tend to go up or down over time. Have the Assistant do the work of writing the code with this request:&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Create a new DataFrame that has the year in a new column. aggregate by year and show the result in a plotly chart&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Plotly is a very common Python charting library. And indeed, when looking at the yearly totals, the numbers are increasing for the years.&amp;nbsp; The year 2025 is not meaningful yet in this chart as the dataset only contains January to May for that year.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="290 yearly totals.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292297i05056079B6F0B3FE/image-size/large?v=v2&amp;amp;px=999" role="button" title="290 yearly totals.gif" alt="290 yearly totals.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="forecast" id="toc-hId-33969827"&gt;Create a forecast with SAP Databricks&lt;/H1&gt;&lt;P&gt;During the above data exploration we saw that the data has a trend (numbers are increasing over time) and some seasonality (ie numbers are largest in the summer). Let's train a Machine Learning that picks up on such patterns and can estimate future values.&amp;nbsp;&lt;FONT color="#000000"&gt;The Notebook created in this section can also be downloaded from the &lt;A href="https://github.com/SAP-samples/mee-samples/tree/main/Hands-on%20Tutorial%20SAP%20Databricks" target="_self" rel="nofollow noopener noreferrer"&gt;repository&lt;/A&gt;.&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;In case you are unsure about how to implement the following 4 code blocks in a new notebook called "020 Demand forecast", then this screencam will guide you along. It shows in a quick scroll through what the output should look like.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="400 forecast prep.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292310iC793B83B80A6B323/image-size/large?v=v2&amp;amp;px=999" role="button" title="400 forecast prep.gif" alt="400 forecast prep.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Read the historic data again into a PySpark DataFrame.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;overnightstays_sdf = spark.read.table("workspace.default.overnightstays") &lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Since we want to create a monthly forecast on the total values, aggregate the history by month. Since we are working with the PySpark DataFrame, the built-in Spark engine is doing the work.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import pyspark.sql.functions as F
overnightstaysmonthly_sdf = overnightstays_sdf.groupBy("month").agg(F.sum("overnightstays").alias("overnightstays"))
display(overnightstaysmonthly_sdf)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;The data needs to be prepared further, so that the Python package called Prophet can train a time-series model on it.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# Convert Spark DataFrame to Pandas DataFrame (the time series algorithm requires a Pandas DataFrame)
overnightstaysmonthly_df = overnightstaysmonthly_sdf.toPandas()

# Sort the DataFrame by date
overnightstaysmonthly_df = overnightstaysmonthly_df.sort_values('month')

# Rename the columns, as required by the time-series algoritm Prophet
overnightstaysmonthly_df = overnightstaysmonthly_df.rename(columns={'month': 'ds', 'overnightstays': 'y'})&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;We want to use the Prophet package for the forecast, which still needs to be installed. Here we install it directly from the Notebook. However, in the "Good to know" section below you see a more elegant way of using additional Python package, by creating your own customised Base environment.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;pip install prophet==1.1.7&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Everything is now in place to finally train the time-series model. The following block of code contains multiple steps. It is useful to run these steps together under the "with mlflow.start_run() section as this allows to log information about the training run in SAP Databricks.&amp;nbsp; The following code:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Trains the time series model&lt;/LI&gt;&lt;LI&gt;Creates a DataFrame with the future 12 months that are to be forecasted&lt;/LI&gt;&lt;LI&gt;Creates a forecast for the known past and the future 12 months&lt;/LI&gt;&lt;LI&gt;Calculates&amp;nbsp; and logs the model's accuracy on the known history&lt;/LI&gt;&lt;LI&gt;Plots and logs the known history against the predicted values&lt;/LI&gt;&lt;LI&gt;Logs the number of records that were used during training&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;Yes, in a real project this code would be even more elaborate. For example you may want to try different model configurations to get even better forecast accuracy. And the accuracy should ideally be calculated on a hold-out sample. Currently the code uses the same data for training as well as for checking the model's accuracy. The accuracy should really be calculated on data the model has never seen before.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;EM&gt;HINT: In case the following code gives the error "MLflow not available", please check in the Environment settings on the right, that the "Environment version" is set to 2. These environments are explained a bit further down in the "Good to know" section further below. For some people it defaults to version 1 (which doesn't include the mlflow library), for others it defaults to the required version 2.&lt;/EM&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import pandas as pd
from prophet import Prophet
from sklearn.metrics import mean_absolute_percentage_error
import matplotlib.pyplot as plt
import mlflow, mlflow.tracking._model_registry.utils

mlflow.tracking._model_registry.utils._get_registry_uri_from_spark_session = lambda: "databricks-uc"

with mlflow.start_run():

    # Initialize the Prophet model
    model = Prophet()

    # Fit the model
    model.fit(overnightstaysmonthly_df)

    # Create a DataFrame that contains all dates for which a prediction is required, the future 12 months but also the known past for comparison
    datestopredict_df = model.make_future_dataframe(periods=12, freq='MS')

    # Forecast the future and known past
    forecast_df = model.predict(datestopredict_df)

    # Plot the predictions together with known past
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.plot(forecast_df['ds'], forecast_df['yhat'], label='Forecast', color='red')
    ax.plot(overnightstaysmonthly_df['ds'], overnightstaysmonthly_df['y'], label='Historical Data')
    ax.fill_between(forecast_df['ds'], forecast_df['yhat_lower'], forecast_df['yhat_upper'], color='red', alpha=0.3)
    ax.set_title('Demand forecast')
    ax.set_xlabel('Month')
    ax.set_ylabel('Total Overnightstays')
    ax.legend()
    ax.grid(True)
    plt.show()

    # Calculate and log model accuracy, here MAPE on training data
    overnightstaysmonthly_df[["ds"]] = overnightstaysmonthly_df[["ds"]].apply(pd.to_datetime)
    anctualsandpredicted_df = overnightstaysmonthly_df[['ds', 'y']].merge(forecast_df[['ds', 'yhat']], on='ds', how='inner', suffixes=('_left', '_right'))
    prophet_mape = mean_absolute_percentage_error(anctualsandpredicted_df['y'], anctualsandpredicted_df['yhat'])
    mlflow.log_metric("mape", prophet_mape)

    # Log the chart in the Unity Catalog / Experiment
    mlflow.log_figure(fig, "Forecast.png") 

    # Log the size of the training dataset
    mlflow.log_metric("rowcount_training", overnightstaysmonthly_df.shape[0])&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Run the above code and we see our prediction!&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="410 forecast.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292311i9103A5F02A8E234B/image-size/large?v=v2&amp;amp;px=999" role="button" title="410 forecast.gif" alt="410 forecast.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;To persist the forecast, save it as DeltaTable. From here the data can be made available to SAP Datasphere and SAP Analytics Cloud. That part is outside the scope of this tutorial, please check the &lt;A href="https://help.sap.com/docs/SAP_BUSINESS_DATA_CLOUD/3708ee482fc441ef8bb91711b1629109/c4464c041dec43b58a32501c6a6dda3a.html?locale=en-US" target="_self" rel="noopener noreferrer"&gt;documentation&lt;/A&gt;.&lt;/P&gt;&lt;P&gt;&lt;EM&gt;HINT: In case the saving fails with the message "PERMISSION_DENIED", then another user has already created that table and you are not allowed to overwrite it. Just change the name of the table you are creating to proceed.&amp;nbsp;&lt;/EM&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;forecast_sdf = spark.createDataFrame(forecast_df)
forecast_sdf.write.mode("overwrite").saveAsTable("workspace.default.overnightstays_forecast")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;The forecast is saved as DeltaTable, that means you can see it in the Catalog!&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="420 forecast in catalog.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292312iF8375CFED009E0F8/image-size/large?v=v2&amp;amp;px=999" role="button" title="420 forecast in catalog.gif" alt="420 forecast in catalog.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;The information that was logged during the training is now available in the Experiments section. The model's MAPE (Median Absolute Percentage Error) is at 1.4%, 41 records were used during training and we can see the chart that compares actuals to predictions. Especially when trying out different settings for the Machine Learning algorithm, this tracking becomes very useful, for instance to select which configuration to use for the prediction.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="430 catalog.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292313i0DFA2CC5C05A03F6/image-size/large?v=v2&amp;amp;px=999" role="button" title="430 catalog.gif" alt="430 catalog.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="schedule" id="toc-hId--162543678"&gt;Schedule&lt;/H1&gt;&lt;P&gt;Everything we need for our demand forecast is implemented and we can run it manually. That means we are also good to have it automated with a schedule. Straight from the notebook you can define the scheduling logic, ie the recurring interval and whether you want any notifications. The notebooks can also be triggered through APIs if you prefer.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="500 schedule.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292322i2DBBC49C66338C25/image-size/large?v=v2&amp;amp;px=999" role="button" title="500 schedule.gif" alt="500 schedule.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="goodtoknow" id="toc-hId-410682900"&gt;Good to know&lt;/H1&gt;&lt;P&gt;A few things that might be good to know.&lt;/P&gt;&lt;H2 id="toc-hId--79233612"&gt;&amp;nbsp;&lt;/H2&gt;&lt;H2 id="toc-hId--275747117"&gt;Base file&lt;/H2&gt;&lt;P&gt;In the above code the Python package Prophet is installed directly from within a Notebook. That's ok, but not ideal for ongoing use. If you were repeatedly re-running all cells from the Notebook, then Python will try to re-install the package every time. It's generally better to centrally specify the packages you know you will be using long term. This also ensures consistency across projects and developers. In SAP Databricks you can specify reusable / shareable lists of Python packages that are to be installed through Base environments.&lt;/P&gt;&lt;P&gt;Create a Base environment by creating a file called&amp;nbsp;base_env_prophet.yaml with this content:&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;dependencies:
  - prophet==1.1.7&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="600 create base env file.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292444i2F3BB0DE70432780/image-size/large?v=v2&amp;amp;px=999" role="button" title="600 create base env file.gif" alt="600 create base env file.gif" /&gt;&lt;/span&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;With this base file in place, you can&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Step 1: Remove the pip install command from the "020 Demand forecast" Notebook&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="660 1 Delete cells.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292457iE2926E77E522CDA1/image-size/large?v=v2&amp;amp;px=999" role="button" title="660 1 Delete cells.gif" alt="660 1 Delete cells.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Step 2: Specify that the Notebook will use the base file instead, to determine which packages to install. Here you can also select the Environment version. These environment versions are documented&amp;nbsp;&lt;A href="https://docs.databricks.com/aws/en/release-notes/serverless/environment-version/" target="_self" rel="nofollow noopener noreferrer"&gt;here&lt;/A&gt;. After selecting a different base file or after changing the environment version you need hit "Apply" and confirm for the change to take effect.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="660 2 Apply base file.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292459i632B6A73035CB9CF/image-size/large?v=v2&amp;amp;px=999" role="button" title="660 2 Apply base file.gif" alt="660 2 Apply base file.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;To test out that the Package is correctly installed through the base file, clear the environment as shown in the screencam and run the Python code in the cells.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="620 run with base file.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292470i7C64B17F50E7A505/image-size/large?v=v2&amp;amp;px=999" role="button" title="620 run with base file.gif" alt="620 run with base file.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId--472260622"&gt;Lineage &amp;amp; Table usage insights&lt;/H2&gt;&lt;P&gt;Now that the table "overnightstays" has been used a few times, check out the lineage, which shows where the table is used and the statistics on how heavily that table is used.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="550 lineage and insights.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292347iEE1C9D6942221C23/image-size/large?v=v2&amp;amp;px=999" role="button" title="550 lineage and insights.gif" alt="550 lineage and insights.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId--668774127"&gt;Visuals&lt;/H2&gt;&lt;P&gt;From the distance most Notebooks look very much alike at first glance. I heard some great feedback that some visual / image on top of the notebook would help to differentiate. In the repository you find &lt;A href="https://github.com/SAP-samples/mee-samples/tree/main/Hands-on%20Tutorial%20SAP%20Databricks/Images" target="_self" rel="nofollow noopener noreferrer"&gt;a few examples&lt;/A&gt;, in case you would like to use those to distinguish between Data Exploration, AI/ML Sandboxing and AI/ML Deployment.&lt;/P&gt;&lt;P&gt;Upload any images that you want to use into the Workspace. To keep things tidy I am creating a separate folder for them.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="650 image upload.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292681i472440DD7D4505B1/image-size/large?v=v2&amp;amp;px=999" role="button" title="650 image upload.gif" alt="650 image upload.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;These images can then be shown in a Markup cell.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="660 image display.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292683i5E97FE69DBC59287/image-size/large?v=v2&amp;amp;px=999" role="button" title="660 image display.gif" alt="660 image display.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="summary" id="toc-hId--571884625"&gt;Summary&lt;/H1&gt;&lt;P&gt;SAP Databricks is adding a dedicated Data Science environment to the SAP landscape. After this hands-on experience you hopefully have a first feel for how a Data Scientist can enrich the data in SAP Business Data Cloud with Machine Learning.&lt;/P&gt;&lt;P&gt;For your own projects, you will want to check out the documentation:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;A href="https://help.sap.com/docs/business-data-cloud/sap-databricks/introducing-sap-databricks" target="_self" rel="noopener noreferrer"&gt;SAP Databricks documentation by SAP&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://docs.databricks.com/sap/en/" target="_self" rel="nofollow noopener noreferrer"&gt;SAP Databricks documentation by Databricks&lt;/A&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;I hope you enjoyed getting hands-on with SAP Databricks!&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/artificial-intelligence-blogs-posts/hands-on-tutorial-sap-databricks/ba-p/14156999"/>
    <published>2025-08-04T09:17:31.648000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-4/ba-p/14174201</id>
    <title>SAP Databricks: Building an Intelligent Enterprise with AI Unleashed – Part 4</title>
    <updated>2025-08-07T08:54:53.397000+02:00</updated>
    <author>
      <name>jing_wen</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1923466</uri>
    </author>
    <content>&lt;P&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-1/ba-p/14166813" target="_self"&gt;&lt;SPAN&gt;Part 1 – SQL analytics with SAP Data Products&lt;BR /&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-2/ba-p/14167025" target="_self"&gt;Part 2 – Build and deploy Mosaic AI and Agent Tools&lt;/A&gt;&lt;BR /&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-how-to-use-automl-to-forecast-sales-data-part-3/ba-p/14174354" target="_self"&gt;Part 3 – How to use AutoML to forecast sales data&lt;/A&gt;&lt;BR /&gt;&lt;/SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-3/ba-p/14174201" target="_self"&gt;&lt;SPAN&gt;Part 4 – Connect SAP Data Products with non-SAP data from AWS S3&lt;BR /&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-4/ba-p/14178056" target="_self"&gt;Part 5 – End-to-end integration: SAP Databricks, SAP Datasphere, and SAP Analytics Cloud&lt;/A&gt;&lt;BR /&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-create-inferences-for-application-integration-with-sap-build/ba-p/14186662" target="_self"&gt;Part 6 – Create inferences and endpoints for application integration with SAP Build&lt;/A&gt;&lt;/SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-create-inferences-for-application-integration-with-sap-build/ba-p/14186662" target="_self"&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/A&gt;&lt;BR /&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-in-sap-business-data-cloud-a-typical-machine-learning/ba-p/14206612" target="_self"&gt;Part 7&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;–&amp;nbsp;SAP Databricks in SAP Business Data Cloud - A Typical Machine Learning Workflow&lt;/SPAN&gt;&lt;/A&gt;&lt;/P&gt;&lt;H3 id="toc-hId-1865165611" id="toc-hId-1866023751"&gt;SAP Databricks in SAP Business Data Cloud&amp;nbsp;&lt;/H3&gt;&lt;P&gt;In today's data-driven world, gaining a 360-degree view of your enterprise requires combining data from all corners of your business. SAP systems house governed business data. Meanwhile, Amazon S3 often contains unstructured or external data—everything from web logs and partner data to machine learning features and fraud detection inputs.&lt;/P&gt;&lt;H3 id="toc-hId-1669510246"&gt;&lt;STRONG&gt;Step-by-Step Playbook: Connecting AWS S3 to SAP Databricks&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;In this part of our blog series, we’ll walk you through &lt;STRONG&gt;connecting SAP Databricks to an Amazon S3 bucket&lt;/STRONG&gt; — empowering you to unify data from diverse sources and accelerate your analytics and AI workflows.&lt;/P&gt;&lt;P&gt;By the end of this guide, you’ll have:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;A &lt;STRONG&gt;Unity Catalog external location&lt;/STRONG&gt; pointing to your S3 bucket&lt;/LI&gt;&lt;LI&gt;A &lt;STRONG&gt;registered table&lt;/STRONG&gt; in Unity Catalog&lt;/LI&gt;&lt;LI&gt;A &lt;STRONG&gt;ready-to-run SAP Databricks notebook&lt;/STRONG&gt; to analyze your unified data&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;We will be covering the following steps in this guide:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Step 1 – Create the External Location (AWS Quickstart)&lt;/LI&gt;&lt;LI&gt;Step 2 – Generate a Personal Access Token (PAT) in SAP Databricks&lt;/LI&gt;&lt;LI&gt;Step 3 – Identify (or create) the target S3 bucket&lt;/LI&gt;&lt;LI&gt;Step 4 – Configure the AWS IAM Role &amp;amp; Policies&lt;/LI&gt;&lt;LI&gt;Step 5 – Launch the AWS Quickstart CloudFormation Stack&lt;/LI&gt;&lt;LI&gt;Step 6 – Provide the PAT to the CloudFormation Stack&lt;/LI&gt;&lt;LI&gt;Step 7 – Verify CloudFormation Completion&lt;/LI&gt;&lt;LI&gt;Step 8 – Test the External Location connection in SAP Databricks&lt;/LI&gt;&lt;LI&gt;Step 9– Create a Unity Catalog table from the S3 data&lt;/LI&gt;&lt;LI&gt;Step 10 – Work with SAP Data Products and non-SAP data from S3 within the SAP Databricks Notebook environment&amp;nbsp;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;Let's dive in.&lt;/P&gt;&lt;H4 id="toc-hId-1602079460"&gt;&lt;STRONG&gt;Prerequisites&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;You must create the S3 bucket that you want to use as an external location before you create the external location object in&amp;nbsp;SAP Databricks. More details documented &lt;A href="https://docs.databricks.com/sap/en/external-locations" target="_blank" rel="noopener nofollow noreferrer"&gt;here&lt;/A&gt;:&lt;/LI&gt;&lt;/UL&gt;&lt;UL&gt;&lt;UL&gt;&lt;LI&gt;The AWS CloudFormation template supports only S3 buckets.&lt;/LI&gt;&lt;LI&gt;The name of an S3 bucket that you want users to read from and write to cannot use dot notation (for example,&amp;nbsp;incorrect.bucket.name.notation). For more bucket naming guidance, see the&amp;nbsp;&lt;A href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html" target="_blank" rel="noopener nofollow noreferrer"&gt;AWS bucket naming rules&lt;/A&gt;.&lt;/LI&gt;&lt;LI&gt;Avoid using a path in S3 that is already defined as an external location in another&amp;nbsp;Unity Catalog&amp;nbsp;metastore. You can safely read data in a single external S3 location from more than one metastore, but concurrent writes to the same S3 location from multiple metastores can lead to consistency issues.&lt;/LI&gt;&lt;/UL&gt;&lt;/UL&gt;&lt;UL&gt;&lt;LI&gt;If you don't use the AWS CloudFormation template to create the external location, you must first create a storage credential in&amp;nbsp;SAP Databricks&amp;nbsp;that gives access to the cloud storage location path. See the following AWS doc:&amp;nbsp;&lt;A href="https://docs.databricks.com/aws/connect/unity-catalog/cloud-storage/storage-credentials" target="_blank" rel="noopener nofollow noreferrer"&gt;Create a storage credential&lt;/A&gt;.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;If you use the AWS CloudFormation flow, that storage credential is created for you.&lt;/P&gt;&lt;H4 id="toc-hId-1405565955"&gt;&lt;STRONG&gt;Permissions requirements&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;You must have the&amp;nbsp;&lt;STRONG&gt;CREATE EXTERNAL LOCATION&lt;/STRONG&gt;&amp;nbsp;privilege on both the metastore and the storage credential referenced in the external location. Metastore admins have&amp;nbsp;CREATE EXTERNAL LOCATION&amp;nbsp;on the metastore by default.&lt;/LI&gt;&lt;LI&gt;If you are using the AWS CloudFormation template, you must also have the&amp;nbsp;&lt;STRONG&gt;CREATE STORAGE CREDENTIAL&amp;nbsp;&lt;/STRONG&gt;privilege on the metastore. Metastore admins have&amp;nbsp;CREATE STORAGE CREDENTIAL&amp;nbsp;on the metastore by default.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;This guide assumes you're using the &lt;STRONG&gt;SAP Databricks Quickstart on AWS&lt;/STRONG&gt;, which automates much of the setup. Advanced users may opt for a manual setup with your IAM role (ARN), including the encryption algorithm of SSE-S3 or SSE-KMS.&lt;/P&gt;&lt;H4 id="toc-hId-1209052450"&gt;&lt;STRONG&gt;Step 1 – Create the External Location&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;In&amp;nbsp;&lt;STRONG&gt;SAP Databricks Unity Catalog,&amp;nbsp;&lt;/STRONG&gt;navigate to "Create an external location".&lt;/LI&gt;&lt;LI&gt;Use the &lt;STRONG&gt;Quickstart&lt;/STRONG&gt; wizard to create your external location.&lt;/LI&gt;&lt;LI&gt;Alternatively, advanced users can manually configure an &lt;STRONG&gt;AWS IAM Role&lt;/STRONG&gt; and specify the &lt;STRONG&gt;S3 bucket path&lt;/STRONG&gt;.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="1.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297594i52AB164F42A7AC08/image-size/large?v=v2&amp;amp;px=999" role="button" title="1.png" alt="1.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="2.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297596i873B39FF94B3F71D/image-size/large?v=v2&amp;amp;px=999" role="button" title="2.png" alt="2.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H4 id="toc-hId-1012538945"&gt;&lt;STRONG&gt;Step 2 – Generate a Personal Access Token (PAT)&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;In SAP Databricks, generate a &lt;STRONG&gt;PAT&lt;/STRONG&gt; that will be used during the AWS CloudFormation setup.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="3 - AWS Quickstart.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297597i207587AA1288ACF2/image-size/large?v=v2&amp;amp;px=999" role="button" title="3 - AWS Quickstart.png" alt="3 - AWS Quickstart.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H4 id="toc-hId-816025440"&gt;&lt;STRONG&gt;Step 3 – Identify Your S3 Bucket&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;Meanwhile, choose or create the &lt;STRONG&gt;S3 bucket&lt;/STRONG&gt; that holds your non-SAP data.&lt;/LI&gt;&lt;LI&gt;Example: Fraud detection data (&lt;A href="https://github.com/aws-samples/aws-fraud-detector-samples/tree/master/data" target="_blank" rel="noopener nofollow noreferrer"&gt;AWS sample data&lt;/A&gt;).&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="4 - AWS S3 Bucket.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297598i925D069B74492B1E/image-size/large?v=v2&amp;amp;px=999" role="button" title="4 - AWS S3 Bucket.png" alt="4 - AWS S3 Bucket.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H4 id="toc-hId-619511935"&gt;&lt;STRONG&gt;Step 4 – Configure Your AWS IAM Role &amp;amp; Policies&lt;/STRONG&gt;&lt;/H4&gt;&lt;P&gt;Ensure the IAM role used by &lt;STRONG&gt;SAP Databricks&lt;/STRONG&gt; has the following permissions to enable secure and functional access to your AWS data environment:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;AmazonS3FullAccess&lt;/STRONG&gt;&lt;BR /&gt;Grants full access to all Amazon S3 buckets and objects within the account.&lt;BR /&gt;&lt;EM&gt;Used to read/write data from S3, list buckets, and manage objects.&lt;/EM&gt;&lt;EM&gt;&lt;BR /&gt;&lt;/EM&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;AmazonS3TablesFullAccess&lt;/STRONG&gt;&lt;BR /&gt;Grants access to AWS Glue tables and data lake metadata.&lt;BR /&gt;&lt;EM&gt;Supports Unity Catalog integration with external tables stored in S3.&lt;/EM&gt;&lt;EM&gt;&lt;BR /&gt;&lt;/EM&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;AmazonS3ObjectLambdaExecutionRolePolicy&lt;/STRONG&gt;&lt;BR /&gt;Enables the role to invoke AWS Lambda functions when accessing S3 objects.&lt;BR /&gt;&lt;EM&gt;Required only if using S3 Object Lambda to dynamically transform or filter data before it is consumed by Databricks.&lt;BR /&gt;&lt;/EM&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;IAMFullAccess&lt;/STRONG&gt;&lt;BR /&gt;Grants full access to manage IAM resources such as roles, policies, users, and groups.&lt;BR /&gt;&lt;EM&gt;Recommended only for administrative or automated setup environments (e.g., CloudFormation). For production, follow the principle of least privilege and only allow specific IAM actions such as &lt;/EM&gt;&lt;EM&gt;iam:PassRole&lt;/EM&gt;&lt;EM&gt; or &lt;/EM&gt;&lt;EM&gt;iam:AssumeRole&lt;/EM&gt;&lt;EM&gt;.&lt;BR /&gt;&lt;/EM&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;EM&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="4 - AmazonS3Access .png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297601iF40C9B86D2A955E1/image-size/large?v=v2&amp;amp;px=999" role="button" title="4 - AmazonS3Access .png" alt="4 - AmazonS3Access .png" /&gt;&lt;/span&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="4 - IAMFullAccess.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297602iB42FDD37280BEBA9/image-size/large?v=v2&amp;amp;px=999" role="button" title="4 - IAMFullAccess.png" alt="4 - IAMFullAccess.png" /&gt;&lt;/span&gt;&lt;/EM&gt;&lt;/P&gt;&lt;H4 id="toc-hId-422998430"&gt;&lt;STRONG&gt;Step 5 – Launch the AWS Quickstart CloudFormation Stack&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;Launch the Quickstart template from within SAP Databricks, and it will direct you to the AWS Quickstart CloudFormation Stack.&lt;/LI&gt;&lt;LI&gt;Note the Personal Access Token (PAT) as you'll be prompted to input it in AWS.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="5 - Launch in Quickstart.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297608iE93B69E4AD46AAAB/image-size/large?v=v2&amp;amp;px=999" role="button" title="5 - Launch in Quickstart.png" alt="5 - Launch in Quickstart.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H4 id="toc-hId-226484925"&gt;&lt;STRONG&gt;Step 6 – Provide Your Personal Access Token to the Stack&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;Input the &lt;STRONG&gt;PAT&lt;/STRONG&gt; generated in the previous step to allow SAP Databricks access.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="5 - Quickstart.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297609i2CF23993EB954E0A/image-size/large?v=v2&amp;amp;px=999" role="button" title="5 - Quickstart.png" alt="5 - Quickstart.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H4 id="toc-hId--467745675"&gt;&lt;STRONG&gt;Step 7 – Verify Stack Completion&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;Monitor the CloudFormation Stack – the status will change from &lt;STRONG&gt;CREATE_IN_PROGRESS&lt;/STRONG&gt; to &lt;STRONG&gt;CREATE_COMPLETE&lt;/STRONG&gt;.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="6 - Quickstart Successful Create_Complete.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297610i6971C5330C84E07B/image-size/large?v=v2&amp;amp;px=999" role="button" title="6 - Quickstart Successful Create_Complete.png" alt="6 - Quickstart Successful Create_Complete.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H4 id="toc-hId--664259180"&gt;&lt;STRONG&gt;Step 8 – Test the External Location Connection&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;In SAP Databricks, return to your external location under Catalog Explorer and &lt;STRONG&gt;click “Test Connection”&lt;/STRONG&gt;.&lt;/LI&gt;&lt;LI&gt;Verify external location and credential permissions for &lt;STRONG&gt;Read&lt;/STRONG&gt;, &lt;STRONG&gt;Write&lt;/STRONG&gt;, and more.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="7 - SAP Databricks External Data.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297611iF09BB1A9C1062738/image-size/large?v=v2&amp;amp;px=999" role="button" title="7 - SAP Databricks External Data.png" alt="7 - SAP Databricks External Data.png" /&gt;&lt;/span&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="8 - S3 Files.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297612i5FE943658CF2A33D/image-size/large?v=v2&amp;amp;px=999" role="button" title="8 - S3 Files.png" alt="8 - S3 Files.png" /&gt;&lt;/span&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="9 - SAP Databricks Test Connection.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297613i87F80F84500A569B/image-size/large?v=v2&amp;amp;px=999" role="button" title="9 - SAP Databricks Test Connection.png" alt="9 - SAP Databricks Test Connection.png" /&gt;&lt;/span&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="10 - Validate Configuration.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297614iBE4F3B7D83C5AE50/image-size/large?v=v2&amp;amp;px=999" role="button" title="10 - Validate Configuration.png" alt="10 - Validate Configuration.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H4 id="toc-hId--860772685"&gt;&lt;STRONG&gt;Step 9 –&amp;nbsp;&lt;/STRONG&gt;&lt;STRONG&gt;Create a Unity Catalog Table from S3 Data&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;Select a file in your S3 bucket and &lt;STRONG&gt;register it as a table&lt;/STRONG&gt; in Unity Catalog.&lt;/LI&gt;&lt;LI&gt;Navigate to Unity Catalog to confirm your new table appears and is accessible for querying&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="11 - Create Table.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297616i7A6F34A35E5E0B9B/image-size/large?v=v2&amp;amp;px=999" role="button" title="11 - Create Table.png" alt="11 - Create Table.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="12 - Select Table.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297618iFF0A10EA29EB96D9/image-size/large?v=v2&amp;amp;px=999" role="button" title="12 - Select Table.png" alt="12 - Select Table.png" /&gt;&lt;/span&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="13 - Preview &amp;amp; Create Table.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297619i46A13DCAFE21360F/image-size/large?v=v2&amp;amp;px=999" role="button" title="13 - Preview &amp;amp; Create Table.png" alt="13 - Preview &amp;amp; Create Table.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="14 - Table in UC.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297620i25D3A182C447310E/image-size/large?v=v2&amp;amp;px=999" role="button" title="14 - Table in UC.png" alt="14 - Table in UC.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H4 id="toc-hId--1057286190"&gt;&lt;STRONG&gt;Step 10 – Work with SAP Data Products and non-SAP S3 Data within the Notebook environment&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;Use the SAP Databricks notebook environment to run analyses on your newly unified data.&lt;/LI&gt;&lt;LI&gt;You can join SAP Data Products with external S3 data for richer insights, all within the same environment.&lt;/LI&gt;&lt;LI&gt;For basic visualizations, harness the power of the notebook environment. For comprehensive dashboarding, SAP Analytics Cloud—part of the SAP Business Data Cloud—offers pre-built templates, robust architecture, and seamless integration with enterprise data, enabling scalable and insightful analytics.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Notebook Dashboard.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297622i085B94E22DC7BDB5/image-size/large?v=v2&amp;amp;px=999" role="button" title="Notebook Dashboard.png" alt="Notebook Dashboard.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;By connecting AWS S3 to SAP Databricks, you can seamlessly unify external and SAP business data for more powerful analytics and AI workflows. This integration enables easy access, management, and analysis of diverse datasets within SAP Business Data Cloud.&amp;nbsp;&lt;/P&gt;&lt;P&gt;In the &lt;STRONG&gt;next part&lt;/STRONG&gt;, we will explore &lt;STRONG&gt;end-to-end integration between SAP Databricks, SAP Datasphere, and SAP Analytics Cloud&lt;/STRONG&gt;, completing the unified analytics journey from data ingestion to actionable insights.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-4/ba-p/14174201"/>
    <published>2025-08-07T08:54:53.397000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/building-agents-for-a-simple-microservice-architecture-with-fastapi-part-2/ba-p/14176702</id>
    <title>🚀Building Agents  for a Simple Microservice Architecture with FastAPI (Part 2)</title>
    <updated>2025-08-10T09:01:21.368000+02:00</updated>
    <author>
      <name>Yogananda</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/75</uri>
    </author>
    <content>&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Previous Blog :&amp;nbsp;&amp;nbsp;&lt;/STRONG&gt;&lt;SPAN class=""&gt;&lt;A class="" href="https://community.sap.com/t5/technology-blog-posts-by-sap/building-collaborative-microservices-in-python-with-fastapi-echo-amp/ba-p/14170025" target="_blank"&gt;Building Collaborative Microservices in Python with FastAPI: Echo &amp;amp; Reverse Agents (Beginner -Part1)&lt;/A&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;EM&gt;Microservices are a powerful way to design scalable and maintainable applications. &lt;/EM&gt;&lt;/P&gt;&lt;P&gt;&lt;EM&gt;In this blog, we will explore a minimal yet effective microservice setup using&amp;nbsp;&lt;STRONG&gt;FastAPI&lt;/STRONG&gt;, perfect for learning and experimentation. This will help to you build better Microservices and deploy in SAP BTP - Kyma&lt;/EM&gt;&lt;/P&gt;&lt;H3 id="toc-hId-1866088139"&gt;Sample Use Case&lt;/H3&gt;&lt;P&gt;A client sends a city name to the Weather Agent. The agent fetches enrichment data from the Data Enricher, generates fake weather data, and returns a combined report. This mimics real-world API composition and data aggregation.&lt;/P&gt;&lt;H3 id="toc-hId-1669574634"&gt;Overview&lt;/H3&gt;&lt;P&gt;It consists of two core services:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Fake Weather Agent&amp;nbsp;(&lt;FONT color="#FF6600"&gt;weather_agent.py&lt;/FONT&gt;)&lt;/LI&gt;&lt;LI&gt;Data Enricher&amp;nbsp;(&lt;FONT color="#FF6600"&gt;data_enricher.py&lt;/FONT&gt;)&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;A shell script (&lt;FONT color="#FF6600"&gt;run.sh&lt;/FONT&gt;) is included to launch both services on separate ports, simulating a real-world microservice environment.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Yogananda_0-1754809227004.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/298973i1E5B429726C6F429/image-size/large?v=v2&amp;amp;px=999" role="button" title="Yogananda_0-1754809227004.png" alt="Yogananda_0-1754809227004.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId-1473061129"&gt;&lt;span class="lia-unicode-emoji" title=":sun_behind_rain_cloud:"&gt;🌦&lt;/span&gt;️ 1. Fake Weather Agent (&lt;FONT color="#FF6600"&gt;weather_agent.py&lt;/FONT&gt;)&lt;/H3&gt;&lt;P&gt;&lt;FONT color="#3366FF"&gt;Purpose&lt;/FONT&gt;:&amp;nbsp; &amp;nbsp;Generates a fake weather report for a given city.&lt;/P&gt;&lt;P&gt;&lt;FONT color="#3366FF"&gt;API Endpoint:&amp;nbsp;&amp;nbsp;&lt;/FONT&gt;&lt;FONT color="#FF6600"&gt;POST /weather&lt;/FONT&gt;&amp;nbsp;— Accepts a JSON payload with a city name.&lt;/P&gt;&lt;P&gt;&lt;FONT color="#3366FF"&gt;How It Works:&lt;/FONT&gt;&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;Receives a city name from the client.&lt;/LI&gt;&lt;LI&gt;Optionally calls the&amp;nbsp;Data Enricher&amp;nbsp;service to fetch additional info (e.g., population, country).&lt;/LI&gt;&lt;LI&gt;Generates random weather data:&lt;UL&gt;&lt;LI&gt;Temperature&lt;/LI&gt;&lt;LI&gt;Condition (e.g., sunny, rainy)&lt;/LI&gt;&lt;LI&gt;Humidity&lt;/LI&gt;&lt;LI&gt;Wind speed&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;LI&gt;Returns a combined weather report, enriched with city metadata if available.&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&lt;FONT color="#3366FF"&gt;Tech Stack:&lt;/FONT&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;FastAPI for API development&lt;/LI&gt;&lt;LI&gt;Pydantic for data validation&lt;/LI&gt;&lt;LI&gt;httpx for asynchronous HTTP calls&lt;/LI&gt;&lt;/UL&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import httpx
import os
import random

PORT = int(os.getenv("PORT", 8002))
TARGET = os.getenv("TARGET_URL", "http://localhost:8003")   # downstream agent

app = FastAPI(title="Fake-Weather-Agent")

class Location(BaseModel):
    city: str

class WeatherReport(BaseModel):
    source: str
    city: str
    temperature: float   # °C
    condition: str
    humidity: int        # %
    wind_kmh: float

CONDITIONS = ["Sunny", "Cloudy", "Rain", "Snow", "Thunderstorm"]

@app.post("/weather", response_model=WeatherReport)
async def get_weather(loc: Location):
    """Generate a fake weather report for the given city."""
    # Optionally call another agent (e.g. a “data-enrichment” service)
    async with httpx.AsyncClient() as client:
        try:
            r = await client.post(
                f"{TARGET}/enrich",
                json={"city": loc.city}
            )
            r.raise_for_status()
            extra = r.json()
        except Exception:
            extra = {}

    return WeatherReport(
        source="Fake-Weather-Agent",
        city=loc.city,
        temperature=round(random.uniform(-10, 40), 1),
        condition=random.choice(CONDITIONS),
        humidity=random.randint(20, 95),
        wind_kmh=round(random.uniform(0, 40), 1),
        **extra
    )&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-1276547624"&gt;&lt;span class="lia-unicode-emoji" title=":cityscape:"&gt;🏙&lt;/span&gt;️ 2. Data Enricher (&lt;FONT color="#FF6600"&gt;data_enricher.py&lt;/FONT&gt;)&lt;/H3&gt;&lt;P&gt;&lt;FONT color="#3366FF"&gt;Purpose&lt;/FONT&gt;:&amp;nbsp;Provides additional metadata about a city.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;API Endpoint:&amp;nbsp;&lt;/STRONG&gt;&lt;FONT color="#FF6600"&gt;POST /enrich&lt;/FONT&gt;&amp;nbsp;— Accepts a JSON payload with a city name.&lt;/P&gt;&lt;P&gt;&lt;FONT color="#3366FF"&gt;How It Works:&lt;/FONT&gt;&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;Looks up the city in a fake in-memory database.&lt;/LI&gt;&lt;LI&gt;Returns population and country if found.&lt;/LI&gt;&lt;LI&gt;If not found, returns default placeholder values.&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&lt;FONT color="#3366FF"&gt;Tech Stack:&lt;/FONT&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;FastAPI&lt;/LI&gt;&lt;LI&gt;Pydantic&lt;/LI&gt;&lt;/UL&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Data-Enricher")

class EnrichRequest(BaseModel):
    city: str

class EnrichResponse(BaseModel):
    population: int
    country: str

FAKE_DB = {
    "london": {"population": 9_000_000, "country": "UK"},
    "paris":  {"population": 2_100_000, "country": "France"},
    "tokyo":  {"population": 14_000_000, "country": "Japan"},
}

@app.post("/enrich", response_model=EnrichResponse)
def enrich(req: EnrichRequest):
    city = req.city.lower()
    if city not in FAKE_DB:
        return EnrichResponse(population=0, country="Unknown")
    return FAKE_DB[city]&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-1080034119"&gt;&lt;span class="lia-unicode-emoji" title=":desktop_computer:"&gt;🖥&lt;/span&gt;️ 3. Running the Services (&lt;FONT color="#FF6600"&gt;run.sh&lt;/FONT&gt;)&lt;/H3&gt;&lt;P&gt;&lt;FONT color="#3366FF"&gt;Purpose:&amp;nbsp;&lt;/FONT&gt;Starts both services using&amp;nbsp;uvicorn, FastAPI’s ASGI server.&lt;BR /&gt;A shell script (&lt;A title="" href="vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/code/electron-browser/workbench/workbench.html" target="_blank" rel="noopener nofollow noreferrer"&gt;run.sh&lt;/A&gt;) is provided to run both services on different ports.&lt;/P&gt;&lt;P&gt;&lt;FONT color="#3366FF"&gt;How It Works&lt;/FONT&gt;:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Launches&amp;nbsp;Fake Weather Agent&amp;nbsp;on port&amp;nbsp;&lt;FONT color="#FF6600"&gt;8002&lt;/FONT&gt;&lt;/LI&gt;&lt;LI&gt;Launches&amp;nbsp;Data Enricher&amp;nbsp;on port&amp;nbsp;&lt;FONT color="#FF6600"&gt;8003&lt;/FONT&gt;&lt;/LI&gt;&lt;LI&gt;Each service runs in its own terminal window&lt;/LI&gt;&lt;/UL&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# Terminal 1
uvicorn fake_weather:app --port 8002 --reload

# Terminal 2
uvicorn data_enricher:app --port 8003 --reload&lt;/code&gt;&lt;/pre&gt;&lt;H2 id="toc-hId-754437895"&gt;Key Points :&amp;nbsp;&lt;/H2&gt;&lt;UL&gt;&lt;LI&gt;Microservice Communication:&lt;BR /&gt;The Weather Agent calls the Data Enricher via HTTP to demonstrate service-to-service communication.&lt;/LI&gt;&lt;LI&gt;Extensibility:&lt;BR /&gt;Easy to add more enrichment services or expand the fake database.&lt;/LI&gt;&lt;LI&gt;FastAPI Features:&lt;BR /&gt;Shows how to use Pydantic models, async endpoints, and response models.&lt;/LI&gt;&lt;LI&gt;Local Development:&amp;nbsp;&amp;nbsp;&lt;BR /&gt;Simple to run both services locally for testing and learning.&lt;/LI&gt;&lt;/UL&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/building-agents-for-a-simple-microservice-architecture-with-fastapi-part-2/ba-p/14176702"/>
    <published>2025-08-10T09:01:21.368000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/enterprise-resource-planning-blog-posts-by-members/easy-way-to-move-zeroes-in-sap-btp-abap-steampunk-js-amp-python/ba-p/14176847</id>
    <title>Easy way to move zeroes in SAP BTP ABAP(Steampunk), JS &amp; Python</title>
    <updated>2025-08-10T15:27:33.552000+02:00</updated>
    <author>
      <name>kallolathome</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/14879</uri>
    </author>
    <content>&lt;H2 id="toc-hId-962976781" id="toc-hId-1737006510"&gt;Introduction&lt;/H2&gt;&lt;P&gt;&lt;SPAN&gt;This is part of the&amp;nbsp;&lt;/SPAN&gt;&lt;A href="https://blogs.sap.com/2022/12/20/easy-way-to-write-algorithms-in-abap-series-01/" target="_blank" rel="noopener noreferrer"&gt;&lt;STRONG&gt;Easy way to write algorithms in ABAP: Series 01&lt;/STRONG&gt;&lt;/A&gt;&lt;SPAN&gt;. For more algorithms, please check the main blog-post.&lt;/SPAN&gt;&lt;/P&gt;&lt;H2 id="toc-hId-766463276" id="toc-hId-1540493005"&gt;Problem&lt;/H2&gt;&lt;P&gt;Given an integer array&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;nums, move all&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;0's to the end of it while maintaining the relative order of the non-zero elements.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Note&lt;/STRONG&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;that you must do this in-place without making a copy of the array.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Example 1:&lt;/STRONG&gt;&lt;/P&gt;&lt;PRE&gt;&lt;STRONG&gt;Input:&lt;/STRONG&gt; nums = [0,1,0,3,12]
&lt;STRONG&gt;Output:&lt;/STRONG&gt; [1,3,12,0,0]&lt;/PRE&gt;&lt;P&gt;&lt;STRONG&gt;Example 2:&lt;/STRONG&gt;&lt;/P&gt;&lt;PRE&gt;&lt;STRONG&gt;Input:&lt;/STRONG&gt; nums = [0]
&lt;STRONG&gt;Output:&lt;/STRONG&gt; [0]&lt;/PRE&gt;&lt;P&gt;&lt;STRONG&gt;Constraints:&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;1 &amp;lt;= nums.length &amp;lt;= 104&lt;/LI&gt;&lt;LI&gt;-231 &amp;lt;= nums[i] &amp;lt;= 231 - 1&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Follow up:&lt;/STRONG&gt;&lt;SPAN&gt;&amp;nbsp;Could you minimize the total number of operations done?&lt;/SPAN&gt;&lt;/P&gt;&lt;H2 id="toc-hId-569949771" id="toc-hId-1343979500"&gt;Solution&lt;/H2&gt;&lt;P&gt;&lt;SPAN&gt;Time Complexity: &lt;STRONG&gt;O(n)&lt;/STRONG&gt;&lt;BR /&gt;Space Complexity: &lt;STRONG&gt;O(1)&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;H3 id="toc-hId-502518985" id="toc-hId-1276548714"&gt;ABAP&lt;/H3&gt;&lt;pre class="lia-code-sample language-abap"&gt;&lt;code&gt;CLASS zmove_zeroes DEFINITION
  PUBLIC
  FINAL
  CREATE PUBLIC .

  PUBLIC SECTION.
    INTERFACES if_oo_adt_classrun.

  PROTECTED SECTION.
  PRIVATE SECTION.
    " Define a table type for integers
    TYPES ty_nums TYPE STANDARD TABLE OF i WITH EMPTY KEY.

    " Method to move zeroes in-place
    METHODS moveZeroes
      CHANGING lt_nums TYPE ty_nums.

ENDCLASS.

CLASS zmove_zeroes IMPLEMENTATION.

  METHOD if_oo_adt_classrun~main.
    " Initialize the number array with some zeroes and non-zeroes
    DATA(lt_nums) = VALUE ty_nums( ( 0 ) ( 1 ) ( 0 ) ( 3 ) ( 12 ) ).

    " Output the array before moving zeroes
    out-&amp;gt;write( |Array before moving zeroes: | ).
    LOOP AT lt_nums INTO DATA(lv_num).
      out-&amp;gt;write( lv_num ).
    ENDLOOP.

    " Call the method to move zeroes to the end
    moveZeroes( CHANGING lt_nums = lt_nums ).

    " Output the array after moving zeroes
    out-&amp;gt;write( |Array after moving zeroes: | ).
    LOOP AT lt_nums INTO lv_num.
      out-&amp;gt;write( lv_num ).
    ENDLOOP.

  ENDMETHOD.

  METHOD moveZeroes.

    DATA(lv_count) = 0. " Counter for non-zero elements

    " First pass: Move all non-zero elements to the front
    LOOP AT lt_nums ASSIGNING FIELD-SYMBOL(&amp;lt;lf_num&amp;gt;).
      IF &amp;lt;lf_num&amp;gt; &amp;lt;&amp;gt; 0.
        " Place the non-zero element at the next available position
        lt_nums[ lv_count + 1 ] = &amp;lt;lf_num&amp;gt;.
        lv_count += 1.
      ENDIF.
    ENDLOOP.

    " Second pass: Fill the rest of the array with zeroes
    WHILE lv_count &amp;lt; lines( lt_nums ).
      lt_nums[ lv_count + 1 ] = 0.
      lv_count += 1.
    ENDWHILE.

  ENDMETHOD.

ENDCLASS.&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-306005480" id="toc-hId-1080035209"&gt;JavaScript&lt;/H3&gt;&lt;pre class="lia-code-sample language-javascript"&gt;&lt;code&gt;function moveZeroes(nums) {
    let left = 0;
    for (let right = 0; right &amp;lt; nums.length; right++) {
        if (nums[right] !== 0) {
            // Swap nums[left] and nums[right]
            let temp = nums[left];
            nums[left] = nums[right];
            nums[right] = temp;
            left++;
        }
    }
    return nums;
}&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-883521704"&gt;&amp;nbsp;&lt;/H3&gt;&lt;H3 id="toc-hId-502518985" id="toc-hId-687008199"&gt;Python&lt;/H3&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;def moveZeroes(nums):
    left = 0
    for right in range(len(nums)):
        if nums[right] != 0:
            nums[left], nums[right] = nums[right], nums[left]
            left += 1
    return nums&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;N.B: For ABAP, I am using SAP BTP ABAP Environment 2309 Release.&lt;/SPAN&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;SPAN&gt;Happy Coding!&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class="lia-unicode-emoji"&gt;&lt;span class="lia-unicode-emoji" title=":slightly_smiling_face:"&gt;🙂&lt;/span&gt;&lt;/SPAN&gt;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/enterprise-resource-planning-blog-posts-by-members/easy-way-to-move-zeroes-in-sap-btp-abap-steampunk-js-amp-python/ba-p/14176847"/>
    <published>2025-08-10T15:27:33.552000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/sap-datasphere-automation-creating-database-user-in-sap-datasphere-using/ba-p/14176606</id>
    <title>SAP Datasphere Automation : Creating Database user in SAP Datasphere using Datasphere CLI &amp; Python</title>
    <updated>2025-08-12T08:06:51.294000+02:00</updated>
    <author>
      <name>shubham521</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/845865</uri>
    </author>
    <content>&lt;H3 id="toc-hId-1866087182"&gt;Introduction&lt;/H3&gt;&lt;P&gt;One way to access datasphere hana database is via database users. Each database user is linked to one space (except database analysis user). We can create database user vai GUI however, its a long process and prone to errors. In this blog, i will share my work on how i automated the process using SAP Datasphere CLI and Python without the need to login into datasphere GUI.&lt;/P&gt;&lt;H3 id="toc-hId-1669573677"&gt;Old Process:&lt;/H3&gt;&lt;P&gt;If you want to create a database user via GUI, you need to perform the below steps.&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;Login into the datasphere tenant. Make sure you have the required application roles to perform the task.&lt;/LI&gt;&lt;LI&gt;Go to Space management and select the space. Scroll down and click create in database user.&amp;nbsp;&lt;/LI&gt;&lt;LI&gt;Provide the username and deploy the space. Save the password and share it with the user.&amp;nbsp;&lt;/LI&gt;&lt;/OL&gt;&lt;H3 id="toc-hId-1473060172"&gt;Automated Process&lt;/H3&gt;&lt;P&gt;To overcome the long process, i have divided the python script to perform all these task in sequential manner.&amp;nbsp;&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;&lt;STRONG&gt;Datasphere CLI Setup&lt;/STRONG&gt; : Set the CLI and login into datasphere&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Reading the metadata&lt;/STRONG&gt;: Read the space metadata using SAP Datasphere CLI &lt;STRONG&gt;space read&lt;/STRONG&gt; command&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Modifying JSON&lt;/STRONG&gt; : Enhance the output JSON with the new user details and deploy it using the &lt;STRONG&gt;space create&lt;/STRONG&gt; command&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Password Reset&lt;/STRONG&gt; : Reset the database user password and save it in a file for sharing.&lt;/LI&gt;&lt;/OL&gt;&lt;H3 id="toc-hId-1276546667"&gt;Set up SAP Datasphere CLI (First time only)&lt;/H3&gt;&lt;P&gt;To start working with Datasphere CLI, we need to perform few one time configuration like setting host, login in and setting cache. If you want to login via a different tenant, you need to change the CLIENT_ID and CLIENT_SECRETS as per the tenant.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;datasphere config set host {"host_url"}
datasphere login --client-id {"client_id"} --client-secret {"client_secret"}
datasphere config set cache --client-id {"client_id"} --client-secret {"client_secret"}&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;Once you are done with the initial setup of CLI, we can start working on the main code which will create database users.&lt;/P&gt;&lt;H3 id="toc-hId-1080033162"&gt;Reading space metadata&lt;/H3&gt;&lt;P&gt;Since we do not have a direct command to create DB user, we extract the space metadata JSON using CLI. This command output the space metadata JSON into console. I have stored this into a variable that i will use later in modifying the JSON&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;space_Json = datasphere space read -space "{space"} --definations&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-883519657"&gt;Modifying the JSON and deploying it in datasphere&amp;nbsp;&lt;/H3&gt;&lt;P&gt;To create a new databased user in space, we need to add the new user details in the dbuser object of the JSON. We can store the space metadata JSON into our local machine and modify it or we can modify it on the go using tempfiles. I have used the later approach as it eliminates the need of JSON management&amp;nbsp;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;#Concatenating the space and username
new_db_user = f"{space}#{username}"

#Appending the new_db_user details into the space metadata JSON
space_JSON[space]["spaceDefinition"]["dbusers"][new_db_user] = {
"ingestion":{
        "auditing":{
          "dppRead":{
            "retentionPeriod":21
            "isAuditPolicyActive":True
          },
         
        }
      },
      "consumption":{
        "consumptionWithGrant":false,
        "spaceSchemaAccess":True,
        "scriptServerAccess":false,
        "localSchemaAccess":false,
        "hdiGrantorForCupsAccess":false
}&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;Once the JSON is modified, write the new JSON into a temporary file. I have stored the file path into a variable called tmp_file_path. I will push this file path to datasphere tenant.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;#Pushing the modified JSON to datasphere tenant
datasphere space create --file-path "{tmp_file_path}" -- force-defination-deployment &lt;/code&gt;&lt;/pre&gt;&lt;P&gt;If you want to delete a database user, remove the entry from the JSON and run the same command and add&amp;nbsp;&lt;SPAN&gt;&lt;STRONG&gt;--enforce-database-user-deletion&lt;/STRONG&gt; in the end. If this command is not added, then the deletion will not work.&lt;/SPAN&gt;&lt;/P&gt;&lt;H3 id="toc-hId-687006152"&gt;Password Reset&lt;/H3&gt;&lt;P&gt;Once the database user is created, we need to reset the password to store it in a local file for sharing. Below is the command to perform that action&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;datasphere dbusers password reset --space{"space"} --databaseuser {"new_db_user"}&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;(&lt;STRONG&gt;Tip: Add a 30 seconds wait time between creating the database user and resetting the password commands because the space deployment takes some time and we cannot reset the password before that&lt;/STRONG&gt;).&lt;/P&gt;&lt;P&gt;I have also added an automatic email creation in my workflow and inserting database user details into a local table for future analysis.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-08-09 at 20.56.29.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/298901iDFDFD74E03AF67EA/image-size/large?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-08-09 at 20.56.29.png" alt="Screenshot 2025-08-09 at 20.56.29.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId-490492647"&gt;Conclusion:&lt;/H3&gt;&lt;P&gt;With this automation, we can create database users in datasphere tenant without even login into datasphere. This workflow provides a more robust way to handle creation and maintenance database users.&amp;nbsp;&lt;/P&gt;&lt;P&gt;I would love to know your thoughts on this workflow in the comments below. Lets explore more opportunities of automation using datasphere CLI.&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/sap-datasphere-automation-creating-database-user-in-sap-datasphere-using/ba-p/14176606"/>
    <published>2025-08-12T08:06:51.294000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/predictive-stock-transfer-amp-automatic-purchase-re-order-plant-to-plant/ba-p/14170063</id>
    <title>Predictive Stock Transfer &amp; Automatic Purchase Re-Order: Plant-to-Plant A2A Orchestration</title>
    <updated>2025-08-16T14:04:54.954000+02:00</updated>
    <author>
      <name>Yogananda</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/75</uri>
    </author>
    <content>&lt;P&gt;&lt;STRONG&gt;Use-case: Predictive Stock Transfer &amp;amp; Automatic Purchase Re-Order (Plant-to-Plant A2A scenario driven by real-time APIs)&lt;/STRONG&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Yogananda_0-1755345825205.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/301623i7ED9C4CC9FB704BB/image-size/large?v=v2&amp;amp;px=999" role="button" title="Yogananda_0-1755345825205.png" alt="Yogananda_0-1755345825205.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;A fast-moving material in Plant A is kept in stock by an end-to-end, automated flow that &lt;/SPAN&gt;&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;&lt;SPAN&gt;checks forecast demand, &lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;looks for internal surplus in nearby plants, and &lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;automatically creates stock transfers or purchase requisitions as needed. &lt;/SPAN&gt;&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&lt;STRONG&gt;Prerequisities and needed&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;SAP S/4HANA Cloud (Inventory &amp;amp; MRP) – On-hand stock, stock in transit, MRP items&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;SAP Integrated Business Planning (IBP) – Demand forecast for FG-100 at Plant A&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;SAP Extended Warehouse Management (EWM) – Real-time on-hand stock incl. quarantine&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;SAP Ariba – Supplier catalog and pricing for external options&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;External supplier catalog (Ariba-like) – External pricing and lead times&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;SAP Integration Suite / SAP Event Mesh – Orchestrates the end-to-end flow and publishes events&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Sequence – how the APIs interact end-to-end&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 0 – Trigger&lt;/STRONG&gt;&lt;BR /&gt;A nightly iFlow in SAP Integration Suite (or SAP Event Mesh) starts the orchestration.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 1 – Demand forecast&lt;/STRONG&gt;&lt;BR /&gt;GET /api/ibp/v1/demandplanning/forecast?material=FG-100&amp;amp;plant=A&amp;amp;weeks=4&lt;BR /&gt;→ Returns 1,200 pcs forecasted demand.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 2 – Current &amp;amp; projected stock in Plant A&lt;/STRONG&gt;&lt;BR /&gt;GET /sap/opu/odata/sap/API_MATERIAL_STOCK_SRV/MaterialStock?material=FG-100&amp;amp;plant=A&lt;BR /&gt;→ 180 pcs unrestricted, 40 pcs blocked.&lt;BR /&gt;GET /sap/opu/odata/sap/API_MRP_COCKPIT_SRV/MrpItems?material=FG-100&amp;amp;plant=A&lt;BR /&gt;&lt;SPAN&gt;Result: confirmed receipts 600 pcs&lt;/SPAN&gt;&lt;BR /&gt;→ Confirmed receipts 600 pcs, so net shortage = 1,200 – 180 – 600 = 420 pcs.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 3 – Locate surplus stock in the network&lt;/STRONG&gt;&lt;BR /&gt;Parallel calls (one per plant):&lt;BR /&gt;GET /sap/opu/odata/sap/API_MATERIAL_STOCK_SRV/MaterialStock?material=FG-100&amp;amp;plant=B&lt;BR /&gt;→ 300 pcs unrestricted.&lt;BR /&gt;GET /sap/opu/odata/sap/API_MATERIAL_STOCK_SRV/MaterialStock?material=FG-100&amp;amp;plant=C&lt;BR /&gt;→ 250 pcs unrestricted.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 4 – Check ATP (available-to-transfer) incl. transit time&lt;/STRONG&gt;&lt;BR /&gt;POST /sap/opu/odata/sap/API_ATP_CHECK_SRV/CheckAvailability&lt;BR /&gt;Body:&lt;/P&gt;&lt;pre class="lia-code-sample language-json"&gt;&lt;code&gt;{ 
"material": "FG-100", 
"plant": "B", 
"demandQty": 300, 
"requiredDate": "2024-06-25" 
}&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;→ Confirms 300 pcs can be delivered by 2024-06-23.&lt;BR /&gt;Same for Plant C → 120 pcs available by 2024-06-24.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 5 – Decide cheapest internal option&lt;/STRONG&gt;&lt;BR /&gt;Cost service (custom REST on S/4):&lt;BR /&gt;POST /internal/transferCost&lt;/P&gt;&lt;pre class="lia-code-sample language-json"&gt;&lt;code&gt;{ 
"fromPlants": ["B","C"], 
"toPlant": "A", 
"qty": [300,120] 
}&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;BR /&gt;→ Plant B has the lowest freight cost (€0.05/pc vs €0.07/pc).&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 6 – Create stock transport order (STO)&lt;/STRONG&gt;&lt;BR /&gt;POST /sap/opu/odata/sap/API_STOCK_TRANSPORT_ORDER_SRV/A_StockTransportOrder&lt;BR /&gt;Body:&lt;/P&gt;&lt;pre class="lia-code-sample language-json"&gt;&lt;code&gt;{ 
"SupplyingPlant": "B", 
"ReceivingPlant": "A", 
"Material": "FG-100", 
"OrderQuantity": 300, 
"DeliveryDate": "2024-06-23" 
}&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;→ STO 4500012345 created.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 7 – Remaining uncovered quantity&lt;/STRONG&gt;&lt;BR /&gt;Shortage after internal transfer = 420 – 300 = 120 pcs.&lt;BR /&gt;Call Ariba to get best external price:&lt;BR /&gt;GET /v2/suppliers/catalog?material=FG-100&amp;amp;qty=120&amp;amp;currency=EUR&lt;BR /&gt;→ Supplier S-987 offers €2.30/pc, lead time 7 days.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 8 – Create purchase requisition in S/4&lt;/STRONG&gt;&lt;BR /&gt;POST /sap/opu/odata/sap/API_PURCHASEREQ_PROCESS_SRV/A_PurchaseRequisitionHeader&lt;BR /&gt;Body:&lt;/P&gt;&lt;pre class="lia-code-sample language-json"&gt;&lt;code&gt;{ 
"Material": "FG-100", 
"Plant": "A", 
"Quantity": 120, 
"DeliveryDate": "2024-06-27", 
"SupplierHint": "S-987" 
}&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;→ PR 1000123456 created.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 9 – Notify planners&lt;/STRONG&gt;&lt;BR /&gt;Publish event to SAP Event Mesh topic /business/plantA/stockReplenished&lt;BR /&gt;Payload:&lt;/P&gt;&lt;pre class="lia-code-sample language-json"&gt;&lt;code&gt;{ 
"material": "FG-100", 
"sto": "4500012345", 
"pr": "1000123456", 
"status": "covered" 
}&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;STRONG&gt;Outcome&lt;/STRONG&gt;&lt;BR /&gt;Plant A will receive 300 pcs from Plant B via an automatically created STO and 120 pcs via a purchase requisition with the cheapest external supplier—no manual intervention, no stock-out, and minimal freight cost.&lt;/P&gt;&lt;P&gt;complete code&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;#!/usr/bin/env python3
"""
End-to-end A2A flow:
1. Read demand forecast (IBP)
2. Check stock &amp;amp; MRP in Plant A
3. Search surplus stock in Plants B/C
4. Run ATP check
5. Create STO (cheapest internal)
6. Create PR for remaining qty (Ariba best price)
"""

import os, json, math
from datetime import datetime, timedelta
from dotenv import load_dotenv
from requests import Session
from requests_oauthlib import OAuth2Session
from oauthlib.oauth2 import BackendApplicationClient

load_dotenv()

# ---------- CONFIG ----------
MATERIAL = "FG-100"
PLANT_A  = "A"
PLANTS_SURPLUS = ["B", "C"]
DEMAND_WEEKS   = 4
TRANSPORT_DAYS = 2
# ----------------------------

s = Session()

# ---------- 0.  OAUTH TOKEN for S/4 ----------
def s4_token():
    url = f"{os.getenv('S4_HOST')}/oauth/token"
    r = s.post(url,
               auth=(os.getenv('S4_USER'), os.getenv('S4_PASSWORD')),
               data={'grant_type':'client_credentials'})
    r.raise_for_status()
    return r.json()['access_token']

s.headers.update({'Authorization': f"Bearer {s4_token()}"})

# ---------- 1.  IBP – demand forecast ----------
def ibp_forecast():
    url = (f"{os.getenv('S4_HOST')}/sap/opu/odata/sap/"
           f"API_DEMAND_PLANNING_SRV/DemandForecast")
    params = {
        "$filter": f"Material eq '{MATERIAL}' and Plant eq '{PLANT_A}'",
        "$select": "DemandQuantity",
        "$format": "json"
    }
    r = s.get(url, params=params)
    r.raise_for_status()
    total = sum(item['DemandQuantity'] for item in r.json()['d']['results'])
    return total

demand_qty = ibp_forecast()
print("Demand forecast:", demand_qty)

# ---------- 2.  Stock &amp;amp; MRP ----------
def plant_stock(plant):
    url = (f"{os.getenv('S4_HOST')}/sap/opu/odata/sap/"
           f"API_MATERIAL_STOCK_SRV/MaterialStock")
    params = {
        "$filter": f"Material eq '{MATERIAL}' and Plant eq '{plant}' and "
                   f"StorageLocation ne ''",
        "$format": "json"
    }
    r = s.get(url, params=params)
    r.raise_for_status()
    return sum(item['UnrestrictedStockQuantity']
               for item in r.json()['d']['results'])

def plant_receipts(plant):
    url = (f"{os.getenv('S4_HOST')}/sap/opu/odata/sap/"
           f"API_MRP_COCKPIT_SRV/MrpItems")
    params = {
        "$filter": f"Material eq '{MATERIAL}' and Plant eq '{plant}' and "
                   f"MrpElementCategory eq 'AR'",
        "$format": "json"
    }
    r = s.get(url, params=params)
    r.raise_for_status()
    return sum(item['Quantity'] for item in r.json()['d']['results'])

stock_A = plant_stock(PLANT_A)
receipts_A = plant_receipts(PLANT_A)
shortage = max(0, demand_qty - stock_A - receipts_A)
print("Shortage:", shortage)
if shortage == 0:
    print("No action needed.")
    exit()

# ---------- 3.  Surplus stock ----------
surplus = {}
for p in PLANTS_SURPLUS:
    surplus[p] = plant_stock(p)
print("Surplus:", surplus)

# ---------- 4.  ATP check ----------
def atp_ok(plant, qty, req_date):
    url = (f"{os.getenv('S4_HOST')}/sap/opu/odata/sap/"
           f"API_ATP_CHECK_SRV/CheckAvailability")
    body = {
        "Material": MATERIAL,
        "Plant": plant,
        "DemandQuantity": str(qty),
        "RequiredDate": req_date.isoformat()
    }
    r = s.post(url, json=body)
    r.raise_for_status()
    return r.json()['d']['ConfirmedQuantity'] == str(qty)

best_internal = None
needed = shortage
for plant, qty in surplus.items():
    take = min(qty, needed)
    req = datetime.utcnow().date() + timedelta(days=TRANSPORT_DAYS)
    if atp_ok(plant, take, req):
        best_internal = (plant, take)
        break
if best_internal:
    plant, qty = best_internal
    print(f"Best internal: {qty} from {plant}")

    # ---------- 5.  Create STO ----------
    url = (f"{os.getenv('S4_HOST')}/sap/opu/odata/sap/"
           f"API_STOCK_TRANSPORT_ORDER_SRV/A_StockTransportOrder")
    body = {
        "SupplyingPlant": plant,
        "ReceivingPlant": PLANT_A,
        "Material": MATERIAL,
        "OrderQuantity": str(qty),
        "DeliveryDate": req.isoformat()
    }
    r = s.post(url, json=body)
    r.raise_for_status()
    sto = r.json()['d']['StockTransportOrder']
    print("STO created:", sto)
    shortage -= qty

# ---------- 6.  Ariba – best external price ----------
if shortage &amp;gt; 0:
    client = BackendApplicationClient(client_id=os.getenv('ARIBA_CLIENT_ID'))
    oauth = OAuth2Session(client=client)
    token = oauth.fetch_token(
        token_url=os.getenv('ARIBA_TOKEN_URL'),
        client_id=os.getenv('ARIBA_CLIENT_ID'),
        client_secret=os.getenv('ARIBA_CLIENT_SECRET')
    )
    url = "https://api.ariba.com/v2/suppliers/catalog"
    params = {
        "material": MATERIAL,
        "qty": shortage,
        "currency": "EUR"
    }
    r = oauth.get(url, params=params)
    r.raise_for_status()
    best = min(r.json()['offers'], key=lambda x: float(x['price']))
    print("Best supplier:", best['supplier'], best['price'])

    # ---------- 7.  Create PR ----------
    url = (f"{os.getenv('S4_HOST')}/sap/opu/odata/sap/"
           f"API_PURCHASEREQ_PROCESS_SRV/A_PurchaseRequisitionHeader")
    body = {
        "Material": MATERIAL,
        "Plant": PLANT_A,
        "Quantity": str(shortage),
        "DeliveryDate": (datetime.utcnow().date()
                         + timedelta(days=int(best['leadtime']))).isoformat(),
        "SupplierHint": best['supplier']
    }
    r = s.post(url, json=body)
    r.raise_for_status()
    pr = r.json()['d']['PurchaseRequisition']
    print("PR created:", pr)

print("Flow finished.")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/predictive-stock-transfer-amp-automatic-purchase-re-order-plant-to-plant/ba-p/14170063"/>
    <published>2025-08-16T14:04:54.954000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/sap-datasphere-amp-python-one-click-to-export-data-of-multiple-views-in/ba-p/14180664</id>
    <title>SAP Datasphere &amp; Python : One click to export data of multiple views in Excel/CSV</title>
    <updated>2025-08-19T08:22:39.531000+02:00</updated>
    <author>
      <name>vikasparmar88</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1528256</uri>
    </author>
    <content>&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Introduction&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Currently, SAP Datasphere only allows data export through Analytical Models. That means for every fact view, one has to create a separate Analytical Model just to download the data. It’s not ideal, especially when you have many views. Exporting them one by one becomes slow and repetitive.&lt;/P&gt;&lt;P&gt;To solve this,&amp;nbsp; Python script was developed that connects to SAP Datasphere, runs a query on each view from list, and saves the results as Excel files in a local drive path which is predefined. Now&amp;nbsp; just run the script, and it exports everything in one go—no manual effort needed.&lt;/P&gt;&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;Requirements&amp;nbsp;&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;Before running the script, install the following Python packages:&lt;/P&gt;&lt;P&gt;These packages enable secure connectivity to SAP Datasphere and support efficient data handling.&lt;/P&gt;&lt;PRE&gt;pip install hdbcli
pip install sqlalchemy
pip install sqlalchemy-hana&lt;/PRE&gt;&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;Creating a Database User in SAP Datasphere&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;To enable Python connectivity, create a database user:&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;Navigate to&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;STRONG&gt;Space Management&lt;/STRONG&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;in SAP Datasphere&lt;/LI&gt;&lt;LI&gt;Select the relevant space&lt;/LI&gt;&lt;LI&gt;Click on&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;STRONG&gt;Database Access&lt;/STRONG&gt;&lt;/LI&gt;&lt;LI&gt;Create a new user with read access&lt;/LI&gt;&lt;LI&gt;Copy the host, port, username, and password&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;Helpful links :&amp;nbsp;&lt;A title="Create DB User in Datasphere Space" href="https://developers.sap.com/tutorials/data-warehouse-cloud-intro8-create-databaseuser..html" target="_blank" rel="noopener noreferrer"&gt;Create DB User in Datasphere Space&lt;/A&gt;&amp;nbsp;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Python&amp;nbsp;Script&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;PRE&gt;# &lt;span class="lia-unicode-emoji" title=":package:"&gt;📦&lt;/span&gt; Install required packages (run these in your terminal or notebook)
# pip install hdbcli              # SAP HANA database client
# pip install sqlalchemy          # SQL toolkit and ORM for Python
# pip install sqlalchemy-hana     # SAP HANA dialect for SQLAlchemy

# &lt;span class="lia-unicode-emoji" title=":books:"&gt;📚&lt;/span&gt; Import necessary libraries
import pandas as pd               # For data manipulation and Excel export
from hdbcli import dbapi          # SAP HANA DBAPI for direct connection
import warnings                   # To suppress unnecessary warnings
import os                         # For file path and directory handling

# &lt;span class="lia-unicode-emoji" title=":prohibited:"&gt;🚫&lt;/span&gt; Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# &lt;span class="lia-unicode-emoji" title=":locked_with_key:"&gt;🔐&lt;/span&gt; Define SAP Datasphere connection parameters
# &lt;span class="lia-unicode-emoji" title=":backhand_index_pointing_right:"&gt;👉&lt;/span&gt; Replace the placeholders below with your actual connection details
db_user = '&amp;lt;your_database_user&amp;gt;'           # User with access to target schema
db_password = '&amp;lt;your_secure_password&amp;gt;'     # Password (handle securely)
db_host = '&amp;lt;your_datasphere_host_url&amp;gt;'     # Host URL (e.g., xyz.hanacloud.ondemand.com)
db_port = 443                               # Default HTTPS port for SAP HANA Cloud
db_schema = '&amp;lt;your_schema_name&amp;gt;'           # Target schema containing views

# &lt;span class="lia-unicode-emoji" title=":file_folder:"&gt;📁&lt;/span&gt; Ensure output folder exists for Excel exports
output_folder = r'C:\Datasphere\Excel export'  # Update path as needed
os.makedirs(output_folder, exist_ok=True)

# &lt;span class="lia-unicode-emoji" title=":clipboard:"&gt;📋&lt;/span&gt; Define list of views to extract data from
# &lt;span class="lia-unicode-emoji" title=":backhand_index_pointing_right:"&gt;👉&lt;/span&gt; Add or modify view names based on your schema
view_list = ['VIEW_1', 'VIEW_2']  # Example views

try:
    # &lt;span class="lia-unicode-emoji" title=":globe_with_meridians:"&gt;🌐&lt;/span&gt; Establish secure connection to SAP Datasphere
    connection = dbapi.connect(
        address=db_host,
        port=db_port,
        user=db_user,
        password=db_password,
        encrypt=True,
        sslValidateCertificate=True
    )
    print("&lt;span class="lia-unicode-emoji" title=":white_heavy_check_mark:"&gt;✅&lt;/span&gt; Connected to SAP Datasphere")

    cursor = connection.cursor()

    # &lt;span class="lia-unicode-emoji" title=":repeat_button:"&gt;🔁&lt;/span&gt; Loop through each view and export its data
    for view_name in view_list:
        try:
            # &lt;span class="lia-unicode-emoji" title=":bar_chart:"&gt;📊&lt;/span&gt; Construct and execute SQL query
            sql_query = f'SELECT * FROM "{db_schema}"."{view_name}"'
            print(f"&lt;span class="lia-unicode-emoji" title=":bar_chart:"&gt;📊&lt;/span&gt; Executing query: {sql_query}")
            cursor.execute(sql_query)

            # &lt;span class="lia-unicode-emoji" title=":inbox_tray:"&gt;📥&lt;/span&gt; Fetch results and convert to DataFrame
            rows = cursor.fetchall()
            columns = [desc[0] for desc in cursor.description]
            df = pd.DataFrame(rows, columns=columns)

            # &lt;span class="lia-unicode-emoji" title=":outbox_tray:"&gt;📤&lt;/span&gt; Export DataFrame to Excel
            output_path = os.path.join(output_folder, f'{view_name}.xlsx')
            df.to_excel(output_path, index=False)
            print(f"&lt;span class="lia-unicode-emoji" title=":white_heavy_check_mark:"&gt;✅&lt;/span&gt; Data from '{view_name}' saved to: {output_path}")

        except dbapi.Error as view_err:
            print(f"&lt;span class="lia-unicode-emoji" title=":cross_mark:"&gt;❌&lt;/span&gt; Error querying '{view_name}': {view_err}")

except dbapi.Error as db_err:
    print(f"&lt;span class="lia-unicode-emoji" title=":cross_mark:"&gt;❌&lt;/span&gt; Database error: {db_err}")
except Exception as ex:
    print(f"&lt;span class="lia-unicode-emoji" title=":warning:"&gt;⚠️&lt;/span&gt; Unexpected error: {ex}")
finally:
    # &lt;span class="lia-unicode-emoji" title=":locked:"&gt;🔒&lt;/span&gt; Ensure connection is closed gracefully
    if 'connection' in locals():
        connection.close()
        print("&lt;span class="lia-unicode-emoji" title=":locked:"&gt;🔒&lt;/span&gt; Connection closed")&lt;/PRE&gt;&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Script Capabilities&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Establishes secure connection to SAP Datasphere&lt;/LI&gt;&lt;LI&gt;Executes queries on each listed view&lt;/LI&gt;&lt;LI&gt;Saves data from each view into a separate Excel file&lt;/LI&gt;&lt;LI&gt;Stores all files in a defined folder&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;Only connection details and view names need to be updated. The script handles the rest.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;FONT size="5"&gt;&lt;STRONG&gt;One-Click Execution with a .bat File&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;To simplify execution, create a&amp;nbsp;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;STRONG&gt;RunExport.bat&lt;/STRONG&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;file to run the Python script with a double-click.&lt;/P&gt;&lt;P&gt;Double-clicking the file will automatically export all views to Excel without opening a terminal&lt;/P&gt;&lt;PRE&gt; off
REM Activate Python and run the Export script

REM Change to the script directory
cd /d "C:\Datasphere\Excel export"

REM Run the Python script
python Export.py

REM Pause to keep the window open (optional)
pause&lt;/PRE&gt;&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;Example&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="4"&gt;Before Execution&lt;/FONT&gt;&lt;/STRONG&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="11.jpeg" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/300844iD619132276E2474C/image-size/large?v=v2&amp;amp;px=999" role="button" title="11.jpeg" alt="11.jpeg" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Double Click on "RunExcel.bat" file&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="2.jpeg" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/300845iCD8108A14F47A5B1/image-size/large?v=v2&amp;amp;px=999" role="button" title="2.jpeg" alt="2.jpeg" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Post Execution&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="3.jpg" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/300847iD739F0EA7A4E9615/image-size/large?v=v2&amp;amp;px=999" role="button" title="3.jpg" alt="3.jpg" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;Conclusion&lt;/STRONG&gt;&lt;/FONT&gt;&lt;BR /&gt;This automation simplifies data exports from SAP Datasphere, especially when working with multiple views.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;It reduces manual effort&lt;/LI&gt;&lt;LI&gt;improves consistency and saves time.&lt;/LI&gt;&lt;LI&gt;ideal for recurring tasks or scheduled jobs.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;For setup support or customization, feel free to connect.&lt;/P&gt;&lt;P&gt;Thanks&lt;/P&gt;&lt;P&gt;Vikas Parmar&lt;/P&gt;&lt;P&gt;&lt;a href="https://community.sap.com/t5/c-khhcw49343/SAP+Datasphere/pd-p/73555000100800002141" class="lia-product-mention" data-product="16-1"&gt;SAP Datasphere&lt;/a&gt;&amp;nbsp;&lt;a href="https://community.sap.com/t5/c-khhcw49343/SAP+Business+Data+Cloud/pd-p/73554900100700003531" class="lia-product-mention" data-product="1249-1"&gt;SAP Business Data Cloud&lt;/a&gt;&amp;nbsp;&lt;a href="https://community.sap.com/t5/c-khhcw49343/Python/pd-p/f220d74d-56e2-487e-8e6c-a8cb3def2378" class="lia-product-mention" data-product="126-1"&gt;Python&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/sap-datasphere-amp-python-one-click-to-export-data-of-multiple-views-in/ba-p/14180664"/>
    <published>2025-08-19T08:22:39.531000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-in-sap-business-data-cloud-a-typical-machine-learning/ba-p/14206612</id>
    <title>SAP Databricks in SAP Business Data Cloud – a Typical Machine Learning Workflow</title>
    <updated>2025-09-04T04:16:17.227000+02:00</updated>
    <author>
      <name>js2</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/41060</uri>
    </author>
    <content>&lt;P&gt;With SAP Databricks you have access to an amazing set of capabilities to work with your BDC Data Products and other data.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_0-1756949550337.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308823i50F7383D319ECA1F/image-size/large?v=v2&amp;amp;px=999" role="button" title="js2_0-1756949550337.png" alt="js2_0-1756949550337.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;This blog post is part of a series exploring SAP Databricks in SAP Business Data Cloud:&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-1/ba-p/14166813" target="_self"&gt;&lt;SPAN&gt;Part 1 – SQL analytics with SAP Data Products&lt;BR /&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-2/ba-p/14167025" target="_self"&gt;Part 2 – Build and deploy Mosaic AI and Agent Tools&lt;/A&gt;&lt;BR /&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-how-to-use-automl-to-forecast-sales-data-part-3/ba-p/14174354" target="_self"&gt;Part 3 – How to use AutoML to forecast sales data&lt;/A&gt;&lt;BR /&gt;&lt;/SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-3/ba-p/14174201" target="_self"&gt;&lt;SPAN&gt;Part 4 – Connect SAP Data Products with non-SAP data from AWS S3&lt;BR /&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-4/ba-p/14178056" target="_self"&gt;Part 5 – End-to-end integration: SAP Databricks, SAP Datasphere, and SAP Analytics Cloud&lt;/A&gt;&lt;BR /&gt;&lt;/SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-create-inferences-for-application-integration-with-sap-build/ba-p/14186662" target="_self"&gt;Part 6 – Create inferences for application integration with SAP Build&amp;nbsp;&lt;/A&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Part 7 -&amp;nbsp;SAP Databricks in SAP Business Data Cloud – a Typical Machine Learning Workflow&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;In this blog post we’ll look at the typical workflow you would undertake when trying to train a machine learning model.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Visualise and understand your data&lt;/LI&gt;&lt;LI&gt;Optimise for hyperparameters to tune your model&lt;/LI&gt;&lt;LI&gt;Explore hyperparameter sweep results with MLflow&lt;/LI&gt;&lt;LI&gt;Register the best performing model in MLflow&lt;/LI&gt;&lt;LI&gt;Apply the registered model with batch inference and Databricks Model Serving&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;We’ll use a classic machine learning dataset to predict whether a wine is of high quality or not (&lt;STRONG&gt;&lt;EM&gt;a data classification problem&lt;/EM&gt;&lt;/STRONG&gt;). Of course you have access to a range of SAP Data Products, but by using this dataset you don’t even need a connected S/4HANA system to follow along. The dataset also comes built-in with SAP Databricks.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-1759168994"&gt;Wine Quality Classification&lt;/H2&gt;&lt;P&gt;We will train a binary classification model to predict the quality of Portuguese "Vinho Verde" wine based on the wine's physicochemical properties.&lt;/P&gt;&lt;P&gt;The dataset is from the UCI Machine Learning Repository, presented in Modelling wine preferences by data mining from physicochemical properties [Cortez et al., 2009]. And the good news is that this dataset comes preloaded with your Databricks system.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Let’s create a new notebook in our SAP Databricks system and in a new cell we will install some module dependencies.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;%pip install --upgrade -Uqqq mlflow&amp;gt;=3.0 xgboost hyperopt
%restart_python&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;Installs:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;The latest &lt;STRONG&gt;&lt;A href="https://mlflow.org/" target="_blank" rel="nofollow noopener noreferrer"&gt;mlflow&lt;/A&gt;&lt;/STRONG&gt; for experiment tracking and general MLOps&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;A href="https://xgboost.readthedocs.io/en/stable/" target="_blank" rel="nofollow noopener noreferrer"&gt;xgboost&lt;/A&gt;&lt;/STRONG&gt; being a fantastic and very popular machine learning model architecture (it dominates many Kaggle competitions)&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;A href="https://hyperopt.github.io/hyperopt/" target="_blank" rel="nofollow noopener noreferrer"&gt;hyperopt&lt;/A&gt;&lt;/STRONG&gt; is a python library used for hyperparameter optimisation. It intelligently searches for the optimal hyperparameters to use when training a machine learning model.&lt;/LI&gt;&lt;LI&gt;The `&lt;STRONG&gt;%restart_python&lt;/STRONG&gt;` magic command it necessary in Databricks notebooks because they use long running Python processes and this ensures that the system path and any newly installed python packages are being used.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Next, we create a cell to connect MLFlow to the Databricks Unity Catalog (it would otherwise use an sqlite data store) and create a few constants that will be used later:&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import mlflow
mlflow.set_registry_uri("databricks-uc")

CATALOG_NAME = "workspace"
SCHEMA_NAME = "default"
MODEL_NAME = "wine_quality_classifier"&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-1562655489"&gt;Read and Understand the DATA&lt;/H2&gt;&lt;P&gt;Read the white wine quality and red wine quality CSV datasets and merge them into a single DataFrame. &lt;EM&gt;Note theses datasets come with your Databricks system&lt;/EM&gt;.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import pandas as pd

white_wine = pd.read_csv("/databricks-datasets/wine-quality/winequality-white.csv", sep=";")
red_wine = pd.read_csv("/databricks-datasets/wine-quality/winequality-red.csv", sep=";")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Merge the two DataFrames into a single dataset, with a new binary feature "is_red" that indicates whether the wine is red or white.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;red_wine['is_red'] = 1
white_wine['is_red'] = 0

data = pd.concat([red_wine, white_wine], axis=0)

# cast to float as a best practice (avoids dtype issues with NaN's later)
data["is_red"] = data["is_red"].astype("float32")

# Remove spaces from column names
data.rename(columns=lambda x: x.replace(' ', '_'), inplace=True)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_1-1756949711124.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308824i39D3D0E1BF31443D/image-size/large?v=v2&amp;amp;px=999" role="button" title="js2_1-1756949711124.png" alt="js2_1-1756949711124.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Now we have one dataset with a mix of white and red wines.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H3 id="toc-hId-1495224703"&gt;Visualize data&lt;/H3&gt;&lt;P&gt;Before training a model, explore the dataset using popular charting libraries: Seaborn and Matplotlib.&lt;/P&gt;&lt;P&gt;Plot a histogram of the dependent variable, quality.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import seaborn as sns
sns.displot(data.quality, kde=False)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_2-1756949850129.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308825i87E79215C173B70F/image-size/large?v=v2&amp;amp;px=999" role="button" title="js2_2-1756949850129.png" alt="js2_2-1756949850129.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Looks like quality scores are normally distributed between 3 and 9. Define a wine as high quality if it has quality &amp;gt;= 7.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;high_quality = (data.quality &amp;gt;= 7)
data.quality = high_quality

# cast to float as a best practice (avoids dtype issues with NaN's later)
data["quality"] = data["quality"].astype("float32")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Box plots are useful for identifying correlations between features and a binary label. Create box plots for each feature to compare high-quality and low-quality wines. Significant differences in the box plots indicate good predictors of quality.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import matplotlib.pyplot as plt

dims = (3, 4)

f, axes = plt.subplots(dims[0], dims[1], figsize=(25, 15))
axis_i, axis_j = 0, 0
for col in data.columns:
  if col == 'is_red' or col == 'quality':
    continue # Box plots cannot be used on indicator variables
  sns.boxplot(x=high_quality, y=data[col], ax=axes[axis_i, axis_j])
  axis_j += 1
  if axis_j == dims[1]:
    axis_i += 1
    axis_j = 0&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_3-1756949850144.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308826iF8D13D55625B9536/image-size/large?v=v2&amp;amp;px=999" role="button" title="js2_3-1756949850144.png" alt="js2_3-1756949850144.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;In the above box plots, a few variables stand out as good univariate predictors of quality.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;In the alcohol box plot, the median alcohol content of high-quality wines is greater than even the 75th quantile of low-quality wines. High alcohol content is correlated with quality.&lt;/LI&gt;&lt;LI&gt;In the density box plot, low quality wines have a greater density than high quality wines. Density is inversely correlated with quality.&lt;/LI&gt;&lt;/UL&gt;&lt;H2 id="toc-hId-1169628479"&gt;&amp;nbsp;&lt;/H2&gt;&lt;H2 id="toc-hId-973114974"&gt;Preprocess data&lt;/H2&gt;&lt;P&gt;Before training a model, check for missing values and split the data into training and validation sets.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;data.isna().any()&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_4-1756950017285.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308827i2A0200F5FB07390B/image-size/medium?v=v2&amp;amp;px=400" role="button" title="js2_4-1756950017285.png" alt="js2_4-1756950017285.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;There are no missing values.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P class="lia-indent-padding-left-60px" style="padding-left : 60px;"&gt;&lt;EM&gt;Note: Often you will need to take advantage of &lt;STRONG&gt;feature engineering&lt;/STRONG&gt; at this step. This is where you can build new features as combinations of your existing data… for example multiplying two existing feature columns together to create a new column may enable the model to find better patterns in the data.&lt;/EM&gt;&lt;/P&gt;&lt;P class="lia-indent-padding-left-60px" style="padding-left : 60px;"&gt;&lt;EM&gt;For time series data it can be very helpful to generate a new feature column called “Qtr” for example to show which qtr of the year that data point is in based on a date. You will need to experiment with this…&lt;/EM&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-776601469"&gt;Prepare the dataset to train a baseline model&lt;/H2&gt;&lt;P&gt;Split the input data into 3 sets:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Train (60% of the dataset used to train the model)&lt;/LI&gt;&lt;LI&gt;Validation (20% of the dataset used to tune the hyperparameters)&lt;/LI&gt;&lt;LI&gt;Test (20% of the dataset used to report the true performance of the model on an unseen dataset)&lt;/LI&gt;&lt;/UL&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from sklearn.model_selection import train_test_split

X = data.drop(["quality"], axis=1)
y = data.quality

# Split out the training data
X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.6, random_state=123)

# Split the remaining data equally into validation and test
X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=0.5, random_state=123)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-580087964"&gt;Train a baseline model&lt;/H2&gt;&lt;P&gt;This task seems well suited to a &lt;STRONG&gt;random forest classifier&lt;/STRONG&gt;, since the output is binary and there may be interactions between multiple variables.&lt;/P&gt;&lt;P&gt;Build a simple classifier using scikit-learn and use MLflow to keep track of the model's accuracy and save the model for later use.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P class="lia-indent-padding-left-60px" style="padding-left : 60px;"&gt;&lt;EM&gt;Note: When this cell is executed an MLFlow Experiment will be created by default (automatically) using the full path of this Notebook. In production experiments its best practice to set the Experiment name with&amp;nbsp;mlflow.set_experiment()&amp;nbsp;because you may work on the problem over multiple Notebooks and/or users.&lt;/EM&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import mlflow.pyfunc
import mlflow.sklearn
import numpy as np
import sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
from mlflow.models.signature import infer_signature
from mlflow.utils.environment import _mlflow_conda_env
import cloudpickle
import time

# The predict method of sklearn's RandomForestClassifier returns a binary classification (0 or 1). 
# The following code creates a wrapper function, SklearnModelWrapper, that uses 
# the predict_proba method to return the probability that the observation belongs to each class. 

class SklearnModelWrapper(mlflow.pyfunc.PythonModel):
  def __init__(self, model):
    self.model = model
    
  def predict(self, context, model_input):
    return self.model.predict_proba(model_input)[:,1]

# mlflow.start_run creates a new MLflow run to track the performance of this model. 
# Within the context, you call mlflow.log_param to keep track of the parameters used, and
# mlflow.log_metric to record metrics like accuracy.
with mlflow.start_run(run_name='rf_baseline_n10'):
  n_estimators = 10
  model = RandomForestClassifier(n_estimators=n_estimators, random_state=np.random.RandomState(123))
  model.fit(X_train, y_train)

  # predict_proba returns [prob_negative, prob_positive], so slice the output with [:, 1]
  predictions_test = model.predict_proba(X_test)[:,1]
  auc_score = roc_auc_score(y_test, predictions_test)
  mlflow.log_param('n_estimators', n_estimators)
  # Use the area under the ROC curve as a metric.
  mlflow.log_metric('auc', auc_score)
  wrappedModel = SklearnModelWrapper(model)
  
  # MLflow contains utilities to create a conda environment used to serve models.
  # The necessary dependencies are added to a conda.yaml file which is logged along with the model.
  conda_env = _mlflow_conda_env(
        additional_conda_deps=None,
        additional_pip_deps=["cloudpickle=={}".format(cloudpickle.__version__), "scikit-learn=={}".format(sklearn.__version__)],
        additional_conda_channels=None,
    )

  # Here we log the model and register it to Unity Catalog in one go.
  # MLflow automatically generates model signatures when you provide
  # an `input_example` during model logging. This works for all model 
  # flavors and is the recommended approach for most use cases.
  # By registering this model to Unity Catalog, you can easily reference
  # the model from anywhere within Databricks.
  # 
  sample_input = X_train.head(5)

  model_version = mlflow.pyfunc.log_model(
    name="rf_baseline",
    python_model=wrappedModel,
    conda_env=conda_env,
    input_example=sample_input,
    registered_model_name=MODEL_NAME,
  )&lt;/code&gt;&lt;/pre&gt;&lt;P class="lia-indent-padding-left-60px" style="padding-left : 60px;"&gt;&lt;EM&gt;Note how the trained model is registered when logging it to MLFlow. This can be done separately as we will see later. If you are running many experiments there is no need to register every model but only the best model.&lt;/EM&gt;&lt;/P&gt;&lt;P&gt;Review the learned feature importances output by the model. As illustrated by the previous boxplots, alcohol and density are important in predicting quality.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;feature_importances = pd.DataFrame(model.feature_importances_, index=X_train.columns.tolist(), columns=['importance'])
feature_importances.sort_values('importance', ascending=False)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_5-1756950458175.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308828i525ABF330AAFB999/image-size/medium?v=v2&amp;amp;px=400" role="button" title="js2_5-1756950458175.png" alt="js2_5-1756950458175.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;You logged the Area Under the ROC Curve (AUC) to MLflow. Click the Experiment icon in the right sidebar to display the Experiment Runs sidebar. The model achieved an AUC of 0.854. A random classifier would have an AUC of 0.5, and higher AUC values are better.&lt;/P&gt;&lt;P&gt;The ROC AUC is a good evaluation metric for binary classification problems like we have here (is good quality / is not good quality).&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Next, assign this model the "Best" tag, and load it into this notebook from Unity Catalog.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from mlflow.tracking import MlflowClient

client = MlflowClient()
client.set_registered_model_alias(MODEL_NAME, "Best", model_version.registered_model_version)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;In Unity Catalog, the model version now has the tag "Best". You can now refer to the model using the path&amp;nbsp;&lt;FONT face="terminal,monaco"&gt;models:/{model_name}@Best&lt;/FONT&gt;.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_6-1756950552130.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308830iB8AEA9EA4051F02D/image-size/medium?v=v2&amp;amp;px=400" role="button" title="js2_6-1756950552130.png" alt="js2_6-1756950552130.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-383574459"&gt;Experiment with a new model&lt;/H2&gt;&lt;P&gt;The random forest model performed well even &lt;EM&gt;without&lt;/EM&gt; hyperparameter tuning.&lt;/P&gt;&lt;P&gt;Let's now try and do better and use the xgboost library to train a more accurate model. Run a hyperparameter sweep to train multiple models in parallel, using Hyperopt and Trials. As before, MLflow tracks the performance of each parameter configuration.&lt;/P&gt;&lt;P class="lia-indent-padding-left-60px" style="padding-left : 60px;"&gt;&lt;EM&gt;Note: We must use Trials and not SparkTrials in SAP Databricks, because SparkTrials tries to access the underlying JVM which is not supported on serverless compute.&lt;/EM&gt;&lt;/P&gt;&lt;P&gt;We use the validation dataset here for hyperparameter search.&lt;/P&gt;&lt;P class="lia-indent-padding-left-60px" style="padding-left : 60px;"&gt;&lt;FONT color="#FF0000"&gt;&lt;EM&gt;Note this training cell takes over 20mins to complete!&lt;/EM&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from hyperopt import fmin, tpe, hp, Trials, STATUS_OK
from hyperopt.pyll import scope
from math import exp
import mlflow.xgboost
import numpy as np
import xgboost as xgb

search_space = {
  'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),
  'learning_rate': hp.loguniform('learning_rate', -3, 0),
  'reg_alpha': hp.loguniform('reg_alpha', -5, -1),
  'reg_lambda': hp.loguniform('reg_lambda', -6, -1),
  'min_child_weight': hp.loguniform('min_child_weight', -1, 3),
  'objective': 'binary:logistic',
  'seed': 123, # Set a seed for deterministic training
}

def train_model(params):
  # With MLflow autologging, hyperparameters and the trained model are automatically logged to MLflow.
  mlflow.xgboost.autolog()
  with mlflow.start_run(nested=True):
    train = xgb.DMatrix(data=X_train, label=y_train)
    validation = xgb.DMatrix(data=X_val, label=y_val)
    # Pass in the validation set so xgb can track an evaluation metric. XGBoost terminates training when the evaluation metric
    # is no longer improving.
    booster = xgb.train(params=params, dtrain=train, num_boost_round=1000,
                        evals=[(validation, "validation")], early_stopping_rounds=50)
    validation_predictions = booster.predict(validation)
    auc_score = roc_auc_score(y_val, validation_predictions)
    mlflow.log_metric('auc', auc_score)

    # Don't register the model in one-step here - let the hyperparameter search find the best one first.
    #signature = infer_signature(X_train, booster.predict(train))
    #mlflow.xgboost.log_model(booster, name="xgboost", signature=signature)
    sample_input = X_train.head(5)
    mlflow.xgboost.log_model(booster, name="xgboost", input_example=sample_input)
    
    # Set the loss to -1*auc_score so fmin maximizes the auc_score
    return {'status': STATUS_OK, 'loss': -1*auc_score, 'booster': booster.attributes()}

# Use Trials instead of SparkTrials
trials = Trials()

# Run fmin within an MLflow run context so that each hyperparameter configuration is logged as a child run of a parent
# run called "xgboost_models" .
with mlflow.start_run(run_name='xgboost_models'):
  best_params = fmin(
    fn=train_model, 
    space=search_space, 
    algo=tpe.suggest, 
    max_evals=96,
    trials=trials,
  )&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-187060954"&gt;Use MLflow to view the results&lt;/H2&gt;&lt;P&gt;Open up the &lt;EM&gt;Experiments&lt;/EM&gt; sidebar to see the MLflow runs. Click on Date next to the down arrow to display a menu, and select 'auc' to display the runs sorted by the auc metric. The highest auc value is ~0.90.&amp;nbsp;&lt;STRONG&gt;Remember that this is against the validation data&lt;/STRONG&gt;.&lt;/P&gt;&lt;P class="lia-indent-padding-left-60px" style="padding-left : 60px;"&gt;&lt;EM&gt;Hyperparameter tuning (runs) score against the validation dataset!&lt;/EM&gt;&lt;/P&gt;&lt;P&gt;MLflow tracks the parameters and performance metrics of each run. Click the External Link icon at the top of the Experiment Runs sidebar to navigate to the MLflow Runs Table.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId--9452551"&gt;Update the best version of the&amp;nbsp;&lt;FONT face="terminal,monaco"&gt;wine_quality_classifier&lt;/FONT&gt;&amp;nbsp;model&lt;/H2&gt;&lt;P&gt;Earlier, you saved the baseline model to Unity Catalog with the name&amp;nbsp;&lt;FONT face="terminal,monaco"&gt;wine_quality_classifier&lt;/FONT&gt;. Now you can update&amp;nbsp;&lt;FONT face="terminal,monaco"&gt;wine_quality_classifier&lt;/FONT&gt;&amp;nbsp;to a more accurate model from the hyperparameter sweep. Because you used MLflow to log the model produced by each hyperparameter configuration, you can use MLflow to identify the best performing run and save the model from that run to Unity Catalog.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;best_run = mlflow.search_runs(order_by=['metrics.auc DESC']).iloc[0]
print(f'AUC of Best Run: {best_run["metrics.auc"]}')&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_7-1756950758865.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308831i3D6D3AA368E57F2C/image-size/medium?v=v2&amp;amp;px=400" role="button" title="js2_7-1756950758865.png" alt="js2_7-1756950758865.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;new_model_version = mlflow.register_model(f"runs:/{best_run.run_id}/model", MODEL_NAME)

# Registering the model takes a few seconds, so add a small delay
import time
time.sleep(15)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Click&amp;nbsp;&lt;STRONG&gt;Models&lt;/STRONG&gt;&amp;nbsp;in the left sidebar to see that the&amp;nbsp;&lt;FONT face="terminal,monaco"&gt;wine_quality_classifier&lt;/FONT&gt;&amp;nbsp;model now has a new versions. Assign the "Best" alias to the new version.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from mlflow.tracking import MlflowClient

client = MlflowClient()
client.set_registered_model_alias(MODEL_NAME, "Best", new_model_version.version)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Clients that call&amp;nbsp;&lt;FONT face="terminal,monaco"&gt;load_model()&lt;/FONT&gt;&amp;nbsp;using the "Best" alias now get the new model.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&amp;gt;&amp;gt; &lt;/STRONG&gt;&lt;STRONG&gt;Let's get the AUC score against the Test data&lt;/STRONG&gt;&lt;STRONG&gt;:&lt;/STRONG&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;model = mlflow.pyfunc.load_model(f"models:/{MODEL_NAME}@Best")

from sklearn.metrics import roc_auc_score
print(f'AUC: {roc_auc_score(y_test, model.predict(X_test))}')&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_8-1756950758865.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308832i435B28BDC047F3F0/image-size/medium?v=v2&amp;amp;px=400" role="button" title="js2_8-1756950758865.png" alt="js2_8-1756950758865.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;The new version achieved a better score (AUC = 0.90) on the test set.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-141288301"&gt;Batch inference&lt;/H2&gt;&lt;P&gt;There are many scenarios where you might want to evaluate a model on a corpus of new data. For example, you may have a fresh batch of data or may need to compare the performance of two models on the same corpus of data.&lt;/P&gt;&lt;P&gt;Evaluate the model on data stored in a Delta table, using Spark to run the computation in parallel.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# To simulate a new corpus of data, save the existing X_train data to a Delta table. 
# In the real world, this would be a new batch of data.
spark_df = spark.createDataFrame(X_train)

table_name = f"{CATALOG_NAME}.{SCHEMA_NAME}.wine_data"

(spark_df
  .write
  .format("delta")
  .mode("overwrite")
  .option("overwriteSchema",True)
  .saveAsTable(table_name)
)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Load the model into a Spark UDF, so it can be applied to the Delta table.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;apply_model_udf = mlflow.pyfunc.spark_udf(spark, f"models:/{MODEL_NAME}@Best")&lt;/code&gt;&lt;/pre&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# Read the "new data" from the Unity Catalog table
new_data = spark.read.table(f"{CATALOG_NAME}.{SCHEMA_NAME}.wine_data")&lt;/code&gt;&lt;/pre&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from pyspark.sql.functions import struct

# Apply the model to the new data
udf_inputs = struct(*(X_train.columns.tolist()))

new_data = new_data.withColumn(
  "prediction",
  apply_model_udf(udf_inputs)
)

display(new_data)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_9-1756951022597.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308833iDD624907B34C7B23/image-size/large?v=v2&amp;amp;px=999" role="button" title="js2_9-1756951022597.png" alt="js2_9-1756951022597.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Each row now has an associated prediction. Note that the&amp;nbsp;&lt;STRONG&gt;xgboost&lt;/STRONG&gt;&amp;nbsp;function is using the objective "binary:logistic" so the predictions shown here are probabilities.&lt;/P&gt;&lt;P&gt;We also add a is_good_quality column:&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from pyspark.sql.functions import col, when

new_data = new_data.withColumn("prediction", col("prediction")[0])

new_data = new_data.withColumn(
  "is_good_quality",
  when(col("prediction") &amp;gt; 0.5, True).otherwise(False)
)
display(new_data)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_10-1756951022605.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308834i46308B60261C3565/image-size/large?v=v2&amp;amp;px=999" role="button" title="js2_10-1756951022605.png" alt="js2_10-1756951022605.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Overwrite the table with the new columns:&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;(new_data
  .write
  .format("delta")
  .mode("overwrite")
  .option("overwriteSchema", True)
  .saveAsTable(table_name)
)

# Enable Change Data Feed for the table
# Seems that we can only add this option via SQL!!
spark.sql(f"ALTER TABLE {table_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId--55225204"&gt;Serve the model&lt;/H2&gt;&lt;P&gt;To productionize the model for low latency predictions, use Mosaic AI Model Serving to deploy the model to an endpoint. The following cell shows how to use the MLflow Deployments SDK to create a model serving endpoint (which can also be done view the Serving menu on the left).&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;First of all, let’s just show the current model’s name and best version&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# Get the model vesion we tagged as &lt;a href="https://community.sap.com/t5/user/viewprofilepage/user-id/1725027"&gt;@Best&lt;/a&gt;
from mlflow.tracking import MlflowClient
best_ver = MlflowClient().get_model_version_by_alias(MODEL_NAME, "Best").version
print(MODEL_NAME, best_ver)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_11-1756951367897.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308835i39C1C658C1D6029A/image-size/medium?v=v2&amp;amp;px=400" role="button" title="js2_11-1756951367897.png" alt="js2_11-1756951367897.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Check if any versions of this model are already being served and delete them&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from mlflow.deployments import get_deploy_client

client = get_deploy_client("databricks")
endpoints = client.list_endpoints()

deployed = False
deployed_versions = []
for ep in endpoints:
    ep_detail = client.get_endpoint(ep["name"])
    for entity in ep_detail.get("config", {}).get("served_models", []):
        if entity.get("model_name") == MODEL_NAME or entity.get("model_name").endswith(MODEL_NAME):
            deployed = True
            deployed_versions.append(str(entity.get("model_version")))
            # Delete the serving endpoint if the model is already deployed
            client.delete_endpoint(ep["name"])

if deployed_versions:
    deployed_versions_str = ", ".join(deployed_versions)
else:
    deployed_versions_str = ""

display(spark.createDataFrame([{"model_name": MODEL_NAME, "deployed": deployed, "deployed_versions": deployed_versions_str, "action": "deleting..."}]))&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_12-1756951367899.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308836iCF48545499B7C401/image-size/large?v=v2&amp;amp;px=999" role="button" title="js2_12-1756951367899.png" alt="js2_12-1756951367899.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;EM&gt;&lt;FONT color="#FF0000"&gt;Creating the endpoint can take 5+ minutes...&lt;/FONT&gt;&lt;/EM&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# the "name" property can't include special chars so we drop the catalog and schema from the model_name

from mlflow.deployments import get_deploy_client

client = get_deploy_client("databricks")
endpoint = client.create_endpoint(
    name="wine-model-endpoint",
    config={
        "served_entities": [
            {
                "name": MODEL_NAME,
                "entity_name": f"{CATALOG_NAME}.{SCHEMA_NAME}.{MODEL_NAME}",
                "entity_version": best_ver,
                "workload_size": "Small",
                "scale_to_zero_enabled": True
            }
        ],
      }
)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId--251738709"&gt;Test the Model Serving Endpoint&lt;/H2&gt;&lt;P&gt;Navigate to User Settings -&amp;gt; Developer and create an Access Token for calling the serving endpoint.&lt;/P&gt;&lt;P&gt;Ensure the model is being served as this can take 5-10 mins.&lt;/P&gt;&lt;P&gt;In the below cells the notebook will:&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;Ask for your access token&lt;/LI&gt;&lt;LI&gt;Setup a payload (the required inputs for your model)&lt;/LI&gt;&lt;LI&gt;Call the model serving endpoint!&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from getpass import getpass
token = getpass("🔑  Paste your Databricks token: ")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Setup the api call request payload with sample wine quality data:&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;payload = {
  "dataframe_split": {
    "columns": [
      "fixed_acidity", "volatile_acidity", "citric_acid", "residual_sugar",
      "chlorides", "free_sulfur_dioxide", "total_sulfur_dioxide",
      "density", "pH", "sulphates", "alcohol", "is_red"
    ],
    "data": [
      [7.3, 0.19, 0.27, 1.6, 0.027, 35, 136, 0.99248, 3.38, 0.54, 11, 0],
      [7.8, 0.88, 0.00, 2.6, 0.098, 25, 67, 0.9968, 3.20, 0.68, 9.8, 1]
    ]
  }&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Use the python requests package to make an api call to the SAP Databricks Model Serving Endpoint.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P class="lia-indent-padding-left-30px" style="padding-left : 30px;"&gt;&lt;FONT color="#FF0000"&gt;&lt;EM&gt;Make sure to update the endpoint uri below to match your current SAP Databricks system!&lt;/EM&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import os, json, requests

url   = "https://&amp;lt;uri&amp;gt;.cloud.databricks.com/serving-endpoints/wine-model-endpoint/invocations"

resp = requests.post(
    url,
    headers={
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json"
    },
    data=json.dumps(payload),
    timeout=60
)

if resp.status_code == 404:
    print("The endpoint is still deploying.")
else:
    print(resp.json())
    for i, score in enumerate(resp.json()["predictions"], start=1):
        is_good = score &amp;gt;= 0.5          # quality flag
        print(f"Row {i}: {score:.3f}  ➜  Good quality? {is_good}")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_13-1756951569054.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308837i0BC6D9EA17EBEA20/image-size/large?v=v2&amp;amp;px=999" role="button" title="js2_13-1756951569054.png" alt="js2_13-1756951569054.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;You can use this API endpoint to perform inference from your own applications – as is done with blog post : “&lt;STRONG&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-create-inferences-for-application-integration-with-sap-build/ba-p/14186662" target="_blank"&gt;Part 6 – Create inferences for application integration with SAP Build&amp;nbsp;&lt;/A&gt;&lt;/STRONG&gt;” in the series.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId--448252214"&gt;Conclusion&lt;/H2&gt;&lt;P&gt;We’ve seen in this notebook, if you have followed along, the typical pattern of training a machine learning model.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;It always starts with understanding the data available. Visualising the data with histograms and box plots as shown here can be a great help. Use tools like ChatGPT to assist in the best ways to flesh out information about your data&lt;/LI&gt;&lt;LI&gt;It can often be helpful to create a quick baseline model just to see that we can do better than random luck with the training data&lt;/LI&gt;&lt;LI&gt;Use a hyperparameter optimisation tool to help search for the ideal parameters to tune the best model. Be very careful with the split of training, validation and test data and ensure that there can never be any overlap. Research how to do this if using time-series data&lt;/LI&gt;&lt;LI&gt;Use MLFlow to log training experiments and their generated models. Assign tags to highlight specific or “best” models.&lt;/LI&gt;&lt;LI&gt;Look at Batch Inference or Model Serving.&lt;UL&gt;&lt;LI&gt;The former (batch) being ideal if you want to batch score a table of data and potentially share it back to BDC to be used in analytics models. Make use of scheduled notebooks to keep the data up to date and to train the model on new data&lt;/LI&gt;&lt;LI&gt;Use Model Serving to expose an endpoint for real-time applications to make predictions.&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-in-sap-business-data-cloud-a-typical-machine-learning/ba-p/14206612"/>
    <published>2025-09-04T04:16:17.227000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/hello-python-my-first-script-in-sap-bas-connecting-to-hana-cloud/ba-p/14228993</id>
    <title>Hello Python: My First Script in SAP BAS Connecting to HANA Cloud</title>
    <updated>2025-09-26T13:05:26.454000+02:00</updated>
    <author>
      <name>Sharathmg</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/174516</uri>
    </author>
    <content>&lt;P&gt;Credit:&amp;nbsp;&lt;a href="https://community.sap.com/t5/user/viewprofilepage/user-id/183"&gt;@Vitaliy-R&lt;/a&gt;&amp;nbsp;Your startup blogs kindled my interest to explore working with Python in SAP ecosystem.&amp;nbsp;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/using-python-in-sap-business-application-studio-my-notes/ba-p/14155516" target="_self"&gt;Python in BAS&lt;/A&gt;&amp;nbsp;and&amp;nbsp;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/using-jupyter-in-sap-business-application-studio-my-notes/ba-p/14167294" target="_self"&gt;Jupyter in BAS&lt;/A&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;When I first started exploring SAP Business Application Studio (BAS), I was curious about how Python could fit into the SAP landscape. I’ve mostly associated BAS with HANA artefacts(SQLScript, hdbcalculationview, hdbreptask etc.) and CAP artefacts, so writing a Python script inside BAS felt like venturing into new territory. My goal was simple: write a basic script and connect it to SAP HANA Cloud. What I discovered along the way is that Python not only works smoothly in BAS but also makes it easy to interact with HANA Cloud, opening up opportunities for data exploration, automation, and integration in a way that feels both modern and approachable.&lt;/P&gt;&lt;P&gt;Before jumping into the Python script, I had to get my environment ready in SAP Business Application Studio (BAS). Here’s what I set up:&lt;/P&gt;&lt;P&gt;A BAS dev space with a full-stack cloud application space since it supports multiple runtimes, including Python. I had a space with HANA Native Application type. Since the Python tools extension&amp;nbsp;is not added by default, I edited the space to select the Python tools in the additional extension options.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="HANA Dev Space Python extension" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/320334iFEC4E0932EFEAC15/image-size/large?v=v2&amp;amp;px=999" role="button" title="HANA_DevSpace_Setting.png" alt="HANA Dev Space Python extension" /&gt;&lt;span class="lia-inline-image-caption" onclick="event.preventDefault();"&gt;HANA Dev Space Python extension&lt;/span&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;Note: For initial steps to check the Python version, Jupyter notebook and set ups refer to the blogs listed at the start.&amp;nbsp;&lt;/P&gt;&lt;P&gt;Use Case: I attempted to achieve the following:&amp;nbsp;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Establish a connection to HANA Cloud&lt;/LI&gt;&lt;LI&gt;Execute an SQL query on a table/view&amp;nbsp;&lt;/LI&gt;&lt;LI&gt;Display the results&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;In the BAS, I created a project from Template: SAP HANA Database Project&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Project Template.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/320351iEAE035C8FCA7C5B5/image-size/large?v=v2&amp;amp;px=999" role="button" title="Project Template.png" alt="Project Template.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Next step: Create a notebook file.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="notebook file.png" style="width: 339px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/320356i8F1BB8DEF9D0E888/image-size/medium?v=v2&amp;amp;px=400" role="button" title="notebook file.png" alt="notebook file.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;My guide to connect to HANA Cloud:&amp;nbsp;&lt;A href="https://help.sap.com/docs/SAP_HANA_CLIENT/f1b440ded6144a54ada97ff95dac7adf/d12c86af7cb442d1b9f8520e2aba7758.html" target="_self" rel="noopener noreferrer"&gt;Connect to HANA Cloud&lt;/A&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;When I first tried importing hdbcli into my Jupyter Notebook within BAS, I ran into the same ModuleNotFoundError. Even though I had already installed hdbcli In the terminal, the notebook kernel wasn’t recognizing it. On some search and prompting with GPT( &lt;span class="lia-unicode-emoji" title=":beaming_face_with_smiling_eyes:"&gt;😁&lt;/span&gt;), I understood that it's a common issue because Jupyter can run in a different Python environment than the terminal. The fix was simple: I ran&lt;/P&gt;&lt;PRE&gt;import sys
!{sys.executable} -m pip install hdbcli&lt;/PRE&gt;&lt;P&gt;directly in a notebook cell. This ensures that the HANA client is installed in the same environment as the notebook kernel. After this step, I could successfully import dbapi and connect to HANA Cloud without any errors. It was a small but important lesson about Python environments in BAS, especially when using Jupyter.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="hdbcli Module Not found.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/320378i62AF858DA44FE00C/image-size/large?v=v2&amp;amp;px=999" role="button" title="hdbcli Module Not found.png" alt="hdbcli Module Not found.png" /&gt;&lt;/span&gt;With the hdbcli package installed and working in my Jupyter Notebook, I was ready to write my first Python script to connect to SAP HANA Cloud.&lt;/P&gt;&lt;P&gt;In the next cell, I imported hdbcli in this notebook.&amp;nbsp;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import hdbcli
print(hdbcli.__file__)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="import hdbcli.png" style="width: 854px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/320388iF426637D8D8CCB0F/image-size/large?v=v2&amp;amp;px=999" role="button" title="import hdbcli.png" alt="import hdbcli.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;The next step was to&amp;nbsp;gain access to the dbapi interface, which allows you to establish connections, execute SQL queries, and fetch results from your HANA Cloud instance. This simple import is the gateway to working with HANA directly from Python.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from hdbcli import dbapi&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;The next step is to establish a connection to your HANA Cloud instance. This requires specifying the host, port, username, and password.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="hana cloud connection.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/320408i84F10DA5613166DC/image-size/large?v=v2&amp;amp;px=999" role="button" title="hana cloud connection.png" alt="hana cloud connection.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;After connecting, you can create a cursor object to execute SQL statements. An SQL statement, preferably a Select Query to test the retrieval of data from HANA Cloud. In my case, I used a Select with count on the number of records in a view. Once the variables were ready, execute the connection cursor object.&lt;/P&gt;&lt;P&gt;Note: in the SQL variable, use single quotes and a semicolon at the end of the query. (beginner tip&amp;nbsp;&lt;span class="lia-unicode-emoji" title=":slightly_smiling_face:"&gt;🙂&lt;/span&gt; )&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="Execution Cursor.png" style="width: 799px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/320427iB0929785AAAB7257/image-size/large?v=v2&amp;amp;px=999" role="button" title="Execution Cursor.png" alt="Execution Cursor.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Now is the time to test the data retrieval from the script and compare it with the Database Explorer.&lt;/P&gt;&lt;P&gt;Drum roll....&lt;span class="lia-unicode-emoji" title=":drum:"&gt;🥁&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-left" image-alt="Data in DB explorer.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/320447i3D6BB255F8FDBF13/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Data in DB explorer.png" alt="Data in DB explorer.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-right" image-alt="Data in Script.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/320448iDA977EF3358B8FF8/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Data in Script.png" alt="Data in Script.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Hurray&amp;nbsp;&lt;span class="lia-unicode-emoji" title=":party_popper:"&gt;🎉&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Completing my first Python script in SAP Business Application Studio and connecting it to HANA Cloud was an exciting milestone. From the initial curiosity to the small hurdles like installing hdbcli in the notebook and finally seeing my script return results, every step felt like a mini victory.&lt;/P&gt;&lt;P&gt;That simple output from HANA Cloud made all the effort worthwhile and gave me a real sense of accomplishment.&lt;/P&gt;&lt;P&gt;This experience has sparked my curiosity to explore more complex queries, data analysis, and automation using Python in SAP.&lt;/P&gt;&lt;P&gt;I hope my journey inspires others to take that first step and discover how fun and powerful working with Python and HANA Cloud can be.&lt;/P&gt;&lt;P&gt;Chao.&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/hello-python-my-first-script-in-sap-bas-connecting-to-hana-cloud/ba-p/14228993"/>
    <published>2025-09-26T13:05:26.454000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/financial-management-blog-posts-by-sap/sap-cpq-2511-scripting-for-custom-quote-actions-amp-quote-item-custom/ba-p/14253497</id>
    <title>SAP CPQ 2511 - Scripting for Custom Quote Actions &amp; Quote Item Custom Fields Access Control</title>
    <updated>2025-10-26T14:38:16.450000+01:00</updated>
    <author>
      <name>Yogananda</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/75</uri>
    </author>
    <content>&lt;P&gt;This blog introduces the scripting enhancements in SAP CPQ 2511, tested and verified in my pre-release version. The scripting examples provided here are optimized for implementation and serve as a valuable resource for CPQ developers, functional consultants, and integration specialists seeking for practical guidance.&lt;BR /&gt;&lt;BR /&gt;Special thanks to Nikola &amp;amp; Pavithran for upgrading to 2511 for pre-release testing&lt;/P&gt;&lt;P&gt;Follow for more updates from&amp;nbsp; : &lt;A href="https://profile.sap.com/u/Yogananda" target="_self" rel="noopener noreferrer"&gt;Yogananda Muthaiah&lt;/A&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="2025-10-26_14-05-29.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/332560i4533DD2CD067FDDD/image-size/large?v=v2&amp;amp;px=999" role="button" title="2025-10-26_14-05-29.png" alt="2025-10-26_14-05-29.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId-1892778276"&gt;Dynamic Access Control for Quote Item Custom Fields&lt;/H3&gt;&lt;P&gt;Access level permissions on Quote Item Custom Fields can now be dynamically set through scripting in &lt;SPAN class=""&gt;Quote&lt;/SPAN&gt; 2.0. This feature allows users to control the editability, read-only status, and visibility of fields, enhancing customization and security in managing quotes.&lt;/P&gt;&lt;P class=""&gt;&lt;FONT color="#FF00FF"&gt;Here's an example script:&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from Scripting.Quote import QuoteFieldAccessLevel

for item in context.PagedItems:
    QuoteFieldAccessContext.SetAccessLevel(item, 'TargetPrice', QuoteFieldAccessLevel.Hidden)
    QuoteFieldAccessContext.SetAccessLevel(item, 'TargetPrice', QuoteFieldAccessLevel.Readonly)
    QuoteFieldAccessContext.SetAccessLevel(item, 'TargetPrice', QuoteFieldAccessLevel.Editable)


eachItem = context.Quote.GetItemByItemNumber(1)
QuoteFieldAccessContext.SetAccessLevel(eachItem, 'TargetPrice', QuoteFieldAccessLevel.ReadOnly)  

QuoteFieldAccessContext.SetColumnAccessLevel('TargetPrice', QuoteFieldAccessLevel.ReadOnly)


QuoteFieldAccessContext.SetAccessLevelForProductType(39, 'TargetPrice', QuoteFieldAccessLevel.ReadOnly)

QuoteFieldAccessContext.SetAccessLevelForSection('ProductType', 'TargetPrice', QuoteFieldAccessLevel.Editable)

QuoteFieldAccessContext.SetAccessLevelForCartTotal&lt;/code&gt;&lt;/pre&gt;&lt;DIV&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="2025-10-26_14-10-35.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/332558i583D3AF65C315A6D/image-size/large?v=v2&amp;amp;px=999" role="button" title="2025-10-26_14-10-35.png" alt="2025-10-26_14-10-35.png" /&gt;&lt;/span&gt;&lt;/DIV&gt;&lt;DIV&gt;&amp;nbsp;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/DIV&gt;&lt;H3 id="toc-hId-1696264771"&gt;Executing Custom Quote Actions via Scripting&lt;/H3&gt;&lt;P class=""&gt;As of the 2511 release, you can execute custom quote actions in &lt;SPAN class=""&gt;Quote&lt;/SPAN&gt; 2.0 within the Workflow context through scripting.&lt;/P&gt;&lt;P class=""&gt;The &lt;SPAN class=""&gt;SAP CPQ&lt;/SPAN&gt; system checks all the permissions and conditions defined in the Workflow for these actions to ensure that only available custom quote actions are executed. For each available action, the system performs pre-actions and post-actions and sends notifications. The list of available custom quote actions is also returned in scripting.&lt;/P&gt;&lt;P class=""&gt;&lt;FONT color="#FF00FF"&gt;Here's an example script:&lt;/FONT&gt;&lt;/P&gt;&lt;DIV class=""&gt;&amp;nbsp;&lt;/DIV&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;customQuoteactionAvailable = WorkflowExecutor.IsActionAvailableForQuote(3102, context.Quote.Id)
WorkflowExecutor.ExecuteActionOnQuote(3102, context.Quote.Id)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H3 id="toc-hId-1499751266"&gt;Enhanced ExtenalItemID Editing via API and Scripting&lt;/H3&gt;&lt;P class=""&gt;As of 2511 release, extenalItemID can be edited, updated and accessed through API and scripting. This enhancement enables more flexible and efficient data management and integration scenarios.&lt;/P&gt;&lt;P class=""&gt;&lt;SPAN&gt;to enable ExternalItemId to be changed from the Scripting&lt;/SPAN&gt;&lt;/P&gt;&lt;P class=""&gt;&lt;FONT color="#FF00FF"&gt;Here's an example script:&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;Items = context.Quote.GetAllItems()
Items[0].ExternalItemId = 'Testing from Yoga to update ExternalItemId '

Trace.Write(Items[0].ExternalItemId)&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-1303237761"&gt;Improved &lt;FONT color="#FF6600"&gt;RestClient &lt;/FONT&gt;Methods for Decompressed Responses&lt;/H3&gt;&lt;P&gt;&lt;SPAN class=""&gt;RestClients&lt;/SPAN&gt; previously returned unreadable strings for compressed responses. To address this issue and support the decompression of the response body for compressed responses using Gzip or Deflate compression methods, a new &lt;SPAN class=""&gt;decompressResponse&lt;/SPAN&gt; parameter was added. &lt;FONT color="#FF6600"&gt;By default, it is set to False,&lt;/FONT&gt; maintaining current functionality.&lt;/P&gt;&lt;P&gt;&lt;FONT color="#0000FF"&gt;Setting it to True&lt;/FONT&gt; automatically decompresses responses into readable JSON, allowing users to handle compressed responses without changing existing calls unless needed.&lt;/P&gt;&lt;P&gt;&lt;FONT color="#FF00FF"&gt;Here's an example script:&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;authorization_token = RestClient.GetBasicAuthenticationHeader("YMUTHAIAH", "XXXXXXXXXXXXXXX")
headers = { "Authorization": authorization_token}
token_url = "https://XXXXXXXXX.de1.demo.crm.cloud.sap/sap/c4c/api/v1/iam-service/token"
token = RestClient.Get(token_url,headers )
 
aa = token.value.access_token
 
url = "https://XXXXXXXXX.de1.demo.crm.cloud.sap/sap/c4c/api/v1/document-service/documents/"
headers1 = { "Authorization": "Bearer " + str(aa),
           'Accept': 'application/json; charset=UTF-8'}
 
json_body = {
    'isSelected':'false',
    'isDisplayDocument':'true',
    'fileName': 'CPQ-Document.xlsx',
    'category': 'DOCUMENT',
    'type': '10001'
}
 
newdata=JsonHelper.Deserialize(JsonHelper.Serialize(json_body))
response = RestClient.Post(url, newdata, headers1, True)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H3 id="toc-hId-1106724256"&gt;Script Fix for &lt;SPAN class=""&gt;Quote&lt;/SPAN&gt; 2.0 Alternatives&lt;/H3&gt;&lt;P class=""&gt;The IronPython script issue in &lt;SPAN class=""&gt;Quote&lt;/SPAN&gt; 2.0, where newly added alternative items were incorrectly flagged and excluded from calculations, has been resolved.&lt;/P&gt;&lt;P class=""&gt;This fix ensures accurate total and product type calculations, making it essential for users who rely on scripting to manage quote alternatives efficiently.&lt;/P&gt;&lt;P class=""&gt;&lt;FONT color="#FF00FF"&gt;Here's an example script:&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;quote = QuoteHelper.Get("00021088")
item = quote.GetItemByItemNumber(1)
altProduct = ProductHelper.CreateProduct('00021088',item.QuoteItem)
alternativeItem = quote.AddItem(altProduct,1).AsMainItem
alternativeItem.ChangeItemTypeToAlternative(item.Id)  &lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/financial-management-blog-posts-by-sap/sap-cpq-2511-scripting-for-custom-quote-actions-amp-quote-item-custom/ba-p/14253497"/>
    <published>2025-10-26T14:38:16.450000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/from-sap-datasphere-to-a-local-llm-llama-3-1-hands-on-tutorial/ba-p/14253357</id>
    <title>From SAP Datasphere to a Local LLM (Llama 3.1)  — Hands-On Tutorial</title>
    <updated>2025-10-29T12:05:30.405000+01:00</updated>
    <author>
      <name>SethiR</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1792324</uri>
    </author>
    <content>&lt;P&gt;&lt;FONT size="6"&gt;&lt;BR /&gt;&lt;/FONT&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Introduction&amp;nbsp;&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;This post documents a small, reproducible pattern for bringing SAP Datasphere data to a local large language model (LLM) for lightweight analysis. The goal is simple: keep modeling and governance in Datasphere, pull a view into a Jupyter notebook with pandas, and let a local LLM produce machine-readable JSON that you can filter, join, or visualize. The prototype runs on a CPU-only laptop so anyone can follow along without special hardware.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;What this is&lt;/STRONG&gt;:&lt;BR /&gt;-&amp;nbsp;A step-by-step walkthrough that uses hdbcli to query a Datasphere view and Transformers to run Meta Llama 3.1 locally.&lt;BR /&gt;-&amp;nbsp;A row-by-row prompt pattern that returns compact JSON (total, average, flags) you can plug back into your DataFrame. A row-by-row prompt pattern that returns compact JSON (total, average, flags) you can plug back into your DataFrame.&lt;BR /&gt;-&amp;nbsp;A row-by-row prompt pattern that returns compact JSON (total, average, flags) you can plug back into your DataFrame.&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;What this is not:&lt;/STRONG&gt;&lt;BR /&gt;-&amp;nbsp;A benchmarking or performance guide. CPU runs are slow but convenient for learning.&lt;BR /&gt;-&amp;nbsp;A recommendation to hard-code credentials. The POC mirrors the original notebook for clarity; use environment variables or a vault in real projects.A recommendation to hard-code credentials. The POC mirrors the original notebook for clarity; use environment variables or a vault in real projects.&lt;BR /&gt;-&amp;nbsp;A pattern for sending sensitive data to external endpoints. Use test or masked data if you move past local inference. A pattern for sending sensitive data to external endpoints. Use test or masked data if you move past local inference.&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;What you will build:&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;A compact flow: SAP Datasphere View -&amp;gt; Python/Jupyter (hdbcli + pandas) -&amp;gt; row-level prompt -&amp;gt; Local LLM (Transformers) -&amp;gt; JSON back to DataFrame. The same prompts can be pointed to managed inference later for production.&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;H2 id="toc-hId-1763694472"&gt;Prerequisites&lt;/H2&gt;&lt;P&gt;1.&amp;nbsp;SAP Datasphere space with permission to create a Database User and a SQL View&lt;BR /&gt;2.&amp;nbsp;Python 3.10+ with Jupyter&lt;BR /&gt;3.&amp;nbsp;Libraries: pandas, hdbcli, transformers, torch, accelerate, ipython&lt;BR /&gt;4.&amp;nbsp;Hugging Face account + access token (accept access for the Llama 3.1 model)&lt;BR /&gt;&lt;BR /&gt;Security note: The POC code below uses inline credentials to mirror the original run. In real work, put secrets in environment variables or a vault and keep TLS validation enabled.&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Part A — SAP Datasphere&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;A1. Enable database access for the space&lt;/P&gt;&lt;P&gt;1.&amp;nbsp;Open SAP Datasphere -&amp;gt; Spaces -&amp;gt; select your space.&lt;BR /&gt;2.&amp;nbsp;Go to Database Access and confirm SQL access is enabled for the space&amp;nbsp;&lt;/P&gt;&lt;P&gt;A2. Create a Database User&lt;/P&gt;&lt;P&gt;1.&amp;nbsp;Database Access -&amp;gt; Database Users -&amp;gt; Create.Database Access -&amp;gt; Database Users -&amp;gt; Create.&lt;BR /&gt;2.&amp;nbsp;Grant only read privileges/necessary privileges to the schema/view you will query.&lt;BR /&gt;3.&amp;nbsp;Copy the SQL Endpoint (host) and port 443 for Python connectivity. Make sure you copy password and host details and store it in a safe place.&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;A3. Create a demo view with a few rows&lt;/P&gt;&lt;P&gt;Create a SQL View (or graphical view). Here we have taken&amp;nbsp; "ACN_DWC"."DemoView_SETHIR_PY" with columns:&lt;BR /&gt;Stud_ID, Stud_Fname, Stud_Lname, Stud_DOB, Maths, Physics, Chemistry, Total, Stud_Addr, Stud_Faname for the demo.&lt;BR /&gt;Make sure the view is exposed for Consumption.&lt;BR /&gt;Optional seed SQL you can adapt:&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-sql"&gt;&lt;code&gt;SELECT * FROM (
  SELECT 101 AS Stud_ID, 'Arjun'     AS Stud_Fname, 'Singh'    AS Stud_Lname, DATE'2012-04-06' AS Stud_DOB,
         80 AS Maths, 75 AS Physics, 80 AS Chemistry, 235 AS Total, 'Chandigarh' AS Stud_Addr, 'Sukhdev' AS Stud_Faname
  UNION ALL
  SELECT 102, 'Harpreet', 'Kaur',  DATE'2010-09-08', 85, 78, 85, 248, 'Ludhiana',  'Gurinder'
  UNION ALL
  SELECT 103, 'Gursimran','Gill',  DATE'2011-07-09', 95, 85, 90, 270, 'Amritsar',  'Balwinder'
  UNION ALL
  SELECT 104, 'Manpreet', 'Sidhu', DATE'2014-01-01', 90, 85, 95, 270, 'Patiala',   'Harjit'
  UNION ALL
  SELECT 105, 'Jasleen',  'Dhillon',DATE'2012-03-02', 89, 90, 75, 254, 'Jalandhar', 'Paramjit'
) AS t;
SELECT * FROM (
  SELECT 101 AS Stud_ID, 'Arjun'     AS Stud_Fname, 'Singh'    AS Stud_Lname, DATE'2012-04-06' AS Stud_DOB,
         80 AS Maths, 75 AS Physics, 80 AS Chemistry, 235 AS Total, 'Chandigarh' AS Stud_Addr, 'Sukhdev' AS Stud_Faname
  UNION ALL
  SELECT 102, 'Harpreet', 'Kaur',  DATE'2010-09-08', 85, 78, 85, 248, 'Ludhiana',  'Gurinder'
  UNION ALL
  SELECT 103, 'Gursimran','Gill',  DATE'2011-07-09', 95, 85, 90, 270, 'Amritsar',  'Balwinder'
  UNION ALL
  SELECT 104, 'Manpreet', 'Sidhu', DATE'2014-01-01', 90, 85, 95, 270, 'Patiala',   'Harjit'
  UNION ALL
  SELECT 105, 'Jasleen',  'Dhillon',DATE'2012-03-02', 89, 90, 75, 254, 'Jalandhar', 'Paramjit'
) AS t;&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;SAP Datasphere DB user :&amp;nbsp;&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="SAP Datasphere DB user screen" style="width: 521px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/332993i05D97A6906259C90/image-dimensions/521x754?v=v2" width="521" height="754" role="button" title="Screenshot 2025-10-27 170111.png" alt="SAP Datasphere DB user screen" /&gt;&lt;span class="lia-inline-image-caption" onclick="event.preventDefault();"&gt;SAP Datasphere DB user screen&lt;/span&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Demo View :&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="SethiR_0-1761564892069.png" style="width: 709px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/332994i6C5415EEA2B13F92/image-dimensions/709x443?v=v2" width="709" height="443" role="button" title="SethiR_0-1761564892069.png" alt="SethiR_0-1761564892069.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Part B — Local environment (quick path)&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="3"&gt;1. Create a project folder and venvCreate a project folder and venv&lt;BR /&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-bash"&gt;&lt;code&gt;mkdir datasphere-local-llm &amp;amp;&amp;amp; cd datasphere-local-llm
python -m venv .venv
# Windows: .venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;2.&amp;nbsp;Install requirements&amp;nbsp;&lt;/P&gt;&lt;pre class="lia-code-sample language-bash"&gt;&lt;code&gt;pip install hdbcli
pip install sqlalchemy
pip install sqlalchemy-hana
pip install pandas
pip install hdbcli
pip install transformers torch accelerate
pip install ipython&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;3.&amp;nbsp;Launch Jupyter&lt;/P&gt;&lt;pre class="lia-code-sample language-bash"&gt;&lt;code&gt;jupyter notebook&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Part C — Original POC notebook&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="3"&gt;1. Open a new notebook and write the code.&lt;BR /&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# --- Imports and setup (original)
import pandas as pd
from hdbcli import dbapi
import warnings
from transformers import pipeline
from IPython.display import display
warnings.filterwarnings('ignore')

# --- Inline credentials (POC-style; replace with environment variables for real use)
db_user = 'ACN_DWC#SETHIR_DB'
db_password = 'secret'
db_host = 'secret'
db_port = 443
db_schema = 'ACN_DWC'


connection = dbapi.connect(
    address = db_host,
    port = db_port,
    user = db_user,
    password = db_password,
    encrypt = True,
    sslValidCertificate = False   # POC-only convenience; prefer True in real use
)
print("Connected to SAP Datasphere - confirmation")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;2. If you see the output - "Connected to SAP Datasphere - confirmation" -- this implies you are on right track.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# --- Query the view
view_name = 'DemoView_SETHIR_PY'
sql_query = f'SELECT * FROM "{db_schema}"."{view_name}"'
print(f"Executing query: {sql_query}")

cursor = connection.cursor()
cursor.execute(sql_query)
rows = cursor.fetchall()
columns = [desc[0] for desc in cursor.description]

df = pd.DataFrame(rows, columns=columns)
display(df.head())&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;3. At this point, you should see the outcome of the view that you have. The last statement is used for displaying the data in the form of pandas dataFrame.&lt;BR /&gt;&lt;BR /&gt;4. Now we need to prepare our data , so that the LLM can process it. This can be done by converting our data in the form of string format.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# ==============================================================================
#Prepare Data and Interact with OpenLLM
# ==============================================================================

# Convert DataFrame to a string
data_as_string = df.to_string(index=False)
print("\n--- Data Prepared for LLM ---")
print("The DataFrame has been converted to the following string format:")
print(data_as_string[:300] + "\n...")  # snippet preview&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;5. Prepare data pipeline :&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# --- LLM imports
import os
import torch
import json
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline

# --- LLM Configuration
MODEL_ID = "meta-llama/Meta-Llama-3.1-8B-Instruct"

def load_llama_pipeline():
    """
    Loads the quantized Llama 3 model, tokenizer, and the text-generation pipeline.
    """
    print("\n--- Loading Llama 3 Model ---")
    hf_token = "secret"  # replace with your HF token; accept model access first

    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=hf_token)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        token=hf_token,
        quantization_config=quantization_config,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )

    print("Model loaded. Creating text generation pipeline...")
    return pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
    )&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;6. Build the prompt for the LLM model :&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# --- Prompt builder 
def build_student_analysis_prompt(tokenizer, student_data: pd.Series) -&amp;gt; str:
    """
    Builds a structured prompt for Llama 3 to analyze student marks.
    """
    input_text = (
        f"Student {student_data['Stud_Fname']} {student_data['Stud_Lname']} "
        f"(ID: {student_data['Stud_ID']}) scored {student_data['Maths']} in Maths, "
        f"{student_data['Physics']} in Physics, and {student_data['Chemistry']} in Chemistry."
    )

    messages = [
        {
            "role": "system",
            "content": (
                "You analyze one student's marks and return ONLY a valid JSON object. "
                "Output must start with '{' and end with '}'. No commentary, no markdown, no backticks.\n\n"
                "Schema (keys and types MUST match exactly):\n"
                "{\n"
                '  "total_marks": &amp;lt;int&amp;gt;,\n'
                '  "average_percentage": &amp;lt;float&amp;gt;,\n'
                '  "is_top_performer": &amp;lt;boolean&amp;gt;\n'
                "}\n\n"
                "Rules:\n"
                "1) total_marks = Maths + Physics + Chemistry (each out of 100).\n"
                "2) average_percentage = total_marks / 3.\n"
                "3) Round average_percentage to TWO decimals.\n"
                "4) is_top_performer = true if average_percentage &amp;gt; 80.0; else false.\n"
                "5) Use lowercase true/false for booleans.\n"
                "6) Do not include extra keys. Do not include trailing commas.\n"
                "7) Return JSON only."
            )
        },
        {
            "role": "user",
            "content": input_text
        }
    ]
    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;7. Do the analysis :&amp;nbsp;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# --- Row-by-row analysis
def analyze_student_data(df, text_pipeline):
    """
    Analyzes the student DataFrame row by row using the LLM pipeline.
    """
    print("\n--- Starting Student Performance Analysis with Llama 3 ---")
    results = []
    for index, row in df.iterrows():
        print(f"\nAnalyzing student ID: {row['Stud_ID']}...")

        prompt = build_student_analysis_prompt(text_pipeline.tokenizer, row)

        raw_output = text_pipeline(
            prompt,
            max_new_tokens=128,
            do_sample=False,  # deterministic
            temperature=None,
            top_p=None,
        )[0]['generated_text']

        json_response_str = raw_output[len(prompt):].strip()
        try:
            analysis_result = json.loads(json_response_str)
            results.append(analysis_result)
            print("Analysis successful.")
        except json.JSONDecodeError:
            print(f"  &amp;gt; Failed to decode JSON from model output.")
            print(f"  &amp;gt; Raw model output: {json_response_str}")
            results.append({"error": "Invalid JSON output", "raw_output": json_response_str})

    print("\n--- Analysis Complete ---")
    return pd.DataFrame(results)

# --- Main
if __name__ == "__main__":
    if df is not None and not df.empty:
        try:
            llm_pipeline = load_llama_pipeline()
            analysis_df = analyze_student_data(df, llm_pipeline)

            if analysis_df is not None:
                final_df = pd.concat([df.reset_index(drop=True), analysis_df.reset_index(drop=True)], axis=1)

                print("\n--- Full Data with LLM Analysis ---")
                display(final_df)

                print("\n--- Top Performing Students (Average &amp;gt; 80%) ---")
                top_performers = final_df[final_df['is_top_performer'] == True]

                if not top_performers.empty:
                    display(top_performers[['Stud_ID', 'Stud_Fname', 'Stud_Lname', 'total_marks', 'average_percentage']])
                else:
                    print("No students found with an average greater than 80%.")

        except Exception as e:
            print(f"\nAn unexpected error occurred during the analysis process: {e}")
    else:
        print("\nDataFrame is empty. Cannot proceed with analysis.")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;8.&amp;nbsp;Example output (simulated for the final cell)&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-markup"&gt;&lt;code&gt;Full Data with LLM Analysis (excerpt)

Stud_ID  Stud_Fname  Stud_Lname  Maths  Physics  Chemistry  total_marks  average_percentage  is_top_performer
101      Arjun       Singh       80     75       80         235          78.33               false
102      Harpreet    Kaur        85     78       85         248          82.67               true
103      Gursimran   Gill        95     85       90         270          90.00               true
104      Manpreet    Sidhu       90     85       95         270          90.00               true
105      Jasleen     Dhillon     89     90       75         254          84.67               true

Top Performing Students (Average &amp;gt; 80%)

Stud_ID  Stud_Fname  Stud_Lname  total_marks  average_percentage
102      Harpreet    Kaur        248          82.67
103      Gursimran   Gill        270          90.00
104      Manpreet    Sidhu       270          90.00
105      Jasleen     Dhillon     254          84.67&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Part D —&amp;nbsp;Production path (same pattern, managed inference)&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="3"&gt;When performance or scale matters, swap the local pipeline for a hosted endpoint and keep the Datasphere extraction and prompts identical. When performance or scale matters, swap the local pipeline for a hosted endpoint and keep the Datasphere extraction and prompts identical.&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="3"&gt;1.&amp;nbsp;Databricks Model Serving / Foundation Model Endpoints (REST).&lt;BR /&gt;2.&amp;nbsp;Hugging Face Inference Endpoints (private endpoints; REST with your HF token)&lt;BR /&gt;3.&amp;nbsp;SAP AI Core (containerized model hosting; REST)SAP AI Core (containerized model hosting; REST)&lt;BR /&gt;&lt;BR /&gt;Minimal REST skeleton --&lt;BR /&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import requests, os, json
endpoint = os.getenv("INFERENCE_URL")
token = os.getenv("INFERENCE_TOKEN")
r = requests.post(endpoint, headers={"Authorization": f"Bearer {token}"},
                  json={"inputs": "your_prompt_here", "parameters": {"max_new_tokens": 128, "temperature": 0.0}})
print(r.json())&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;FONT size="3"&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;FONT size="5"&gt;Conclusion ---&lt;BR /&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;This walkthrough demonstrated how to extract data from SAP Datasphere, process it in pandas, and apply a local LLM for row-level analysis that returns clean, machine-readable JSON. The pattern is intentionally small and portable: you can keep iterating locally to refine prompts and outputs, then swap the model call for a managed endpoint (Databricks, Hugging Face, or SAP AI Core) when performance, cost control, or governance call for it. From here, natural next steps include batching larger datasets, persisting results back to a database table, wiring the outputs to SAP Analytics Cloud dashboards, and adding guardrails around data privacy and prompt consistency.&lt;/P&gt;&lt;P&gt;I would be excited to hear how you adapt this to your solutions. Feel free to reach out to me or comment.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/from-sap-datasphere-to-a-local-llm-llama-3-1-hands-on-tutorial/ba-p/14253357"/>
    <published>2025-10-29T12:05:30.405000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/sap-datasphere-export-data-of-analyticalmodel-via-odata-url-amp-oauth/ba-p/14256993</id>
    <title>SAP Datasphere : Export Data of AnalyticalModel via Odata URL &amp; Oauth Client of type Technical User</title>
    <updated>2025-11-05T07:58:30.289000+01:00</updated>
    <author>
      <name>vikasparmar88</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1528256</uri>
    </author>
    <content>&lt;P&gt;In this blog, I have explained how to export data from an analytical model into CSV file securely using an OData URL and a technical user OAuth client.&lt;/P&gt;&lt;P&gt;Step - 1) Create an Oauth Client with Purpose as Technical User and select required roles. get client ID and secret and save it.&amp;nbsp;&lt;A href="https://help.sap.com/docs/SAP_DATASPHERE/9f804b8efa8043539289f42f372c4862/88b13468fc3c4ebd972bcb8faa6cafbf.html" target="_self" rel="noopener noreferrer"&gt;How to Guide&lt;/A&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Oauth.png" style="width: 347px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334244iFF68CE80547515B7/image-dimensions/347x368?v=v2" width="347" height="368" role="button" title="Oauth.png" alt="Oauth.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Step - 2) Create Odata_Tech_User_OauthClient.py file with below code&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# --- IMPORTS ---
import requests              # For making HTTP requests to token and OData endpoints
import pandas as pd          # For handling tabular data from the OData response
import os                    # For file path resolution and environment variable access
from dotenv import load_dotenv  # For loading credentials from a .env file

# --- LOAD ENV VARIABLES ---
load_dotenv()                # Load environment variables from .env file into the runtime

# --- CONFIG: Read credentials and endpoints from environment ---
odata_url = os.getenv("ODATA_URL")             # OData service endpoint
token_url = os.getenv("TOKEN_URL")             # OAuth token endpoint
client_id = os.getenv("CLIENT_ID")             # OAuth client ID
client_secret = os.getenv("CLIENT_SECRET")     # OAuth client secret

# --- GET TOKEN: Request access token using client credentials ---
token_payload = {
    "grant_type": "client_credentials",        # OAuth flow type
    "client_id": client_id,                    # Injected from .env
    "client_secret": client_secret             # Injected from .env
}
token_resp = requests.post(token_url, data=token_payload)  # POST request to token endpoint
token_resp.raise_for_status()                 # Raise error if token request fails
access_token = token_resp.json()["access_token"]  # Extract access token from response

# --- CALL ODATA SERVICE: Fetch data using bearer token ---
headers = {"Authorization": f"Bearer {access_token}"}  # Auth header with token
response = requests.get(odata_url, headers=headers)    # GET request to OData endpoint
response.raise_for_status()                            # Raise error if data fetch fails

# --- PARSE RESULTS: Convert JSON payload to DataFrame ---
data = response.json()["value"]         # Extract 'value' list from OData response
df = pd.DataFrame(data)                 # Convert list of records to pandas DataFrame

# --- DISPLAY OR EXPORT: Show preview and optionally save to CSV ---
print()
print("🔍 Displaying top rows for quick inspection:")
print()
print(df.head())                        # Display top rows for quick inspection
print()

# --- Extract view name from OData URL ---
view_name = odata_url.rstrip("/").split("/")[-1]  # Gets 'AM_EXPORT' from the URL

# --- Construct filename ---
filename = f"{view_name}.csv"

# --- Display and save ---
df.to_csv(filename, index=False)
print(f"📁 Exported data Saved to: {os.path.abspath(filename)}")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Step - 3) Create .env file with all values of variables.&lt;/P&gt;&lt;P&gt;ODATA_URL : Copy the Odata link from analytical model&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="odata.png" style="width: 630px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334255i61C52D8A5777AD69/image-dimensions/630x162?v=v2" width="630" height="162" role="button" title="odata.png" alt="odata.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="odata.png" style="width: 517px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334257iF036F0A98768B92D/image-dimensions/517x297?v=v2" width="517" height="297" role="button" title="odata.png" alt="odata.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;TOKEN_URL : get it from Datasphere -&amp;gt; System -&amp;gt; App Integration page&lt;/P&gt;&lt;P&gt;CLIENT ID &amp;amp; Secret : Get it from Step-1&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Variables.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334246iBF8AA617FEDF18B0/image-size/large?v=v2&amp;amp;px=999" role="button" title="Variables.png" alt="Variables.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Step-&amp;nbsp; 4) Create .bat file with blow code&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt; off

chcp 65001 &amp;gt;nul

REM ───────────────────────────────────────────────
REM 🚀 ODATA TECH USER OAUTH CLIENT EXECUTION SCRIPT
REM ───────────────────────────────────────────────


echo.
echo ==============================================
echo 🔄 Starting OData export process...
echo ==============================================

REM --- Navigate to script directory ---
cd /d "%~dp0"

REM --- Run the Python script ---
echo 🐍 Running Python script: Odata_Tech_User_OauthClient.py
python Odata_Tech_User_OauthClient.py

REM --- Completion message ---
echo.
echo ✅ Script execution completed.
echo ==============================================

REM --- Keep window open ---
pause&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Keep all files in same directory&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Folder.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334250i25167CB60103AFB0/image-size/large?v=v2&amp;amp;px=999" role="button" title="Folder.png" alt="Folder.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;!--  StartFragment   --&gt;&lt;/P&gt;&lt;P&gt;Once everything is set up, just double-click the .bat&amp;nbsp;file to run the process. It will execute the Python script and, once finished, generate a .csv file name exactly same name as the analytical model name. As part of the execution, the first five rows of data will also be displayed on screen for quick preview&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="output.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334261i1BE9F41A80C0F984/image-size/large?v=v2&amp;amp;px=999" role="button" title="output.png" alt="output.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Folder.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334248iA75EC24BCB3F6067/image-size/large?v=v2&amp;amp;px=999" role="button" title="Folder.png" alt="Folder.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Thanks&lt;/P&gt;&lt;P&gt;Vikas Parmar&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/sap-datasphere-export-data-of-analyticalmodel-via-odata-url-amp-oauth/ba-p/14256993"/>
    <published>2025-11-05T07:58:30.289000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/sap-learning-blog-posts/want-to-explore-developing-ai-workflows-with-the-python-machine-learning/ba-p/14263178</id>
    <title>Want to explore developing AI workflows with the Python Machine Learning Client for SAP HANA?</title>
    <updated>2025-11-07T16:40:45.744000+01:00</updated>
    <author>
      <name>Margit_Wagner</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/491</uri>
    </author>
    <content>&lt;P data-unlink="true"&gt;&lt;FONT size="3"&gt;&lt;SPAN&gt;I&amp;nbsp;recommend to access our&amp;nbsp;&lt;/SPAN&gt;&lt;/FONT&gt;&lt;A title="Developing AI workflows with the Python Machine Learning Client for SAP HANA" href="https://learning.sap.com/learning-journeys/developing-ai-workflows-with-the-python-machine-learning-client-for-sap-hana" target="_blank" rel="noopener noreferrer"&gt;Developing AI workflows with the Python Machine Learning Client for SAP HANA&lt;/A&gt;&amp;nbsp; learning journey.&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;Leaning Objectives&lt;/STRONG&gt;&lt;BR /&gt;By the end of this learning journey, learners will be able to:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Know how to install and configure the Python Machine Learning Client (hana-ml) and connect securely to SAP HANA Cloud.&lt;/LI&gt;&lt;LI&gt;Apply HANA DataFrames to access, prepare, and explore SAP HANA data for machine learning tasks.&lt;/LI&gt;&lt;LI&gt;Think critically to build, train, and deploy machine learning models using PAL functions for regression, classification, and time-series forecasting.&lt;/LI&gt;&lt;LI&gt;Impact business outcomes by using real-world datasets (e.g., housing prices, employee churn) to complete end-to-end ML workflows.&lt;/LI&gt;&lt;LI&gt;Evaluate models using appropriate metrics to assess performance, robustness, and business relevance.&lt;/LI&gt;&lt;/UL&gt;&lt;P data-unlink="true"&gt;&lt;STRONG&gt;Goals&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Get started with SAP: Building the basics&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;Develop your expertise: Skill deepening learning&lt;/LI&gt;&lt;LI&gt;Excel in your expertise: Advanced specialization learning&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Prerequisites&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV&gt;&lt;P&gt;Basic python expertise&lt;/P&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Please post you question related&amp;nbsp;to the digital learning Journey in the&amp;nbsp;&lt;/STRONG&gt;&lt;A href="https://groups.community.sap.com/t5/sap-learning-q-a/qa-p/learningqanda-board" target="_blank" rel="noopener noreferrer"&gt;&lt;STRONG&gt;Q&amp;amp;A area&lt;/STRONG&gt;&lt;/A&gt;&lt;STRONG&gt;.&amp;nbsp;&lt;/STRONG&gt;&lt;/P&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;P&gt;Our SAP Learning Experts will get back to you as soon as possible!&amp;nbsp;&lt;BR /&gt;We are here to support you.&lt;/P&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;BR /&gt;I appreciate your feedback and we will make sure to continue sharing interesting topics.&lt;BR /&gt;&lt;BR /&gt;Kind regards&lt;BR /&gt;Margit&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;</content>
    <link href="https://community.sap.com/t5/sap-learning-blog-posts/want-to-explore-developing-ai-workflows-with-the-python-machine-learning/ba-p/14263178"/>
    <published>2025-11-07T16:40:45.744000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/how-to-reduce-prompt-token-costs-using-toon-save-money/ba-p/14267265</id>
    <title>How to Reduce Prompt Token Costs Using Toon = Save Money</title>
    <updated>2025-11-12T17:34:51.069000+01:00</updated>
    <author>
      <name>Yogananda</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/75</uri>
    </author>
    <content>&lt;P&gt;As you already might know prompt tokens are the backbone of communication with large language models (LLMs). However, as usage scales, token costs can quickly become a significant expense. If you’re using &lt;STRONG&gt;Toon&lt;/STRONG&gt;—a tool designed for optimizing prompt workflows—you can dramatically cut down on these costs without sacrificing performance.&lt;/P&gt;&lt;H3 id="toc-hId-1893818944"&gt;&lt;STRONG&gt;Why Token Costs Matter&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;Every interaction with an LLM consumes tokens. These tokens represent:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Input tokens&lt;/STRONG&gt;: The text you send to the model.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Output tokens&lt;/STRONG&gt;: The text generated by the model.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;The more tokens you use, the higher your bill. For businesses running thousands of prompts daily, even small inefficiencies can lead to big costs.&lt;/P&gt;&lt;P&gt;&lt;FONT color="#FF00FF"&gt;&lt;STRONG&gt;Example:&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Original prompt: &lt;EM&gt;“Please summarize the following text in a clear and concise manner, highlighting the key points and provide a RACI Matrix for S/4 HANA”&lt;/EM&gt;&lt;/LI&gt;&lt;LI&gt;Compressed prompt: &lt;EM&gt;“Summarize key points.”&lt;/EM&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;H3 id="toc-hId-1697305439"&gt;&lt;FONT color="#800080"&gt;&lt;STRONG&gt;What is TOON?&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/H3&gt;&lt;P&gt;TOON is a &lt;STRONG&gt;compact, human-readable serialization format&lt;/STRONG&gt; designed for passing structured data to LLMs with &lt;STRONG&gt;significantly reduced token usage&lt;/STRONG&gt;. It acts as a &lt;STRONG&gt;lossless, drop-in representation of JSON&lt;/STRONG&gt;, optimized for token efficiency.&lt;/P&gt;&lt;H3 id="toc-hId-1500791934"&gt;&lt;FONT color="#800080"&gt;&lt;STRONG&gt;Why Use TOON?&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/H3&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Token-efficient&lt;/STRONG&gt;: Saves &lt;STRONG&gt;30–60% tokens&lt;/STRONG&gt; compared to formatted JSON for large uniform arrays.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;LLM-friendly&lt;/STRONG&gt;: Explicit lengths and fields improve parsing and validation.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Minimal syntax&lt;/STRONG&gt;: Removes redundant punctuation (braces, quotes).&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Tabular arrays&lt;/STRONG&gt;: Declare keys once, stream data as rows.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Optional key folding&lt;/STRONG&gt;: Collapses nested chains into dotted paths for fewer tokens.&lt;/LI&gt;&lt;/UL&gt;&lt;H3 id="toc-hId-1304278429"&gt;&lt;STRONG&gt;Benchmarks&lt;/STRONG&gt;&lt;/H3&gt;&lt;UL&gt;&lt;LI&gt;TOON uses &lt;STRONG&gt;39.6% fewer tokens&lt;/STRONG&gt; than JSON while improving retrieval accuracy (73.9% vs 69.7%).&lt;/LI&gt;&lt;LI&gt;For uniform tabular data, TOON is slightly larger than CSV (+6%) but far smaller than JSON (-58%).&lt;/LI&gt;&lt;/UL&gt;&lt;H3 id="toc-hId-1107764924"&gt;&lt;STRONG&gt;When NOT to Use TOON&lt;/STRONG&gt;&lt;/H3&gt;&lt;UL&gt;&lt;LI&gt;Deeply nested or non-uniform structures → JSON may be better.&lt;/LI&gt;&lt;LI&gt;Pure tabular data → CSV is smaller.&lt;/LI&gt;&lt;LI&gt;Latency-critical apps → Benchmark first; compact JSON might be faster.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;How to Use TOON&lt;/STRONG&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-abap"&gt;&lt;code&gt;NPM Lib
npm install -format/toon

Python
https://github.com/xaviviro/python-toon&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;A href="https://github.com/toon-format/toon" target="_blank" rel="noopener nofollow noreferrer"&gt;https://github.com/toon-format/toon&lt;/A&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;A href="https://github.com/toon-format/spec" target="_blank" rel="noopener nofollow noreferrer"&gt;https://github.com/toon-format/spec&lt;/A&gt;&amp;nbsp;&lt;/P&gt;&lt;H3 id="toc-hId-911251419"&gt;&lt;STRONG&gt;Why TOON Matters while your working for different SAP Product LoBs&lt;/STRONG&gt;&lt;/H3&gt;&lt;OL&gt;&lt;LI&gt;&lt;P&gt;&lt;STRONG&gt;Token Cost Reduction&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;SAP S/4 HANA systems handle large datasets (e.g., thousands of line items in a purchase order).&lt;/LI&gt;&lt;LI&gt;JSON representation of these datasets is expensive in token terms.&lt;/LI&gt;&lt;LI&gt;TOON compresses this data by &lt;STRONG&gt;30–60%&lt;/STRONG&gt;, reducing LLM API costs significantly.&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;LI&gt;&lt;P&gt;&lt;STRONG&gt;Performance Gains&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Smaller prompts mean &lt;STRONG&gt;lower latency&lt;/STRONG&gt; and &lt;STRONG&gt;faster response times&lt;/STRONG&gt;.&lt;/LI&gt;&lt;LI&gt;This is critical for real-time SAP applications like &lt;STRONG&gt;Joule for procurement&lt;/STRONG&gt; or &lt;STRONG&gt;HR assistants&lt;/STRONG&gt;.&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;LI&gt;&lt;P&gt;&lt;STRONG&gt;Improved Accuracy&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;TOON’s explicit structure (e.g., tabular arrays with declared keys) helps LLMs parse data better.&lt;/LI&gt;&lt;LI&gt;This reduces hallucinations in SAP workflows like &lt;STRONG&gt;financial reconciliation&lt;/STRONG&gt; or &lt;STRONG&gt;compliance checks&lt;/STRONG&gt;.&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&lt;STRONG&gt;High Level Flow&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Yogananda_0-1762963441825.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/339677iCF14D8DA461534BD/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Yogananda_0-1762963441825.png" alt="Yogananda_0-1762963441825.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H2 id="toc-hId-585655195"&gt;&lt;STRONG&gt;Integration Approach&lt;/STRONG&gt;&lt;/H2&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Middleware Layer&lt;/STRONG&gt;: Convert SAP OData/JSON responses to TOON before sending to LLM.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;SAP BTP Extension&lt;/STRONG&gt;: Implement TOON conversion in &lt;STRONG&gt;CAP,&amp;nbsp;Cloud Foundry apps&lt;/STRONG&gt; or &lt;STRONG&gt;Kyma runtime or SAP Databricks, AI Core Models you have selected&lt;/STRONG&gt;.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Prompt Wrapping&lt;/STRONG&gt;: Always wrap TOON in fenced code blocks for LLM clarity&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="G5qMVMlacAAosFZ.png" style="width: 800px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/340395iDB351B41F1A46C83/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="G5qMVMlacAAosFZ.png" alt="G5qMVMlacAAosFZ.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H2 id="toc-hId-389141690"&gt;&lt;STRONG&gt;Best Practices&lt;/STRONG&gt;&lt;/H2&gt;&lt;UL&gt;&lt;LI&gt;Use &lt;STRONG&gt;tab-delimited rows&lt;/STRONG&gt; for extra token savings.&lt;/LI&gt;&lt;LI&gt;Benchmark TOON vs JSON for your specific SAP dataset.&lt;/LI&gt;&lt;LI&gt;Cache static context to avoid repeated token costs.&lt;/LI&gt;&lt;/UL&gt;&lt;H2 id="toc-hId-192628185"&gt;&lt;STRONG&gt;Conclusion&lt;/STRONG&gt;&lt;/H2&gt;&lt;P&gt;TOON offers a simple yet powerful way to reduce LLM costs in your SAP landscape environments. By compressing structured data without losing meaning, SAP teams can achieve:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Up to 60% cost savings&lt;/STRONG&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Faster response times&lt;/STRONG&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Improved AI accuracy&lt;/STRONG&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;As SAP continues its AI journey, adopting TOON can make intelligent automation more scalable and cost-effective.&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/how-to-reduce-prompt-token-costs-using-toon-save-money/ba-p/14267265"/>
    <published>2025-11-12T17:34:51.069000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/sap-rpt-1-context-model-vs-training-classical-models-the-models-battle/ba-p/14268507</id>
    <title>SAP RPT-1 Context Model vs. Training Classical Models: The Models Battle (Python Hands-on)</title>
    <updated>2025-11-20T07:50:27.670000+01:00</updated>
    <author>
      <name>nicolasestevan</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1198632</uri>
    </author>
    <content>&lt;H2 id="toc-hId-1764768715"&gt;&lt;span class="lia-unicode-emoji" title=":collision:"&gt;💥&lt;/span&gt;The Models Battle&lt;/H2&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_5-1763206328497.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341535i2A2C9A98D24BF43B/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_5-1763206328497.png" alt="nicolasestevan_5-1763206328497.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Predictive modeling is becoming a built-in capability across SAP, improving how teams handle forecasting, pricing, and planning. &lt;STRONG&gt;Many SAP professionals, however, aren’t machine-learning specialists&lt;/STRONG&gt;, and traditional models often demand extensive setup, tuning, and repeated training, which slows down new ideas.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;SAP RPT-1&lt;/STRONG&gt; offers a simpler path. It’s a pretrained model from SAP, also available in an OSS version, that lets developers and consultants produce predictions with far less technical effort, no deep ML background required.&lt;/P&gt;&lt;P&gt;I've explored SAP RPT-1 hands-on, comparing it with traditional regressors using Python and a real public vehicles price dataset.&amp;nbsp;&lt;/P&gt;&lt;BLOCKQUOTE&gt;&lt;P&gt;&lt;STRONG&gt;Goal:&lt;/STRONG&gt; To see (as a non Data Scientist) how &lt;STRONG&gt;SAP RPT-1&lt;/STRONG&gt; behaves in practice, what advantages and limits it shows, and when it could make sense in a predictive scenario.&lt;/P&gt;&lt;/BLOCKQUOTE&gt;&lt;P&gt;Usually for real-world scenario, the right approach would be consume the SAP RPT-1 though the available and simplified API, but for studies proposal and fair comparision over othe traditional ML models, the &lt;STRONG&gt;OSS&lt;/STRONG&gt; fits perfectly for it:&lt;/P&gt;&lt;HR /&gt;&lt;H2 id="toc-hId-1568255210"&gt;&lt;span class="lia-unicode-emoji" title=":thinking_face:"&gt;🤔&lt;/span&gt;&amp;nbsp;SAP RPT-1 vs Traditional Machine Learning - Core Differences&lt;/H2&gt;&lt;P&gt;Before diving into the code, let’s quickly revisit how&lt;STRONG&gt; traditional ML&lt;/STRONG&gt; models work:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Training-based models like Random Forest, LightGBM, and Linear Regression learn patterns directly from data.&amp;nbsp;&lt;/LI&gt;&lt;LI&gt;They require hundreds or thousands of examples to tune their internal parameters.&lt;/LI&gt;&lt;LI&gt;Their performance depends heavily on data quantity and quality.&lt;/LI&gt;&lt;LI&gt;The more relevant examples they see, the smarter they get.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;On the other hand, &lt;STRONG&gt;SAP RPT-1 f&lt;/STRONG&gt;ollows a different philosophy. It’s part of the RPT (Representational Predictive Transformer) family, pretrained on a wide variety of business and contextual data. This means:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;You don’t "train" it in the traditional sense. Instead, it uses context embeddings to predict outcomes.&lt;/LI&gt;&lt;LI&gt;It can be used immediately, even with smaller datasets.&lt;/LI&gt;&lt;LI&gt;The OSS version allows developers to experiment directly in Python.&lt;/LI&gt;&lt;LI&gt;No special SAP backend required.&lt;/LI&gt;&lt;/UL&gt;&lt;BLOCKQUOTE&gt;&lt;P&gt;&lt;STRONG&gt;Outcome:&lt;/STRONG&gt; Traditional ML models learn from high amount of data. SAP RPT-1 already knows how to deal with small context amount of data.&lt;/P&gt;&lt;/BLOCKQUOTE&gt;&lt;HR /&gt;&lt;H2 id="toc-hId-1371741705"&gt;&lt;span class="lia-unicode-emoji" title=":desktop_computer:"&gt;🖥&lt;/span&gt;&amp;nbsp;The Experiment - Setup &amp;amp; Dataset&amp;nbsp;&lt;/H2&gt;&lt;div class="lia-spoiler-container"&gt;&lt;a class="lia-spoiler-link" href="#" rel="nofollow noopener noreferrer"&gt;Spoiler&lt;/a&gt;&lt;noscript&gt; (Highlight to read)&lt;/noscript&gt;&lt;div class="lia-spoiler-border"&gt;&lt;div class="lia-spoiler-content"&gt;Don't worry on "playing puzzles" copying + pasting code below. The full version is available for download at end!&lt;/div&gt;&lt;noscript&gt;&lt;div class="lia-spoiler-noscript-container"&gt;&lt;div class="lia-spoiler-noscript-content"&gt;Don't worry on "playing puzzles" copying + pasting code below. The full version is available for download at end!&lt;/div&gt;&lt;/div&gt;&lt;/noscript&gt;&lt;/div&gt;&lt;/div&gt;&lt;P&gt;To make this comparison tangible, I built a simple yet realistic Python experiment to predict vehicle selling prices using a public dataset containing car attributes like make, model, year, transmission, and mileage.&lt;/P&gt;&lt;P&gt;Why vehicle pricing? Because it’s an intuitive example where both traditional machine learning and pretrained AI models can be applie, and it helps visualize how prediction quality evolves as the sample size grows.&lt;/P&gt;&lt;P&gt;This entire analysis runs on a local Python environment&amp;nbsp;with the following stack:&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import os
import gc
import warnings
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LinearRegression
from sap_rpt_oss import SAP_RPT_OSS_Regressor
import lightgbm as lgb&lt;/code&gt;&lt;/pre&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;pandas&lt;/STRONG&gt; and &lt;STRONG&gt;numpy&lt;/STRONG&gt; for data manipulation&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;scikit-learn&lt;/STRONG&gt; for classical ML regressors (R&lt;STRONG&gt;andom Forest, Linear Regression&lt;/STRONG&gt;)&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;LightGBM&lt;/STRONG&gt; for gradient &lt;STRONG&gt;boosting&lt;/STRONG&gt; comparison&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;sap_rpt_oss&lt;/STRONG&gt; — the open-source Python version of &lt;STRONG&gt;SAP’s RPT-1 model&lt;/STRONG&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;matplotlib&lt;/STRONG&gt; for all &lt;STRONG&gt;visualizations&lt;/STRONG&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;BLOCKQUOTE&gt;&lt;P&gt;&lt;STRONG&gt;SAP RPT-1 OSS &lt;/STRONG&gt;can be downloaded installed following official Hugging Face:&amp;nbsp;&lt;A title="https://huggingface.co/SAP/sap-rpt-1-oss?library=sap-rpt-1-oss" href="https://huggingface.co/SAP/sap-rpt-1-oss?library=sap-rpt-1-oss" target="_blank" rel="noopener nofollow noreferrer"&gt;https://huggingface.co/SAP/sap-rpt-1-oss?library=sap-rpt-1-oss&lt;/A&gt;&amp;nbsp;. Python can be installed with executable download on Windows, or via &lt;STRONG&gt;Home Brew&lt;/STRONG&gt; for Mac and &lt;STRONG&gt;apt&lt;/STRONG&gt; commands for Linux. Libraries dependencies can be downloaded with &lt;STRONG&gt;pip&lt;/STRONG&gt; commands. Googling it may not be a road blocker.&lt;/P&gt;&lt;/BLOCKQUOTE&gt;&lt;P&gt;We use a sample&amp;nbsp;vehicle sales dataset. The complete file is about to 88Mb but for such experiment a restricted sample of 20k as it's more than enough to prove our the concept, still it's faster and consuming less computing resources.&lt;/P&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;TABLE border="1" width="498px"&gt;&lt;TBODY&gt;&lt;TR&gt;&lt;TD&gt;&lt;STRONG&gt;Feature&lt;/STRONG&gt;&lt;/TD&gt;&lt;TD&gt;&lt;STRONG&gt;Description&lt;/STRONG&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;year&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Vehicle model year&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;make&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Brand (e.g., Toyota, Ford, BMW)&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;model&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Specific model name&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;body&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Type (SUV, Sedan, etc.)&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;transmission&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Gear type&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;odometer&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Vehicle mileage&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;color&lt;/CODE&gt;, &lt;CODE&gt;interior&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Visual attributes&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;sellingprice&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;The target variable to predict&lt;/TD&gt;&lt;/TR&gt;&lt;/TBODY&gt;&lt;/TABLE&gt;&lt;P&gt;&lt;STRONG&gt;&lt;span class="lia-unicode-emoji" title=":bar_chart:"&gt;📊&lt;/span&gt;&amp;nbsp;Dataset Download:&lt;/STRONG&gt;&amp;nbsp;&lt;A title="https://www.kaggle.com/datasets/syedanwarafridi/vehicle-sales-data?resource=download" href="https://www.kaggle.com/datasets/syedanwarafridi/vehicle-sales-data?resource=download" target="_blank" rel="noopener nofollow noreferrer"&gt;https://www.kaggle.com/datasets/syedanwarafridi/vehicle-sales-data?resource=download&lt;/A&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;The dataset is loaded and preprocessed in a few simple steps:&lt;/P&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;df = pd.read_csv("car_prices.csv").sample(n=20000, random_state=42)

# Fill missing values for categorical columns
fill_defaults = {
    'make': 'Other', 'model': 'Other', 'color': 'Other',
    'interior': 'Unknown', 'body': 'Unknown', 'transmission': 'Unknown'
}
for col, val in fill_defaults.items():
    df[col] = df[col].fillna(val)

X = df[["year", "make", "model", "body", "transmission", "odometer", "color", "interior"]]
y = df["sellingprice"]&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;At this point, the stage is set:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;The data is clean.&lt;/LI&gt;&lt;LI&gt;The environment is ready.&lt;/LI&gt;&lt;LI&gt;All models, traditionals and SAP RPT-1, are ready to be tested under identical conditions.&lt;/LI&gt;&lt;/UL&gt;&lt;HR /&gt;&lt;H2 id="toc-hId-1175228200"&gt;&lt;span class="lia-unicode-emoji" title=":robot_face:"&gt;🤖&lt;/span&gt;&amp;nbsp;Training the Models - Three different ones&lt;/H2&gt;&lt;P&gt;With the dataset ready, the &lt;STRONG&gt;next step&lt;/STRONG&gt; is to run each model under the same conditions: &lt;STRONG&gt;same features, same target, same train/test split and same random seed&lt;/STRONG&gt;. This ensures the comparison is fair and repeatable.&lt;/P&gt;&lt;P&gt;We evaluate prediction performance using &lt;STRONG&gt;R² (coefficient of determination)&lt;/STRONG&gt;, which indicates how much of the price variation the model can explain (1.0 = perfect prediction).&lt;/P&gt;&lt;HR /&gt;&lt;H3 id="toc-hId-1107797414"&gt;Training Model #1 - Random Forest&lt;/H3&gt;&lt;P&gt;Random Forest is often the first model used in tabular ML. It works by creating &lt;STRONG&gt;many decision trees&lt;/STRONG&gt; and averaging their predictions. Before training, categorical variables need to be &lt;STRONG&gt;label-encoded&lt;/STRONG&gt; into numbers, a common requirement for classical ML models:&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;def train_random_forest(X, y):
    X = X.copy()
    cat_cols = ["make", "model", "body", "transmission", "color", "interior"]
    le = LabelEncoder()

    for col in cat_cols:
        X[col] = le.fit_transform(X[col].astype(str).fillna("Unknown"))

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=default_test_size, random_state=42
    )

    model = RandomForestRegressor(
        n_estimators=150, max_depth=20, random_state=42, n_jobs=-1
    )

    try:
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        r2 = r2_score(y_test, preds)
    except Exception as e:
        preds, r2 = np.zeros_like(y_test), 0

    return [preds, r2, y_test]&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-911283909"&gt;Up to 50 rows:&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_3-1763206176248.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341502i82216AA724092E03/image-size/large?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_3-1763206176248.png" alt="nicolasestevan_3-1763206176248.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId-714770404"&gt;Up to 7067 rows:&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_8-1763206511155.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341538iF2A25E0C0EBE0612/image-size/large?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_8-1763206511155.png" alt="nicolasestevan_8-1763206511155.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId-518256899"&gt;Live view&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="RandomForest_20251115_092355.gif" style="width: 960px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341551i3A2C874AFAF47388/image-size/large?v=v2&amp;amp;px=999" role="button" title="RandomForest_20251115_092355.gif" alt="RandomForest_20251115_092355.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;HR /&gt;&lt;H3 id="toc-hId-321743394"&gt;Training Model #2 - LightGBM&lt;/H3&gt;&lt;P&gt;LightGBM is one of the most powerful models for tabular data. Unlike Random Forest (many independent trees), LightGBM builds trees &lt;STRONG&gt;sequentially&lt;/STRONG&gt;, each correcting the errors of the previous one. It supports categorical features natively, which simplifies preprocessing.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;def train_lightgbm(X, y):
    X = X.copy()
    cat_cols = ["make", "model", "body", "transmission", "color", "interior"]
    for col in cat_cols:
        X[col] = X[col].astype(str).fillna("Unknown").astype("category")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=default_test_size, random_state=42
    )

    model = lgb.LGBMRegressor(
        n_estimators=500, learning_rate=0.05, num_leaves=31,
        subsample=0.8, colsample_bytree=0.8, random_state=42
    )

    try:
        model.fit(X_train, y_train, categorical_feature=cat_cols)
        preds = model.predict(X_test)
        r2 = r2_score(y_test, preds)
    except Exception:
        preds, r2 = np.zeros_like(y_test), 0

    return [preds, r2, y_test]&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-125229889"&gt;Up to 50 rows:&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_2-1763205951324.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341474i1AAB214E2D01C2B2/image-size/large?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_2-1763205951324.png" alt="nicolasestevan_2-1763205951324.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId--146514985"&gt;Up to 7067 rows:&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_7-1763206474860.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341537i0ACD453B96C87ADF/image-size/large?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_7-1763206474860.png" alt="nicolasestevan_7-1763206474860.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId--343028490"&gt;Live view&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="LightGBM_20251115_092355.gif" style="width: 960px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341552i30BC4DE94C4988F6/image-size/large?v=v2&amp;amp;px=999" role="button" title="LightGBM_20251115_092355.gif" alt="LightGBM_20251115_092355.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;HR /&gt;&lt;H3 id="toc-hId--539541995"&gt;Training Model #3 - Linear Regression&lt;/H3&gt;&lt;P&gt;Not fancy and even not complex, Linear Regression provides a baseline that shows:&amp;nbsp;&lt;SPAN&gt;“If the relationship between attributes and price is roughly linear, how well can a simple model perform?”&lt;/SPAN&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;def train_linear_model(X, y):
    X = X.copy()
    cat_cols = ["make", "model", "body", "transmission", "color", "interior"]
    for col in cat_cols:
        X[col] = LabelEncoder().fit_transform(X[col].astype(str).fillna("Unknown"))

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=default_test_size, random_state=42
    )

    model = LinearRegression()
    X_train = X_train.fillna(X_train.mean(numeric_only=True))
    X_test = X_test.fillna(X_test.mean(numeric_only=True))

    try:
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        r2 = r2_score(y_test, preds)
    except Exception:
        preds, r2 = np.zeros_like(y_test), 0

    return [preds, r2, y_test]&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId--736055500"&gt;&lt;STRONG&gt;Up to 50 rows:&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_1-1763205857765.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341472i81AFB2D0BE770F90/image-size/large?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_1-1763205857765.png" alt="nicolasestevan_1-1763205857765.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId--932569005"&gt;&lt;STRONG&gt;Up to 7067 rows:&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_6-1763206428099.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341536iC708165AEAE11D46/image-size/large?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_6-1763206428099.png" alt="nicolasestevan_6-1763206428099.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId--1129082510"&gt;Live view&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="LinearModel_20251115_092355.gif" style="width: 960px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341553i0849B4C842A417EE/image-size/large?v=v2&amp;amp;px=999" role="button" title="LinearModel_20251115_092355.gif" alt="LinearModel_20251115_092355.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H2 id="toc-hId--1032193008"&gt;&lt;span class="lia-unicode-emoji" title=":chequered_flag:"&gt;🏁&lt;/span&gt;&amp;nbsp;&lt;SPAN&gt;SAP RPT-1 OSS: Context Model&lt;/SPAN&gt;&lt;/H2&gt;&lt;P&gt;This is where things get interesting. SAP RPT-1 does &lt;STRONG&gt;not&lt;/STRONG&gt; rely on learning patterns from the dataset. Instead, it uses a pretrained transformer architecture to infer relationships directly through &lt;STRONG&gt;context embeddings&lt;/STRONG&gt;. Lean and simple, "for non-Data Science PhD":&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;def train_sap_rpt1(X, y):
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=default_test_size, random_state=42
    )

    model = SAP_RPT_OSS_Regressor(max_context_size=8192, bagging=8)
    model.fit(X_train, y_train)

    preds = model.predict(X_test)
    r2 = r2_score(y_test, preds)

    return [preds, r2, y_test]&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId--1522109520"&gt;&lt;STRONG&gt;Up to 50 rows:&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_0-1763205729558.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341471i4AC7007DCA5A0F76/image-size/large?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_0-1763205729558.png" alt="nicolasestevan_0-1763205729558.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId--1718623025"&gt;&lt;STRONG&gt;Up to 2055 rows:&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_4-1763206228416.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341505i9ADE9D2D2B38C363/image-size/large?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_4-1763206228416.png" alt="nicolasestevan_4-1763206228416.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId--1915136530"&gt;Live view&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="SAP_RPT1_20251115_092355.gif" style="width: 960px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341566i0BE0E0D666836951/image-size/large?v=v2&amp;amp;px=999" role="button" title="SAP_RPT1_20251115_092355.gif" alt="SAP_RPT1_20251115_092355.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;HR /&gt;&lt;H2 id="toc-hId--1650063337"&gt;&lt;STRONG&gt;&lt;span class="lia-unicode-emoji" title=":magnifying_glass_tilted_right:"&gt;🔎&lt;/span&gt;&amp;nbsp;Running Experiments at Multiple Sample Sizes&lt;/STRONG&gt;&lt;/H2&gt;&lt;P&gt;This section breaks down how the iterative experiment loop works, why the SAP RPT-1 OSS model has a max-context limit, and how performance changes as we scale up the dataset. By running the same models across several sample sizes, we can see where traditional ML shines, where RPT-1 stays competitive, and how both behave as the data grows.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;sample_sizes = np.linspace(50, len(X), 200, dtype=int)
results, max_r2_rpt1, max_sample_rpt1 = [], 0, 0

for n in sample_sizes:
    idx = np.random.choice(len(X), n, replace=False)
    X_sample, y_sample = X.iloc[idx], y.iloc[idx]


    # SAP RPT-1 OSS (limited sample size)
    if n &amp;lt;= rpt1_limit:
        rpt_res = train_sap_rpt1(X_sample, y_sample)
        fn = plot_predictions(rpt_res[2], rpt_res[0], rpt_res[1], "SAP_RPT1", n)
        video_frames["SAP_RPT1"].append(fn)
        r2_rpt1 = rpt_res[1]
        max_r2_rpt1 = max(max_r2_rpt1, r2_rpt1)
    else:
        r2_rpt1 = max_r2_rpt1
        if max_sample_rpt1 == 0:
            max_sample_rpt1 = n

    # Train and plot models
    rf_res = train_random_forest(X_sample, y_sample)
    fn = plot_predictions(rf_res[2], rf_res[0], rf_res[1], "RandomForest", n)
    video_frames["RandomForest"].append(fn)

    lgb_res = train_lightgbm(X_sample, y_sample)
    fn = plot_predictions(lgb_res[2], lgb_res[0], lgb_res[1], "LightGBM", n)
    video_frames["LightGBM"].append(fn)

    lin_res = train_linear_model(X_sample, y_sample)
    fn = plot_predictions(lin_res[2], lin_res[0], lin_res[1], "LinearModel", n)
    video_frames["LinearModel"].append(fn)

    results.append((n, rf_res[1], r2_rpt1, lgb_res[1], lin_res[1]))

    # Early stop if traditional model reaches SAP RPT-1
    if rf_res[1] &amp;gt;= max_r2_rpt1 or lgb_res[1] &amp;gt;= max_r2_rpt1 or lin_res[1] &amp;gt;= max_r2_rpt1:
        break
    gc.collect()&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;This loop compares SAP RPT-1 OSS with traditional ML models as sample sizes increase. Each iteration randomly selects a subset of the data and trains all models on the same slice for a fair comparison. SAP RPT-1 can only run up to its max-context limit, so once the sample size exceeds that threshold, it stops retraining and simply carries forward its best R². The traditional models continue training at every step. The loop ends early when any traditional model matches or surpasses RPT-1’s best score, making the experiment efficient while showing how performance evolves as data grows.&lt;/P&gt;&lt;HR /&gt;&lt;H2 id="toc-hId--1846576842"&gt;&lt;STRONG&gt;&lt;span class="lia-unicode-emoji" title=":end_arrow:"&gt;🔚&lt;/span&gt;&amp;nbsp;Conclusion and Final Thoughts&lt;/STRONG&gt;&lt;/H2&gt;&lt;P&gt;&amp;nbsp;SAP RPT-1 OSS stands out because it performs well with small datasets, requires minimal code, and can generate useful predictions with just an API call and a bit of context. This makes it ideal for jump-starting predictive use cases early on, delivering fast business value without a full ML pipeline. Traditional models, however, still shine when projects mature, data grows, and fine-tuned control becomes important. It’s not about choosing one over the other, but understanding where each approach brings the most value.&lt;/P&gt;&lt;TABLE border="1" width="100%"&gt;&lt;TBODY&gt;&lt;TR&gt;&lt;TD&gt;&lt;STRONG&gt;&amp;nbsp;&lt;/STRONG&gt;&lt;STRONG&gt;Aspect&amp;nbsp;&lt;/STRONG&gt;&lt;/TD&gt;&lt;TD&gt;&lt;STRONG&gt;SAP RPT-1 OSS&amp;nbsp;&lt;/STRONG&gt;&lt;/TD&gt;&lt;TD&gt;&lt;STRONG&gt;Traditional ML (RF, LGBM, Linear)&lt;/STRONG&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Data Requirements&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;Low (performs well with small samples)&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Medium/High (performance scales with data&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Setup Effort&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;Minimal (API call + context)&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Higher (preprocessing, encoding, tuning)&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Training Process&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;None (pretrained context model)&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Full training pipeline required&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Speed to Insights&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;Very fast&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Moderate to slow&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Best Use Case&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;Early-stage predictive cases, quick baselines&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Mature pipelines, high control and customization&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Flexibility&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;Limited tuning / plug-and-play&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Highly customizable&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Business Value&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;Immediate, fast, accessible&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Strong when optimized and scaled&lt;/TD&gt;&lt;/TR&gt;&lt;/TBODY&gt;&lt;/TABLE&gt;&lt;P&gt;This experiment highlights a simple truth: &lt;STRONG&gt;SAP RPT-1 isn’t here to replace traditional ML, it jump-starts it.&amp;nbsp;&lt;/STRONG&gt;With a pretrained, context-driven approach, RPT-1 delivers fast, reliable insights with very little data and almost no setup. Traditional models still excel in mature, data-rich scenarios, but RPT-1 shines as a rapid accelerator and early-value generator inside SAP landscapes.&lt;/P&gt;&lt;HR /&gt;&lt;H3 id="toc-hId-1958473942"&gt;&lt;STRONG&gt;&lt;span class="lia-unicode-emoji" title=":speech_balloon:"&gt;💬&lt;/span&gt;Open for Exchange&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;If you're testing RPT-1, exploring predictive cases, or want the full code, feel free to reach out.&lt;BR /&gt;&lt;STRONG&gt;Happy to connect, compare experiences, and push this topic forward together.&lt;/STRONG&gt;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/sap-rpt-1-context-model-vs-training-classical-models-the-models-battle/ba-p/14268507"/>
    <published>2025-11-20T07:50:27.670000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/querying-rdf-graphs-with-sap-hana-cloud-knowledge-graph-engine/ba-p/14289126</id>
    <title>Querying RDF Graphs with SAP HANA Cloud Knowledge Graph Engine</title>
    <updated>2025-12-12T03:01:55.876000+01:00</updated>
    <author>
      <name>jing_wen</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1923466</uri>
    </author>
    <content>&lt;P&gt;SAP HANA Cloud’s multi-model platform brings together vector, graph, text, spatial, and relational data natively. It enables developers and data teams to build smarter, more context-aware AI solutions — directly on operational data.&lt;/P&gt;&lt;P&gt;SAP HANA Cloud uniquely supports:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Vector data&lt;/STRONG&gt;&amp;nbsp;for semantic and similarity search&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Graph data&lt;/STRONG&gt;&amp;nbsp;for explicit relationship modeling and knowledge graphs&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Text and spatial data&lt;/STRONG&gt;&amp;nbsp;for real-world context&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Relational data&lt;/STRONG&gt;&amp;nbsp;for structured operations and analytics&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;Rather than sending data across disparate services, you can store and process all of it in one place, accelerating time-to-value while reducing the risk of misalignment. This is multi-model done right, and it is the foundation for powerful AI workloads that scale.&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;This blog demonstrates how to &lt;STRONG&gt;query RDF knowledge graphs in SAP HANA Cloud&lt;/STRONG&gt;, both:&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Directly from the &lt;STRONG&gt;HANA Cloud Central SQL Console&lt;/STRONG&gt;, and&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Programmatically using &lt;STRONG&gt;Python&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;SPAN&gt;We will also show how RDF graphs can be combined seamlessly with &lt;STRONG&gt;vector similarity search, spatial filtering, and SQL analytics&lt;/STRONG&gt; to create intelligent, real‑world use cases. The blog expands on this &lt;A href="https://news.sap.com/2025/07/unifying-ai-workloads-sap-hana-cloud-one-database/" target="_self" rel="noopener noreferrer"&gt;post&lt;/A&gt;.&lt;/SPAN&gt;&lt;/P&gt;&lt;H2 id="toc-hId-1766641765"&gt;&lt;SPAN&gt;From supplier notes to AI‑ready supply‑chain intelligence&lt;/SPAN&gt;&lt;/H2&gt;&lt;P class=""&gt;&lt;SPAN&gt;To ground the concepts, we start with a simple but realistic scenario: turning unstructured supplier feedback into AI‑ready supply‑chain intelligence.&lt;/SPAN&gt;&lt;/P&gt;&lt;P class=""&gt;&lt;SPAN&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="1.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/351371i531B6FE2D035385F/image-size/large?v=v2&amp;amp;px=999" role="button" title="1.png" alt="1.png" /&gt;&lt;/span&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P class=""&gt;&lt;SPAN&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="1.1.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/351376i034D72271FF3A2DD/image-size/large?v=v2&amp;amp;px=999" role="button" title="1.1.png" alt="1.1.png" /&gt;&lt;/span&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P class=""&gt;&lt;SPAN&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="1.2.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/351377i7E5A3692E6F81D96/image-size/large?v=v2&amp;amp;px=999" role="button" title="1.2.png" alt="1.2.png" /&gt;&lt;/span&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;H3 id="toc-hId-1699210979"&gt;&lt;SPAN&gt;Step 1: Create a supply‑chain schema and table&lt;/SPAN&gt;&lt;/H3&gt;&lt;P class=""&gt;&lt;SPAN&gt;We first create a dedicated schema and table to store supplier identifiers, short operational reports, vector embeddings, and geographic locations.&lt;/SPAN&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-sql"&gt;&lt;code&gt;CREATE SCHEMA KG_SUPPLYCHAIN;

CREATE TABLE KG_SUPPLYCHAIN.SUPPLIER_REPORTS_LOOKUP (
  supplier_uri       NVARCHAR(200),
  report_text        NVARCHAR(1000),
  report_embedding   REAL_VECTOR(768),
  geo_location       ST_POINT(4326)
);&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-1502697474"&gt;&lt;SPAN&gt;Step 2: Insert natural‑language supplier reports&lt;/SPAN&gt;&lt;/H3&gt;&lt;P class=""&gt;&lt;SPAN&gt;The reports represent real‑world observations such as customs delays or smooth clearance. At this stage, AI embeddings and spatial attributes are not yet populated.&lt;/SPAN&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-sql"&gt;&lt;code&gt;INSERT INTO KG_SUPPLYCHAIN.SUPPLIER_REPORTS_LOOKUP VALUES
('http://example.org/supplier/AlphaGmbH', 'Consistently on time. No customs issue.', NULL, NULL),
('http://example.org/supplier/BetaGmbH',  'Severe customs delays reported last month.', NULL, NULL),
('http://example.org/supplier/GammaLogistics', 'Smooth operations, cleared customs quickly.', NULL, NULL);&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-1306183969"&gt;&lt;SPAN&gt;Step 3: Add geographic context&lt;/SPAN&gt;&lt;/H3&gt;&lt;P class=""&gt;&lt;SPAN&gt;Each supplier is enriched with a geographic location using WGS84 coordinates. This enables proximity analysis and regional risk assessment.&lt;/SPAN&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-sql"&gt;&lt;code&gt;UPDATE KG_SUPPLYCHAIN.SUPPLIER_REPORTS_LOOKUP
SET geo_location = ST_GeomFromText('POINT(8.6821 50.1109)', 4326)
WHERE supplier_uri = 'http://example.org/supplier/AlphaGmbH';&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-1109670464"&gt;&lt;SPAN&gt;Step 4: Generate vector embeddings inside the database&lt;/SPAN&gt;&lt;/H3&gt;&lt;P class=""&gt;&lt;SPAN&gt;SAP HANA Cloud generates vector embeddings directly in‑database using SAP’s native embedding model. This enables &lt;STRONG&gt;semantic understanding&lt;/STRONG&gt; of supplier reports.&lt;/SPAN&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-sql"&gt;&lt;code&gt;UPDATE KG_SUPPLYCHAIN.SUPPLIER_REPORTS_LOOKUP
SET report_embedding = VECTOR_EMBEDDING(
  report_text,
  'QUERY',
  'SAP_NEB.20240715'
);&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;SPAN&gt;At this point, we have a single &lt;STRONG&gt;AI‑ready table&lt;/STRONG&gt; that supports:&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Semantic similarity search&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Spatial analysis&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Generative AI grounding&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;SPAN&gt;—all without exporting data outside SAP HANA Cloud.&lt;/SPAN&gt;&lt;/P&gt;&lt;H2 id="toc-hId-784074240"&gt;&lt;SPAN&gt;Adding semantic supplier knowledge with SAP HANA Knowledge Graph&lt;/SPAN&gt;&lt;/H2&gt;&lt;P class=""&gt;&lt;SPAN&gt;Relational tables capture operational facts well, but &lt;STRONG&gt;business meaning and rules&lt;/STRONG&gt; are better expressed semantically. This is where the SAP HANA Cloud Knowledge Graph engine comes in.&lt;/SPAN&gt;&lt;/P&gt;&lt;P class=""&gt;&lt;SPAN&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="2.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/351372i3324C3D707173F47/image-size/large?v=v2&amp;amp;px=999" role="button" title="2.png" alt="2.png" /&gt;&lt;/span&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;H3 id="toc-hId-716643454"&gt;&lt;SPAN&gt;Step 5: Initialize the RDF graph&lt;/SPAN&gt;&lt;/H3&gt;&lt;P class=""&gt;&lt;SPAN&gt;We start by removing any existing graph to ensure a clean load.&lt;/SPAN&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-sql"&gt;&lt;code&gt;CALL SPARQL_EXECUTE('DROP GRAPH &amp;lt;kg_supplychain&amp;gt;', '', ?, ?);&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-520129949"&gt;&lt;SPAN&gt;Step 6: Insert semantic supplier knowledge&lt;/SPAN&gt;&lt;/H3&gt;&lt;P class=""&gt;&lt;SPAN&gt;We then insert RDF triples describing suppliers and their business attributes. Each supplier uses the &lt;STRONG&gt;same URI&lt;/STRONG&gt; as the relational table, creating a natural bridge between SQL and RDF.&lt;/SPAN&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-sql"&gt;&lt;code&gt;CALL SPARQL_EXECUTE(
'
INSERT DATA {
  GRAPH &amp;lt;kg_supplychain&amp;gt; {
    &amp;lt;http://example.org/supplier/AlphaGmbH&amp;gt; a &amp;lt;Supplier&amp;gt; ;
      &amp;lt;hasCertification&amp;gt; "ISO 9001" ;
      &amp;lt;hasCarbonTaxRate&amp;gt; "low" ;
      &amp;lt;isFlaggedForDelays&amp;gt; false .
  }
}',
'', ?, ?);&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;SPAN&gt;Within the Knowledge Graph, suppliers are enriched with structured, machine‑readable facts such as certifications, carbon‑tax exposure, and compliance flags.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;The result is a &lt;STRONG&gt;dual‑layer architecture&lt;/STRONG&gt;:&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;SPAN&gt;Relational layer&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;: text, embeddings, spatial data&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;SPAN&gt;Semantic layer&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;: business meaning, rules, and relationships&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;SPAN&gt;Both are connected by shared URIs and executed in the same database.&lt;/SPAN&gt;&lt;/P&gt;&lt;H2 id="toc-hId-194533725"&gt;&lt;SPAN&gt;Querying across graph, vector, spatial, and SQL models&lt;/SPAN&gt;&lt;/H2&gt;&lt;H3 id="toc-hId-127102939"&gt;&lt;SPAN&gt;Query 1: Filter Knowledge Graph data directly in SQL&lt;/SPAN&gt;&lt;/H3&gt;&lt;P&gt;&lt;SPAN&gt;SAP HANA Cloud allows SPARQL queries to be consumed as relational tables using &lt;/SPAN&gt;&lt;SPAN&gt;SPARQL_TABLE&lt;/SPAN&gt;&lt;SPAN&gt;, enabling seamless integration with SQL analytics.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="4.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/351375i0A0BC8C8223AF7BB/image-size/large?v=v2&amp;amp;px=999" role="button" title="4.png" alt="4.png" /&gt;&lt;/span&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-sql"&gt;&lt;code&gt;SELECT * FROM SPARQL_TABLE('
  SELECT ?supplier ?certification ?carbontax ?flag
  FROM &amp;lt;kg_supplychain&amp;gt;
  WHERE {
    ?supplier a &amp;lt;Supplier&amp;gt; .
    ?supplier &amp;lt;hasCertification&amp;gt; ?certification .
    ?supplier &amp;lt;hasCarbonTaxRate&amp;gt; ?carbontax .
    ?supplier &amp;lt;isFlaggedForDelays&amp;gt; ?flag .
    FILTER(
      ?certification = "ISO 9001" &amp;amp;&amp;amp;
      ?carbontax = "low" &amp;amp;&amp;amp;
      STR(?flag) = "false"
    )
  }
');&lt;/code&gt;&lt;/pre&gt;&lt;P class=""&gt;&lt;SPAN&gt;This bridges &lt;STRONG&gt;semantic reasoning&lt;/STRONG&gt; and &lt;STRONG&gt;relational analytics&lt;/STRONG&gt; in a single query flow.&lt;/SPAN&gt;&lt;/P&gt;&lt;H3 id="toc-hId--144641935"&gt;&lt;SPAN&gt;Query 2: Hybrid SPARQL + SQL with vector and spatial filtering&lt;/SPAN&gt;&lt;/H3&gt;&lt;P&gt;&lt;SPAN&gt;This advanced query combines:&lt;/SPAN&gt;&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;SPAN&gt;Geospatial filtering&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; (nearby suppliers)&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;SPAN&gt;Vector similarity search&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; on supplier reports&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;SPAN&gt;Knowledge Graph constraints&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="3.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/351374iB2F8DCBEBBB8F360/image-size/large?v=v2&amp;amp;px=999" role="button" title="3.png" alt="3.png" /&gt;&lt;/span&gt;&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-sql"&gt;&lt;code&gt;CALL SPARQL_EXECUTE(
'
SELECT *
FROM &amp;lt;kg_supplychain&amp;gt;
WHERE {
  SQL_TABLE("SELECT \"uri_str\", \"REPORT_TEXT\", \"SCO\" FROM (
    SELECT *, sco - FIRST_VALUE(sco) OVER(ORDER BY sco DESC) AS diff
    FROM (
      SELECT \"SUPPLIER_URI\" AS \"uri_str\", \"REPORT_TEXT\",
      COSINE_SIMILARITY(
        \"REPORT_EMBEDDING\",
        VECTOR_EMBEDDING(''no customs delay'', ''QUERY'', ''SAP_NEB.20240715'')
      ) AS sco
      FROM KG_SUPPLYCHAIN.SUPPLIER_REPORTS_LOOKUP
      WHERE \"GEO_LOCATION\".ST_Distance(
        ST_GeomFromText(''POINT(8.6821 50.1109)'', 4326)
      ) &amp;lt; 50000
      ORDER BY sco DESC
    )
  ) WHERE diff &amp;gt; -0.2") .
}
ORDER BY DESC(?SCO)
LIMIT 10
',
'Accept: application/sparql-results+csv Content-Type: application/sparql-query',
?, ?);&lt;/code&gt;&lt;/pre&gt;&lt;P class=""&gt;&lt;SPAN&gt;This single query performs &lt;STRONG&gt;AI‑driven supplier discovery&lt;/STRONG&gt;, ranking results by semantic meaning while respecting spatial and business rules.&lt;/SPAN&gt;&lt;/P&gt;&lt;H2 id="toc-hId--47752433"&gt;&lt;SPAN&gt;Querying and managing Knowledge Graphs with Python&lt;/SPAN&gt;&lt;/H2&gt;&lt;P&gt;&lt;SPAN&gt;Beyond the SQL console, Knowledge Graphs can be managed programmatically using Python and the HANA DBAPI.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;The following example shows how to:&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Upload an RDF ontology (Turtle .ttl format)&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Verify graph load success&lt;BR /&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Query RDF data programmatically&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from hdbcli import dbapi

conn = dbapi.connect(
    address='XXX.hana.prod-ap10.hanacloud.ondemand.com',
    port=443,
    user='XXX',
    password='XXX'
)

cursor = conn.cursor()&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId--537668945"&gt;&lt;SPAN&gt;Load an ontology into SAP HANA Cloud Knowledge Graph&lt;/SPAN&gt;&lt;/H3&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# Populate a new RDF in HANA Cloud with the turtle file content. We put the ontology into a specific graph.
ttl_filename = "/Users/materials_ontology.ttl"
graph_name = "materials_ontology"

# Load the ontology into HANA Cloud KG
print("Loading ontology...")

try:
    with open(ttl_filename, 'r') as ttlfp:
        request_hdrs = ''
        request_hdrs += 'rqx-load-protocol: true' + '\r\n'            # required header for upload protocol
        request_hdrs += 'rqx-load-filename: ' + ttl_filename + '\r\n' # optional header
        request_hdrs += 'rqx-load-graphname: ' + graph_name + '\r\n'   # optional header to specify name of the graph
        
        print(f"Loading file: {ttl_filename}")
        print(f"Graph name: {graph_name}")
        
        # Execute the load
        result = conn.cursor().callproc('SPARQL_EXECUTE', (ttlfp.read(), request_hdrs, '?', None))
        print("Materials ontology loaded successfully!")
        print(f"Result: {result}")
        
except Exception as e:
    print(f"Error loading materials ontology: {e}")
    print(f"Error type: {type(e)}")&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId--734182450"&gt;&lt;SPAN&gt;Query the ontology&amp;nbsp;&lt;/SPAN&gt;&lt;/H3&gt;&lt;P&gt;&lt;SPAN&gt;Do note that you will have to adjust the query based on your ontology structure.&lt;/SPAN&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# Verify the materials ontology was loaded
print("Verifying materials ontology load...")

try:
    # Count triples in the materials graph
    query = f"""
    SELECT (COUNT(*) as ?Triples) 
    WHERE 
      {{ GRAPH &amp;lt;{graph_name}&amp;gt; 
          {{ ?s ?p ?o }} 
      }}
    """
    
    resp = conn.cursor().callproc('SPARQL_EXECUTE', (query, 'Accept: application/sparql-results+csv', '?', None))
    print("Materials ontology statistics:")
    print(resp[2])
    
    # Query for some material instances
    query2 = f"""
    PREFIX mat: &amp;lt;http://example.com/materials/&amp;gt;
    PREFIX matprop: &amp;lt;http://example.com/materials/property/&amp;gt;
    
    SELECT ?material ?materialId ?level2 ?level3
    FROM &amp;lt;{graph_name}&amp;gt;
    WHERE {{
        ?material a mat:Material .
        ?material matprop:materialId ?materialId .
        OPTIONAL {{ ?material matprop:level2 ?level2 . }}
        OPTIONAL {{ ?material matprop:level3 ?level3 . }}
    }}
    LIMIT 5
    """
    
    resp2 = conn.cursor().callproc('SPARQL_EXECUTE', (query2, 'Accept: application/sparql-results+csv', '?', None))
    print("\n Sample material instances:")
    print(resp2[2])
    
except Exception as e:
    print(f"Error verifying materials ontology: {e}")&lt;/code&gt;&lt;/pre&gt;&lt;H2 id="toc-hId--637292948"&gt;&lt;SPAN&gt;Summary&lt;/SPAN&gt;&lt;/H2&gt;&lt;P&gt;&lt;SPAN&gt;SAP HANA Cloud enables &lt;STRONG&gt;true multi‑model intelligence&lt;/STRONG&gt; by allowing relational data, vector embeddings, spatial context, and RDF knowledge graphs to coexist and execute together.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;By combining i&lt;/SPAN&gt;&lt;SPAN&gt;n‑database vector embeddings, s&lt;/SPAN&gt;&lt;SPAN&gt;emantic Knowledge Graphs, and&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;SQL, SPARQL, Python access, o&lt;/SPAN&gt;&lt;SPAN&gt;rganizations can build powerful AI applications that are &lt;STRONG&gt;context‑aware, explainable, and operationally grounded&lt;/STRONG&gt;—all from a single platform.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;SAP HANA Cloud is not just a database for AI. It is where &lt;STRONG&gt;data, meaning, and intelligence converge&lt;/STRONG&gt;.&lt;/SPAN&gt;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/querying-rdf-graphs-with-sap-hana-cloud-knowledge-graph-engine/ba-p/14289126"/>
    <published>2025-12-12T03:01:55.876000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/artificial-intelligence-blogs-posts/implementing-thread-safe-structured-logging-for-python-fastapi/ba-p/14292907</id>
    <title>Implementing  Thread-Safe Structured Logging for Python FastAPI</title>
    <updated>2025-12-18T05:41:59.266000+01:00</updated>
    <author>
      <name>Kunal__Kumar</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1692145</uri>
    </author>
    <content>&lt;P&gt;With the focous on bussiness AI scenarios and high &lt;STRONG&gt;throughput&lt;/STRONG&gt; applications, fastapi has come to be the de facto for standard python developement due to it's native support for &lt;STRONG&gt;async I/0&lt;/STRONG&gt; and pydantic based validations ensuring type safety. However, deploying fastapi applications to SAP BTP CF enviroment gives us issues with observalibity.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;the standard library provided by SAP, &lt;STRONG&gt;sap-cf-logging&lt;/STRONG&gt; relies heavily on &lt;STRONG&gt;WSGI middleware&lt;/STRONG&gt; and &lt;STRONG&gt;thread-local storage&lt;/STRONG&gt; to manage requests contexts. in &lt;STRONG&gt;ASGI&lt;/STRONG&gt; enviroment, a single thread can manage multiple concurrent requests via an event loop.&amp;nbsp;&lt;/P&gt;&lt;P&gt;relying on thread-local storage in this enviroment can lead to &lt;STRONG&gt;Context Leakage&lt;/STRONG&gt; where logs from one Request A appear with Coorelation id of Request B, or worse context is lost entirely due to await calls.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;to ensure this doesn't occur in our application and observalibity is not effected in Kibana without compromising on non-blocking operations, there needed to be a custom solution using python &lt;STRONG&gt;contextvars&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Architecture: Context-Local vs Thread-Local&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;to maintain the integrity of correlation id across async boundaries, there was a need for moving away from global state and implement context aware logger. this has mainly three components:-&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;&lt;STRONG&gt;context variables&lt;/STRONG&gt;: to hold the state safley across asyncio event loop&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;custom json formatter&lt;/STRONG&gt;: to serialize logs into strict JSON format required by SAP cloud logging&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;ASGI Middleware&lt;/STRONG&gt;: to handle lifecycle management of the correlation id&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;1. Async-safe Context&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;first we need to define a &lt;STRONG&gt;ContextVar&lt;/STRONG&gt;, unlike &lt;STRONG&gt;threading.local&lt;/STRONG&gt; , contextvars are natively supported by &lt;STRONG&gt;asyncio&lt;/STRONG&gt;. when a task awaits a coroutine then context is preserved for that specific chain of execution, ensuring concurrent reqeusts never corrupt each other's state.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;loggger.py&lt;/STRONG&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import logging
import sys
import json
import uuid
from contextvars import ContextVar
from typing import Optional

# ContextVar to store Correlation ID safely (Async safe!)
# This works per-task, ensuring concurrent requests preserve their unique ID.
correlation_id_ctx: ContextVar[Optional[str]] = ContextVar("correlation_id", default=None)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;STRONG&gt;&amp;nbsp;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;2. JSON Formatter&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;SAP BTP's log ingestion pipeline expects specifc fields (mgs, level, correlation_id, written_at) to index logs correctly in Kibana. we need to extend the standard python &lt;STRONG&gt;logging.Formatter&lt;/STRONG&gt;&amp;nbsp;to intercept every log record adn inject the current context dynamically.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;logger.py&lt;/STRONG&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;class JSONFormatter(logging.Formatter):
    """
    Custom formatter that outputs logs as a JSON string.
    Automatically injects the Correlation ID from the async context.
    """
    def format(self, record):
        # Retrieve the current correlation ID (or None if outside a request)
        cid = correlation_id_ctx.get()

        # Build the structured log dictionary required by SAP Cloud Logging
        log_record = {
            "level": record.levelname,
            "msg": record.getMessage(),
            "logger": record.name,
            "correlation_id": cid,
            "written_at": self.formatTime(record, self.datefmt),
        }

        # Include exception info if present (essential for stack traces in Kibana)
        if record.exc_info:
            log_record["exc_info"] = self.formatException(record.exc_info)

        return json.dumps(log_record)

def setup_logging():
    logger = logging.getLogger()
    if logger.handlers:
        logger.handlers = []
    
    # Stream to stdout is crucial for Cloud Foundry loggregator
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(JSONFormatter())
    
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;STRONG&gt;&amp;nbsp;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;3. Middleware injection&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;we need to implement a &lt;STRONG&gt;BaseHTTPMiddleware&lt;/STRONG&gt; to intercept every incoming request. this middleware handles the extraction of the &lt;STRONG&gt;X-CorrelationID&lt;/STRONG&gt; header or generates a new &lt;STRONG&gt;UUID&lt;/STRONG&gt; V4 if one is missing.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;logger.py&lt;/STRONG&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from fastapi import Request
from starlette.middleware.base import BaseHTTPMiddleware

class CorrelationMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        # 1. Extract ID from header OR generate a new one
        correlation_id = request.headers.get("X-CorrelationID") or str(uuid.uuid4())
        
        # 2. Set the ID in the ContextVar and capture the Token
        token = correlation_id_ctx.set(correlation_id)
        
        try:
            # 3. Process the request
            response = await call_next(request)
            
            # 4. Propagate the ID back to the client (UI) via headers
            response.headers["X-CorrelationID"] = correlation_id
            return response
            
        finally:
            # 5. Clean up: Reset the context to prevent leakage in thread pooling
            correlation_id_ctx.reset(token)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;4. "Background-Task" Conundrum&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;a specfic challenge encountered was handlign FASTAPI BackgroundTasks. since background tasks run after the HTTP reponse is returned, the middleware describes above has already finished execution and called &lt;STRONG&gt;reset(token).&amp;nbsp;&lt;/STRONG&gt;consequence being that background tasks start wtih an empty context, causing logs to show &lt;STRONG&gt;correlation_id: null&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;to solve this, we need to implementa &lt;STRONG&gt;Context Injection Pattern.&amp;nbsp;&lt;/STRONG&gt;we need to explicitily capture the ID while the requrst is still active and pass it to a wrapper function that &lt;STRONG&gt;re-hydrates&lt;/STRONG&gt; the context inside the background thread.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;api.py&lt;/STRONG&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;&lt;a href="https://community.sap.com/t5/user/viewprofilepage/user-id/1523598"&gt;@Api&lt;/a&gt;_router.post("/extract")
def extract(data: ExtractRequest, background_tasks: BackgroundTasks):
    # Capture the ID from the current context before the response is sent
    current_cid = correlation_id_ctx.get()
    
    # Pass it explicitly to the background task wrapper
    background_tasks.add_task(process_contract_sync, data.contract_id, current_cid)
    
    return {"message": f"Extracting data for contract ID: {data.contract_id}"}

def process_contract_sync(contract_id: str, correlation_id: str):
    # RE-INJECT the ID into the context for this isolated thread
    token = correlation_id_ctx.set(correlation_id)
    
    try:
        # Run the business logic with full logging context
        asyncio.run(process_contract(contract_id))
    except Exception as e:
        logging.exception(f"Critical failure in background task for {contract_id}")
    finally:
        # Strict cleanup using the token
        correlation_id_ctx.reset(token)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Conclusion&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;by leaving syncchronous &lt;STRONG&gt;sap-cf-logging&lt;/STRONG&gt; library in favour of a native &lt;STRONG&gt;contextvars&lt;/STRONG&gt; approach, we acheived:&lt;/P&gt;&lt;P&gt;1. &lt;STRONG&gt;full traceability&lt;/STRONG&gt;: every log entry, including those in disconnected background tasks is tagged with a consistent Correlation ID&lt;/P&gt;&lt;P&gt;2. &lt;STRONG&gt;Stability&lt;/STRONG&gt;: eliminated "socket hang up" errors caused by WSGI middleware incompabilties&lt;/P&gt;&lt;P&gt;3. &lt;STRONG&gt;Observability&lt;/STRONG&gt;: logs appear in Kibana as structured JSON, allowing for deep filtering on specific correlation id or error states.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;this&lt;/SPAN&gt; &lt;SPAN&gt;approach&lt;/SPAN&gt; &lt;SPAN&gt;offers&lt;/SPAN&gt;&lt;SPAN&gt; a &lt;/SPAN&gt;&lt;SPAN&gt;blueprint&lt;/SPAN&gt;&lt;SPAN&gt; for any team &lt;/SPAN&gt;&lt;SPAN&gt;aiming&lt;/SPAN&gt;&lt;SPAN&gt; to &lt;/SPAN&gt;&lt;SPAN&gt;utilize&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;FastAPI&lt;/SPAN&gt; &lt;SPAN&gt;alongside&lt;/SPAN&gt;&lt;SPAN&gt; the &lt;/SPAN&gt;&lt;SPAN&gt;strict&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;enterprise &lt;/SPAN&gt;&lt;SPAN&gt;observability&lt;/SPAN&gt; &lt;SPAN&gt;requirements&lt;/SPAN&gt; &lt;SPAN&gt;of&lt;/SPAN&gt;&lt;SPAN&gt; SAP BTP.&lt;/SPAN&gt;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/artificial-intelligence-blogs-posts/implementing-thread-safe-structured-logging-for-python-fastapi/ba-p/14292907"/>
    <published>2025-12-18T05:41:59.266000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/new-machine-learning-nlp-and-ai-features-in-sap-hana-cloud-2025-q4/ba-p/14293152</id>
    <title>New Machine Learning, NLP and AI features in SAP HANA Cloud 2025 Q4</title>
    <updated>2025-12-22T08:08:25.334000+01:00</updated>
    <author>
      <name>ChristophMorgen</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/14106</uri>
    </author>
    <content>&lt;P&gt;&lt;SPAN&gt;With the&amp;nbsp;&lt;/SPAN&gt;&lt;STRONG&gt;SAP HANA Cloud 2025 Q4 release&lt;/STRONG&gt;&lt;SPAN&gt;, several&amp;nbsp;&lt;/SPAN&gt;&lt;STRONG&gt;new embedded Machine Learning / AI functions &lt;/STRONG&gt;&lt;SPAN&gt;&amp;nbsp;have been released with the with the Predictive Analysis Library (PAL), the Automated Predictive Library (APL) and the NLP Services in SAP HANA Cloud.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Key new capabilities to be highlighted include&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;A new unified time series procedure, serving developers to utilize the same interface across different times series algorithms&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;text embedding model enhancements, supporting output vector dimensionality reduction, while maintaining retrieval accuracy&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;a new cross encoder model with the NLP services, for accurately re-ranking search results&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;text column input, text embedding operators with AutoML classification and regression models&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;text tokenization enhancements supporting regular expression token filtering and a new text log parsing function for detecting and determining new log pattern &lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;the HANA ML experiment monitor UI now supports visual model monitoring and drift analysis&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;SPAN&gt;An enhancement summary is available in the &lt;STRONG&gt;What’s new document&lt;/STRONG&gt; for&amp;nbsp;&lt;A href="https://help.sap.com/whats-new/2495b34492334456a49084831c2bea4e?Category=Predictive+Analysis+Library&amp;amp;Valid_as_Of=2025-12-01:2025-12-31&amp;amp;locale=en-US" target="_blank" rel="noopener noreferrer"&gt;SAP HANA Cloud database 2025.40 (QRC 4/2025)&lt;/A&gt;.&lt;/SPAN&gt;&lt;/P&gt;&lt;H2 id="toc-hId-1767386629"&gt;&amp;nbsp;&lt;/H2&gt;&lt;H2 id="toc-hId-1570873124"&gt;&lt;SPAN&gt;Time series enhancements&lt;/SPAN&gt;&lt;/H2&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Introducing Unified Time Series interfaces&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&lt;A href="https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/unified-time-series?)" target="_blank" rel="noopener noreferrer"&gt;Unified time series&lt;/A&gt; is a newly introduced interface for a simplified use of multiple time series algorithms (ARIMA, Exponential Smoothing, Bayesian Structural Time Series (BSTS) and Additive Model Analysis (aka prophet)). &lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;it allows for application developers to flexibly consume different time series analysis algorithms using the same procedure interface and a harmonized the handling of time series data patterns, avoiding algorithm-specific data preparation.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;It also supports massive, data-parallel time series modeling for maximum parallelism and performance when modeling thousands of time series in parallel&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="ChristophMorgen_0-1766163060120.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/354281i72B7D8752D9AD268/image-size/large?v=v2&amp;amp;px=999" role="button" title="ChristophMorgen_0-1766163060120.png" alt="ChristophMorgen_0-1766163060120.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;A more detailed introduction to the new function is given in the following blog post &lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/simplifying-time-series-analytics-with-unified-time-series-interface/ba-p/14292218" target="_blank"&gt;https://community.sap.com/t5/technology-blog-posts-by-sap/simplifying-time-series-analytics-with-unified-time-series-interface/ba-p/14292218&lt;/A&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;AutoML time series now support Prediction Intervals&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;AutoML time series predict function, in addition to all regular time series functions, now supports prediction intervals for probabilistic forecasting &lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;the uncertainty associated with a forecast is quantified by providing a range (lower/upper bounds) into which a future observation likely falls with a specific confidence level.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;For example a 95% prediction interval contains the true value 95% of the time&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="ChristophMorgen_1-1766163060130.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/354280i60475911734D2868/image-size/large?v=v2&amp;amp;px=999" role="button" title="ChristophMorgen_1-1766163060130.png" alt="ChristophMorgen_1-1766163060130.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H2 id="toc-hId-1374359619"&gt;&amp;nbsp;&lt;/H2&gt;&lt;H2 id="toc-hId-1177846114"&gt;&lt;SPAN&gt;Text embedding model enhancements (NLP services)&lt;/SPAN&gt;&lt;/H2&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Output vector dimension reduction&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;The Text Embedding models in SAP HANA Cloud have been enhanced with an attached a linear layer&lt;/SPAN&gt;​​&lt;SPAN&gt;, derived from PCA trainings, to allow for the output vector dimensionality reduction&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;The arget dimension cardinality can be flexibly set to 128, 256 384, 512, 768 dimensions using &lt;/SPAN&gt;​​&lt;SPAN&gt;PCA_DIM_NUM parameter&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;A near-original retrieval task accuracy is sustained with 256 dimensions, at 1/3rd of the original vector size (768 dimensions)&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;While lower dimension values than 128 would lead to significant and critical-level accuracy loss for retrieval tasks, hence 256 dimensions&lt;/SPAN&gt;​​&lt;SPAN&gt; is recommended for efficiency &amp;amp; performance&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Now &lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt;text embeddings of significantly &lt;STRONG&gt;&lt;EM&gt;lower dimensionality can be utilized at a&lt;/EM&gt;&lt;/STRONG&gt; &lt;STRONG&gt;&lt;EM&gt;minimal information loss&lt;/EM&gt;&lt;/STRONG&gt; for retrieval tasks as well as machine learning.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Moreover &lt;STRONG&gt;s&lt;EM&gt;maller vector sizes &lt;/EM&gt;&lt;/STRONG&gt;unlock much &lt;STRONG&gt;&lt;EM&gt;faster machine learning &lt;/EM&gt;&lt;/STRONG&gt;processing times and may also serve &lt;STRONG&gt;&lt;EM&gt;vector retrieval queries&lt;/EM&gt;&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="ChristophMorgen_2-1766163135495.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/354283iE2DA5A040F70C036/image-size/large?v=v2&amp;amp;px=999" role="button" title="ChristophMorgen_2-1766163135495.png" alt="ChristophMorgen_2-1766163135495.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;A more detailed introduction to the output vector dimensionality reduction is given in the following blog post&lt;/SPAN&gt;&amp;nbsp;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/new-cross-encoder-and-text-embedding-support-dimensionality-reduction-in/ba-p/14293164" target="_blank"&gt;New Cross Encoder and Text Embedding support Dimensionality Reduction in HANA NLP Service 2025 Q4- SAP Community blog post.&lt;/A&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-981332609"&gt;&lt;SPAN&gt;Text feature data support with AutoML models&lt;/SPAN&gt;&lt;/H2&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Text column data and Text Embedding Operator in AutoML models&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;With the introduction of the a new &lt;STRONG&gt;text embedding operator&lt;/STRONG&gt; for AutoML models, &lt;STRONG&gt;text columns&lt;/STRONG&gt; can be directly used as input feature data and benefit from &lt;EM&gt;automatic&lt;/EM&gt;, and &lt;EM&gt;optimized text vectorization&lt;/EM&gt; utilizing SAP HANA Clouds text embedding models.&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Text columns can be processed as feature, specified with the new parameter TEXT_VARIABLE&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;In addition, the new TextEmbedding operator supports the new target dimension reduction with the parameter PCA_DIM_NUM&amp;nbsp; &lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;The enhancement are available with the SQL interface as well as hana-ml 2.27 in Python&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;SPAN&gt;With that, the semantic insights from text columns can get automatically unlocked when building AutoML classification / regression models.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="ChristophMorgen_3-1766163135508.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/354284i535154FFAB1373DB/image-size/large?v=v2&amp;amp;px=999" role="button" title="ChristophMorgen_3-1766163135508.png" alt="ChristophMorgen_3-1766163135508.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-784819104"&gt;&lt;SPAN&gt;Cross Encoder Model (NLP services)&lt;/SPAN&gt;&lt;/H2&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Accurate re-ranking of search results&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;The family NLP services in SAP HANA Cloud has now been enriched with a new &lt;A href="https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/cross-encoder?" target="_blank" rel="noopener noreferrer"&gt;&lt;STRONG&gt;cross encoder model&lt;/STRONG&gt;&lt;/A&gt; and respective PAL functions. The cross encoder model&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Processes pairs/sets of text sentences (query, candidate results) together&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Therefore allows for a more precise semantic similarity re-ranking of search results, based on the full contextual interaction analysis between the query and the input candidate set (e.g. a an initial result set retrieved from a vector engine similarity search)&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Thus much higher accurate and relevant-ranked similarity search results can be achieved&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="ChristophMorgen_5-1766163220596.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/354289iCFA46CCE741FFD81/image-size/large?v=v2&amp;amp;px=999" role="button" title="ChristophMorgen_5-1766163220596.png" alt="ChristophMorgen_5-1766163220596.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Moreover the use of cross encoder models allows to combine multiple result sets retrieved from for example classic text search and vector engine similarity search queries into a hybrid search result, which can be passed into the cross encoder for its overall and combined re-ranking.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Custom AI, Retrieval Augmented Generation Applications (RAG) can now fully be served by Text Embedding Models, Vector Engine with Similarity Search and the Cross Encoder model, managing context privacy from the SAP HANA Cloud database.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="ChristophMorgen_6-1766163220604.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/354288i5A22ACEECABE4AC3/image-size/large?v=v2&amp;amp;px=999" role="button" title="ChristophMorgen_6-1766163220604.png" alt="ChristophMorgen_6-1766163220604.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;A more detailed introduction to the new cross encoder model is given in the following blog post&lt;/SPAN&gt;&amp;nbsp;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/new-cross-encoder-and-text-embedding-support-dimensionality-reduction-in/ba-p/14293164" target="_blank"&gt;New Cross Encoder and Text Embedding support Dimensionality Reduction in HANA NLP Service 2025 Q4- SAP Community blog post.&lt;/A&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-588305599"&gt;&lt;SPAN&gt;Text tokenization enhancements and new automated log text analysis&lt;/SPAN&gt;&lt;/H2&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Text tokenization support for regular expressions&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;The text &lt;EM&gt;pre-processing&lt;/EM&gt; function &lt;A href="https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/text-tokenize?" target="_blank" rel="noopener noreferrer"&gt;Text Tokenize &lt;/A&gt;function for &lt;EM&gt;splitting input text into smaller units called tokens, has now been enhanced and supports r&lt;/EM&gt;egular expression for filtering token output patterns.&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Custom filtering (removal/keeping) of text patterns can be applied&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;List of regular expressions, matching tokens will be kept / excluded&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Typical filtering examples include extracting e-mail addresses, URLs, or other token patterns for domain-specific needs&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="ChristophMorgen_7-1766163220609.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/354287i7BC8633CA0DAB217/image-size/large?v=v2&amp;amp;px=999" role="button" title="ChristophMorgen_7-1766163220609.png" alt="ChristophMorgen_7-1766163220609.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Automatic pattern detection from log texts&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;A new analysis function for log text documents &lt;A href="https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/text-log-parse?" target="_blank" rel="noopener noreferrer"&gt;Text Log Parse&lt;/A&gt; has been added to the Predictive Analysis Library, which allows&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;For an automatic extraction of new log patterns and derive templates for new log patterns&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;High-performant processing of log texts for log classification and automated log analysis, the ability to detect and alert for new log patterns&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="ChristophMorgen_8-1766163220624.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/354292iBFB4981D8D6C0303/image-size/large?v=v2&amp;amp;px=999" role="button" title="ChristophMorgen_8-1766163220624.png" alt="ChristophMorgen_8-1766163220624.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-391792094"&gt;&lt;SPAN&gt;Real-time prediction performance improvements&lt;/SPAN&gt;&lt;/H2&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Using PAL stateful ML models for real-time prediction performance&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;When a PAL ML model state is created, the model is parsed only once and kept as a runtime in-memory object (see PAL documentation on &lt;A href="https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/state-enabled-real-time-scoring-functions" target="_blank" rel="noopener noreferrer"&gt;state-enabled-real-time-scoring-functions&lt;/A&gt;)&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;The actual prediction-function references to the PAL ML model by its STATE_ID&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;The repeated overhead of PAL ML model parsing with every predict-function call can be avoided in scenarios &lt;/SPAN&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;with rather complex and larger PAL ML models with significant model parsing time proportion&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;the prediction runtime shall be as minimal as possible and near real-time&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;The prediction runtime performance has now been improved from ~100ms to ~20ms in exemplary use cases (see example &lt;A href="https://github.com/SAP-samples/hana-ml-samples/blob/main/PAL-SQL/usage-patterns/PAL%20ML%20model%20state%20for%20real-time%20predictions.sql" target="_blank" rel="nofollow noopener noreferrer"&gt;PAL_ML_model_for_real-time_predictions.sql&lt;/A&gt;)&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="ChristophMorgen_0-1767869320915.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/359353iBBFA7DB11C804EF3/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="ChristophMorgen_0-1767869320915.png" alt="ChristophMorgen_0-1767869320915.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;H2 id="toc-hId-195278589"&gt;&lt;SPAN&gt;ML experiment tracking and task scheduling enhancements&lt;/SPAN&gt;&lt;/H2&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Experiment tracking enhancements&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&lt;A href="https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/pal-track?" target="_blank" rel="noopener noreferrer"&gt;Tracking of experiments&lt;/A&gt;, now supports custom track entity tags&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Standard track entities generated in PAL Training, Forecast, etc. can now be enriched with custom tags like business related information associated with related track entity&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Tag is name-value pair for annotation or note, binding extra information to existing entities&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="ChristophMorgen_9-1766163220625.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/354290i96134FE7913A4153/image-size/large?v=v2&amp;amp;px=999" role="button" title="ChristophMorgen_9-1766163220625.png" alt="ChristophMorgen_9-1766163220625.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;A more detailed introduction is provided in the following blog post &lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/comprehensive-guide-to-mltrack-in-sap-hana-cloud-end-to-end-machine/ba-p/14134217" target="_blank"&gt;comprehensive-guide-to-mltrack-in-sap-hana-cloud-end-to-end-machine&lt;/A&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Moreover the Experiment Monitor in the Python Machine Learning client (hana-ml) supports for visually monitoring ML model performance degradation and drift.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="ChristophMorgen_10-1766163220626.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/354291i656B4787F3256EFB/image-size/large?v=v2&amp;amp;px=999" role="button" title="ChristophMorgen_10-1766163220626.png" alt="ChristophMorgen_10-1766163220626.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;One-off scheduling of PAL tasks&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;The &lt;A href="https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/calling-pal-with-schedule?" target="_blank" rel="noopener noreferrer"&gt;PAL procedure scheduling interfaces&lt;/A&gt;&lt;/SPAN&gt; has been enhance with a &lt;SPAN&gt;One-OFF schedule option, allowing for &lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;ad-hoc, automatic one-off scheduled execution of PAL task procedures with dynamic setting of time-frequency information based on current UTC timestamp&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;it triggers a scheduled job to execute immediately, and the corresponding one-off schedule is removed right away and doesn’t require to be manually maintained.&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId--1234916"&gt;&lt;SPAN&gt;Python ML client (hana-ml) enhancements&lt;/SPAN&gt;&lt;/H2&gt;&lt;P&gt;&lt;EM&gt;The full list of new methods and enhancements with hana_ml 2.27&amp;nbsp; is summarized in the &lt;/EM&gt;&lt;SPAN&gt;&lt;A href="https://help.sap.com/doc/cd94b08fe2e041c2ba778374572ddba9/2025_4_QRC/en-US/change_log.html" target="_blank" rel="noopener noreferrer"&gt;&lt;EM&gt;changelog for hana-ml 2.27&lt;/EM&gt;&lt;/A&gt; &lt;/SPAN&gt;&lt;EM&gt;as part of the documentation.&amp;nbsp; You can find an examples notebook illustrating the highlighted feature enhancements &lt;SPAN&gt;&lt;A href="https://github.com/SAP-samples/hana-ml-samples/blob/main/Python-API/pal/notebooks/25QRC04_2.27.ipynb" target="_blank" rel="noopener nofollow noreferrer"&gt;here 25QRC04_2.27.ipynb&lt;/A&gt;.&lt;/SPAN&gt;&lt;/EM&gt;&lt;/P&gt;&lt;P&gt;&lt;EM&gt;The key enhancements in this release include the following:&lt;/EM&gt;&lt;EM&gt;&amp;nbsp;&lt;/EM&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Time series data outlier detection with threshold support&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;The method for time series outlier detection in the Predictive Analysis Library has added support outlier threshold settings, in addition to the outlier voting capability using different outlier evaluation methods incl. &amp;nbsp;Z1 score, Z2 score, IIQR score, MAD score, IsolationForest and DBSCAN &lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;If the absolute value of outlier score is beyond the threshold, the respective data point will be considered as an outlier.&lt;BR /&gt;&lt;BR /&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Time series reports for massive, data-parallel model scenarios&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Massive AutoML Time Series modeling scenarios often utilize random-search with hyperband as the fastest optimization, potentially with larger number of time series data segment groups to be processed and forecasted in parallel, each time series segment group again with a significant number of forecast models to be explored.&amp;nbsp; &lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Hence the display of forecasts which have been explored by AutoML within each time series segment group is collapsed by default and can be expanded for review. &lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="ChristophMorgen_11-1766163388491.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/354294iEE72CE6111257A87/image-size/large?v=v2&amp;amp;px=999" role="button" title="ChristophMorgen_11-1766163388491.png" alt="ChristophMorgen_11-1766163388491.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Classification and regression function enhancements&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Support Vector Machine (SVM)&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;SPAN&gt; model training is computationally expensive, and computational costs are specifically sensitive to the number of training points, which makes SVM models often impractical for large datasets. &lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;The SVM algorithm now supports &lt;STRONG&gt;Coreset Sampling&lt;/STRONG&gt; &lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;which allows to automatically sample small, representative subsets (the "coreset") from larger datasets, &lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;enabling faster, more efficient training and processing while maintaining similar model accuracy as using the full data. &lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;SPAN&gt;This enhancement significantly reduces SVM training time with minimal impact on accuracy. &lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&amp;nbsp;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;The&lt;STRONG&gt; model report &lt;/STRONG&gt;for&lt;STRONG&gt; classification &lt;/STRONG&gt;tasks now supports a&lt;STRONG&gt; percentage display &lt;/STRONG&gt;in&lt;STRONG&gt; confusion matrix &lt;/STRONG&gt;for easier visual interpretation of classification results.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="ChristophMorgen_12-1766163388493.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/354293i102668E672266574/image-size/large?v=v2&amp;amp;px=999" role="button" title="ChristophMorgen_12-1766163388493.png" alt="ChristophMorgen_12-1766163388493.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;High-dimensional feature data reduction using UMAP &lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&lt;A href="https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/uniform-manifold-approximation-and-projection?version=LATEST&amp;amp;q=UMAP&amp;amp;locale=en-US" target="_blank" rel="noopener noreferrer"&gt;UMAP (Uniform Manifold Approximation and Projection)&lt;/A&gt; is a non-linear dimensionality reduction algorithm used to simplify complex, high-dimensional feature spaces, while preserving its essential structure. It is widely considered the modern gold standard for visualizing targeted dimension reduction of large-scale datasets, because it balances computational speed with the ability to maintain both local and global relationships.&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;It reduces thousands of variables (dimensions) into 2D or 3D scatter plots that humans can easily interpret.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Unlike comparable methods like t-SNE, UMAP is better at preserving global structure, meaning the relative positions between different clusters remain more meaningful.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;It is significantly faster and more memory-efficient than t-SNE, capable of processing datasets with millions of points in a reasonable timeframe.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;It can be used as a "transformer" preprocessing step in Machine Learning scenarios to reduce large feature spaces before applying clustering (e.g., k-means, HDBSCAN) or classification models, often improving their performance.&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Calculating pairwise distances &lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Many algorithms, for example clustering algorithms utilize distance matrixes as a preprocessing step, often inbuild the functions. Often there is the wish to decouple though the distance matrix calculation from the follow-up task like the actual clustering. Moreover, if decoupled custom calculated matrixes can be fed into algorithms as input.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Most PAL clustering functions support to feed-in a pre-calculated similarity matrix&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;Now, a pairwise &lt;A href="https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/distance-md?version=LATEST&amp;amp;q=distance&amp;amp;locale=en-US" target="_blank" rel="noopener noreferrer"&gt;distance calculation function&lt;/A&gt; is provided&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;It supports distance metrics like &lt;EM&gt;Manhattan, Euclidien, Minkowski, Chebyshey&lt;/EM&gt; as well as &lt;EM&gt;Levenshtein&lt;/EM&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;The &lt;STRONG&gt;Levenshtein distance&lt;/STRONG&gt; (or edit distance) is a distance metric specifically targeting distance between text-columns. It calculates the minimum number of single-character edits (insertions, deletions, or substitutions) needed to transform one word into another, acting as a measure of their similarity. A lower distance indicates a higher similarity.&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;SPAN&gt;Applicable use cases&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;It is useful in data cleaning, table column similarity analysis between columns of the same data type.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;After calculating the column similarity across all data types, clustering like K-Means can be applied to group similar fields and propose mappings for fields within the same cluster&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;EM&gt;Again, an incredible set of enhancements in the SAP HANA Cloud database AI engine and NLP services!&lt;/EM&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Enjoy to try out all the enhancements and let us know what you think here!&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/new-machine-learning-nlp-and-ai-features-in-sap-hana-cloud-2025-q4/ba-p/14293152"/>
    <published>2025-12-22T08:08:25.334000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/leveraging-sap-ai-core-to-build-custom-ai-agents-with-crewai/ba-p/14279604</id>
    <title>Leveraging SAP AI Core to Build Custom AI Agents with CrewAI</title>
    <updated>2025-12-24T06:41:18.594000+01:00</updated>
    <author>
      <name>Manisha_19</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1695623</uri>
    </author>
    <content>&lt;H1 id="toc-hId-1636640266"&gt;Introduction&lt;/H1&gt;&lt;P&gt;AI agents are becoming the go-to pattern for building modular, autonomous assistants that can think, act, and collaborate. &lt;STRONG&gt;CrewAI&lt;/STRONG&gt;&amp;nbsp;gives you a developer-friendly way to orchestrate these agents, while &lt;STRONG&gt;SAP AI Core&lt;/STRONG&gt;&amp;nbsp;provides enterprise-grade access to LLMs with proper governance, scalability, and security.&lt;/P&gt;&lt;P&gt;This quickstart shows you how to connect &lt;STRONG&gt;CrewAI&lt;/STRONG&gt;&amp;nbsp;to &lt;STRONG&gt;SAP AI Core&lt;/STRONG&gt;, create a custom LLM interface, and build your first working agent — all in under 10 minutes.&lt;/P&gt;&lt;BLOCKQUOTE&gt;&lt;P&gt;By the end, you’ll have a simple &lt;STRONG&gt;CrewAI&lt;/STRONG&gt; agent powered by an LLM hosted in &lt;EM&gt;SAP AI Core&lt;/EM&gt;.&lt;/P&gt;&lt;/BLOCKQUOTE&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="toc-hId-1440126761"&gt;How SAP Facilitates Building Custom Agents&lt;/H1&gt;&lt;P&gt;CrewAI handles the &lt;EM&gt;agent orchestration layer&lt;/EM&gt;, while SAP AI Core hosts and manages your LLMs. &amp;nbsp;&lt;/P&gt;&lt;P&gt;Connecting them allows you to use SAP’s AI infrastructure directly inside your CrewAI workflows — no external API juggling.&lt;/P&gt;&lt;P&gt;Here’s the high-level flow:&lt;/P&gt;&lt;P&gt;User Prompt → CrewAI Agent → Custom LLM Class → SAP AI Core Endpoint → Model Response&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Flow summary:&lt;/STRONG&gt;&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;User sends a prompt → CrewAI Agent receives it.&lt;/LI&gt;&lt;LI&gt;Agent forwards it to a custom LLM wrapper (`SAPAILLM`).&lt;/LI&gt;&lt;LI&gt;The wrapper authenticates using the token and sends the request to SAP AI Core.&lt;/LI&gt;&lt;LI&gt;SAP AI Core executes the model and returns a response to the agent.&lt;/LI&gt;&lt;LI&gt;CrewAI handles the output and delivers the final answer.&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;This setup ensures enterprise-level reliability with full control over which models your agents use.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="toc-hId-1243613256"&gt;Prerequisites&lt;/H1&gt;&lt;P&gt;Before jumping in, make sure you have:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Access to &lt;STRONG&gt;SAP AI Core&lt;/STRONG&gt;&amp;nbsp;with deployed LLM models (via AI Launchpad)&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Python 3.10+&lt;/STRONG&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;VS Code&lt;/STRONG&gt;&amp;nbsp;or your favorite IDE&lt;/LI&gt;&lt;LI&gt;Installed libraries:&lt;/LI&gt;&lt;/UL&gt;&lt;pre class="lia-code-sample language-bash"&gt;&lt;code&gt;pip install ipykernel
pip install crewai
pip install crewai_tools
pip install generative-ai-hub-sdk[all]
pip install langchain_community&lt;/code&gt;&lt;/pre&gt;&lt;UL&gt;&lt;LI&gt;Your SAP AI Core Service Key downloaded as a .json file&lt;/LI&gt;&lt;LI&gt;Your A-game to create some amazing agents&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;*If you want to learn how to deploy your LLM model on launchpad, you can go through this &lt;A href="https://developers.sap.com/tutorials/ai-core-generative-ai.html" target="_self" rel="noopener noreferrer"&gt;course.&lt;/A&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="toc-hId-1047099751"&gt;Step-by-Step Guide&lt;/H1&gt;&lt;P&gt;Let us now dive into building our own custom agent which utilizes SAP's AI Core.&lt;/P&gt;&lt;H2 id="toc-hId-979668965"&gt;Step 1: Get Your SAP AI Core Credentials&lt;/H2&gt;&lt;P&gt;In your SAP BTP Cockpit → AI Core Instance → Service Keys, create a new key and save it locally as credentials.json.&lt;/P&gt;&lt;P&gt;Example structure:&lt;/P&gt;&lt;pre class="lia-code-sample language-json"&gt;&lt;code&gt;{
    "clientid": "your-client-id",
    "clientsecret": "your-client-secret",
    "url": "https://&amp;lt;your-region&amp;gt;.authentication.sap.hana.ondemand.com",
    "serviceurls": {
        "AI_API_URL": "https://***.aws.ml.hana.ondemand.com"
    }
}&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;This file contains everything needed to authenticate and access your deployed LLM.&lt;/P&gt;&lt;H2 id="toc-hId-783155460"&gt;Step 2: Get Your BTP LLM Access Token&lt;/H2&gt;&lt;P&gt;We’ll use this token to authenticate each request to SAP AI Core.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import json, requests

# open credentials file
with open("&amp;lt;path-to-file&amp;gt;/credentials.json", "r") as key_file:
    svcKey = json.load(key_file)
authUrl = svcKey["url"]
clientid = svcKey["clientid"]
clientsecret = svcKey["clientsecret"]
apiUrl = svcKey["serviceurls"]["AI_API_URL"]

# request token
params = {"grant_type": "client_credentials" }
resp = requests.post(f"{authUrl}/oauth/token",
                    auth=(clientid, clientsecret),
                    params=params)

BtpLlmAccessToken = resp.json()["access_token"]
print("Token retrieved successfully!")&lt;/code&gt;&lt;/pre&gt;&lt;BLOCKQUOTE&gt;&lt;P&gt;Tokens usually expire after 1 hour. Make sure to refresh them before making multiple requests.&lt;/P&gt;&lt;/BLOCKQUOTE&gt;&lt;H2 id="toc-hId-586641955"&gt;Step 3: Create a Custom LLM Class Template for CrewAI&lt;/H2&gt;&lt;P&gt;CrewAI lets you define your own LLM wrappers, which means we can make one that talks directly to SAP AI Core.&lt;/P&gt;&lt;P&gt;In this class, we can define the procedure to use the deployed models from SAP AI Core.&lt;/P&gt;&lt;P&gt;You can now import this class into any CrewAI workflow to use SAP’s hosted LLMs.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from crewai import BaseLLM
from typing import Any, Dict, List, Optional, Union

class CustomLLM(BaseLLM):
    def __init__(self, model: str, api_key: str, endpoint: str, temperature: Optional[float] = None):
        super().__init__(model=model, temperature=temperature)

        self.api_key = api_key
        self.endpoint = endpoint

    def call(
        self,
        messages: Union[str, List[Dict[str, str]]],
        tools: Optional[List[dict]] = None,
        **kwargs
    ) -&amp;gt; Union[str, Any]:
        """Call the LLM with the given messages."""
        # Convert string to message format if needed
        if isinstance(messages, str):
            messages = [{"role": "user", "content": messages}]

        payload = {
            "messages": messages,
            "temperature": self.temperature,
            "max_tokens": 1000 # This can be modified as per your use case. You can parameterize this as well.
        }

        headers = {
            'AI-Resource-Group': "&amp;lt;your-resource-group-name&amp;gt;",
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {BtpLlmAccessToken}',
        }

        # Make API call
        response = requests.post(
            self.endpoint,
            headers= headers,
            json=payload,
            timeout=30
        )
        response.raise_for_status()

        result = response.json()
        return result["choices"][0]["message"]["content"]&lt;/code&gt;&lt;/pre&gt;&lt;H2 id="toc-hId-390128450"&gt;Step 4: Initialize your Custom LLM Class&lt;/H2&gt;&lt;P&gt;Alright, time to bring that class to life. &amp;nbsp;&lt;/P&gt;&lt;P&gt;The cool thing here is that you can hook it up to any model you’ve deployed in SAP AI Core, whether it’s GPT-4o, Claude, or your own fine-tuned beast.&lt;/P&gt;&lt;P&gt;In this example, we’ll wire it up to a **&lt;STRONG&gt;GPT-4o&lt;/STRONG&gt;** deployment and let CrewAI start chatting through it. You can get the deployment endpoint from your SAP AI Launchpad. You can find the payload formats for different models &lt;A href="https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/example-payloads-for-inferencing-sap-ai-core-hosted" target="_self" rel="noopener noreferrer"&gt;here&lt;/A&gt;.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;deployment_url = "&amp;lt;deployment URL&amp;gt;" + "/chat/completions?api-version=2024-02-01"

# instantiating Custom LLM object
custom_llm = CustomLLM(
    model="gpt-4o",
    api_key=BtpLlmAccessToken,
    endpoint=deployment_url,
    temperature=0.7
)&lt;/code&gt;&lt;/pre&gt;&lt;H2 id="toc-hId-193614945"&gt;Step 5: Build a Simple CrewAI Agent&lt;/H2&gt;&lt;P&gt;Now that your LLM class is ready, let’s create a basic Research CrewAI agent that uses it.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from crewai import Agent, Task, Crew

# Load credentials
agent = Agent(
    role="Research Assistant",
    goal="Find and analyze information",
    backstory="You serve as a research assistant, helping to gather and analyze information about SAP tools and related topics.",
    llm=custom_llm
)

# Create and execute tasks
task = Task(
    description="Research the latest developments in SAP AI Core",
    expected_output="A comprehensive summary",
    agent=agent
)

crew = Crew(agents=[agent], tasks=[task])
result = crew.kickoff()&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;If all goes well, you’ll see a clean response from your SAP-hosted LLM, directly through your CrewAI agent. Here is a sample output:&lt;/P&gt;&lt;BLOCKQUOTE&gt;&lt;P&gt;CrewOutput(raw="The latest developments in SAP AI Core focus on enhancing the capabilities for deploying and managing AI models in a scalable and efficient manner. SAP AI Core is part of SAP Business Technology Platform (BTP) and is designed to integrate with other SAP applications to streamline AI operat....ding tools that enhance productivity, and driving innovation through AI-powered solutions.", pydantic=None, json_dict=None, agent='Research Assistant', output_format=&amp;lt;OutputFormat.RAW: 'raw'&amp;gt;)], token_usage=UsageMetrics(total_tokens=0, prompt_tokens=0, cached_prompt_tokens=0, completion_tokens=0, successful_requests=0))&lt;/P&gt;&lt;/BLOCKQUOTE&gt;&lt;H1 id="toc-hId--131981279"&gt;&amp;nbsp;&lt;/H1&gt;&lt;H1 id="toc-hId-441245299"&gt;Conclusion&lt;/H1&gt;&lt;P&gt;And that’s it — you just built your first CrewAI agent powered by &lt;STRONG&gt;SAP AI Core&lt;/STRONG&gt;.&lt;/P&gt;&lt;P&gt;From here, you can:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Add tools for the agent to interact with SAP APIs (e.g., fetching invoice data).&lt;/LI&gt;&lt;LI&gt;Add memory and multi-agent orchestration.&lt;/LI&gt;&lt;LI&gt;Deploy it on &lt;STRONG&gt;SAP BTP&lt;/STRONG&gt;&amp;nbsp;or your internal infrastructure for enterprise use.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;This quick start gives you the foundation — the next step is making your agent truly &lt;EM&gt;yours&lt;/EM&gt;.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="toc-hId-244731794"&gt;References&lt;/H1&gt;&lt;UL&gt;&lt;LI&gt;&lt;A href="https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/example-payloads-for-inferencing-sap-ai-core-hosted" target="_blank" rel="noopener noreferrer"&gt;Example Payloads for Inferencing - SAP AI Core Hosted | SAP Help Portal&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://developers.sap.com/tutorials/ai-core-generative-ai.html" target="_blank" rel="noopener noreferrer"&gt;Prompt LLMs in the generative AI hub in SAP AI Core &amp;amp; Launchpad | SAP Tutorials&lt;/A&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/leveraging-sap-ai-core-to-build-custom-ai-agents-with-crewai/ba-p/14279604"/>
    <published>2025-12-24T06:41:18.594000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/new-machine-learning-nlp-and-ai-features-in-sap-hana-cloud-2025-q3/ba-p/14304443</id>
    <title>New Machine Learning, NLP and AI features in SAP HANA Cloud 2025 Q3</title>
    <updated>2026-01-09T12:54:46.437000+01:00</updated>
    <author>
      <name>ChristophMorgen</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/14106</uri>
    </author>
    <content>&lt;P&gt;&lt;SPAN&gt;With the SAP HANA Cloud 2025 Q3 release, several new embedded Machine Learning / AI functions&amp;nbsp;have been released with the SAP HANA Cloud Predictive Analysis Library (PAL) and the Automated Predictive Library (APL). &lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;An enhancement summary is available in the What’s new document for &lt;A href="https://help.sap.com/whats-new/2495b34492334456a49084831c2bea4e?Category=Predictive+Analysis+Library&amp;amp;Valid_as_Of=2025-09-01:2025-09-30&amp;amp;locale=en-US" target="_self" rel="noopener noreferrer"&gt;SAP HANA Cloud database 2025.28 (QRC 3/2025)&lt;/A&gt;.&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;H2 id="toc-hId-1787736735"&gt;&amp;nbsp;&lt;/H2&gt;&lt;H2 id="toc-hId-1591223230"&gt;&lt;SPAN&gt;Time series analysis and forecasting function enhancements&lt;/SPAN&gt;&lt;/H2&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Threshold support in timeseries outlier detection &lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;In time series, an outlier is a data point that is different from the general behavior of remaining data points.&amp;nbsp; In the PAL &lt;STRONG&gt;&lt;EM&gt;time series outlier detection&lt;/EM&gt;&lt;/STRONG&gt; function, the outlier detection task is divided into two steps&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;In step 1 the residual values are derived from the original series, &lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;In step 2, the outliers are detected from the residual values.&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;SPAN&gt;Multiple methods are available to evaluate a data point to be an outlier or not. &lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Including Z1 score, Z2 score, IIQR score, MAD score, IsolationForest, DBSCAN&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;If used in combination, outlier voting can be applied for a combined evaluation.&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;SPAN&gt;Now, &lt;STRONG&gt;new&lt;/STRONG&gt; and in addition, &lt;STRONG&gt;&lt;EM&gt;thresholds values for outlier scores&lt;/EM&gt;&lt;/STRONG&gt; are supported&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;New parameter OUTPUT_OUTLIER_THRESHOLD &lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Based on the given threshold value, if the time series value is beyond the (upper and lower) outlier threshold for the time series, the corresponding data point as an outlier.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Only valid when outlier_method = 'iqr', 'isolationforest', 'mad', 'z1', 'z2'.&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="ChristophMorgen_0-1767958753257.jpeg" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/359750iE20F7716FF87FA07/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="ChristophMorgen_0-1767958753257.jpeg" alt="ChristophMorgen_0-1767958753257.jpeg" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;H2 id="toc-hId-1394709725"&gt;&lt;SPAN&gt;Classification and regression function enhancements&lt;/SPAN&gt;&lt;/H2&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Corset sampling support with SVM models&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Coreset sampling&lt;/STRONG&gt;&amp;nbsp;is a machine learning technique to&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;select a small, representative subset (the "coreset") from larger datasets,&lt;/LI&gt;&lt;LI&gt;enabling faster, more efficient training and processing while maintaining similar model accuracy as using the full data.&lt;/LI&gt;&lt;LI&gt;It works by identifying the most "informative" samples, filtering out redundant or noisy data, and allowing complex algorithms to run on a manageable dataset sizes.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Support Vector Machine (SVM)&lt;/STRONG&gt;&amp;nbsp;model training is computationally expensive, and computational costs are specifically sensitive to the number of training points, which makes SVM models often impractical for large datasets.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Therefore SVM in the Predictive Analysis Library has been enhanced and now&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;offers&amp;nbsp;&lt;STRONG&gt;embedded coreset sampling&lt;/STRONG&gt;&amp;nbsp;capabilities&lt;/LI&gt;&lt;LI&gt;enabled with the new parameters USE_CORESET and CORESET_SCALE as the &lt;SPAN&gt;sampling ratio when constructing coreset&lt;/SPAN&gt;.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;This enhancement significantly reduces SVM training time with minimal impact on accuracy.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="ChristophMorgen_1-1767958753264.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/359751iDA955B4D29D2C3A9/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="ChristophMorgen_1-1767958753264.png" alt="ChristophMorgen_1-1767958753264.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-1198196220"&gt;&lt;SPAN&gt;AutoML and pipeline function enhancements&lt;/SPAN&gt;&lt;/H2&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Target encoding support in&amp;nbsp;AutoML&amp;nbsp;&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;The PAL AutoML framework introduces a new pipeline operator for target encoding of categorial features&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Categorical data is often required to be preprocessed and required to get converted from non-numerical features into formats suitable for the respective machine learning algorithm, i.e. numeric values&lt;/SPAN&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Examples features: text labels (e.g., “red,” “blue”) or discrete categories (e.g., “high,” “medium,” “low”)&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;One-hot encoding converts each categorial feature value &amp;nbsp;into a binary column (0 or 1), which works well for features with a limited number of unique values. PAL already applies an optimized one-hot encoding method aggregating very low frequent values.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Target encoding replaces the categorial values with the mean of the target / label column for high-cardinality features, which avoids to create large and sparse one-hot encoded feature matrices&lt;/SPAN&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Example of a high cardinality feature: “city” column with hundreds-thousands of unique values, postal code, product IDs etc.&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;The PAL AutoML engine will analyze the input feature cardinality and then automatically decide if to apply target encoding or another encoding method. For medium to high cardinality categorial features, target encoding may improve the performance significantly.&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;By automating target encoding, the PAL AutoML engine aims to improve model performance and generalization, especially when dealing with complex, high-cardinality categorical features, without requiring manual intervention.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;In addition, the AutoML and pipeline function now also support columns of type half precision vector.&lt;/P&gt;&lt;H2 id="toc-hId-1001682715"&gt;&amp;nbsp;&lt;/H2&gt;&lt;H2 id="toc-hId-805169210"&gt;&lt;SPAN&gt;Misc. Machine Learning and statistics function enhancements&lt;/SPAN&gt;&lt;/H2&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;High-dimensional feature data reduction using UMAP&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;UMAP (Uniform Manifold Approximation and Projection) is a non-linear dimensionality reduction algorithm used to simplify complex, high-dimensional feature spaces, while preserving its essential structure. It is widely considered the modern gold standard for visualizing targeted dimension reduction of large-scale datasets, because it balances computational speed with the ability to maintain both local and global relationships.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;It reduces thousands of variables (dimensions) into 2D or 3D scatter plots that humans can easily interpret.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Unlike comparable methods like t-SNE, UMAP is better at preserving global structure, meaning the relative positions between different clusters remain more meaningful.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;It is significantly faster and more memory-efficient than t-SNE, capable of processing datasets with millions of points in a reasonable timeframe.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;It can be used as a "transformer" preprocessing step in Machine Learning scenarios to reduce large feature spaces before applying clustering (e.g., k-means, HDBSCAN) or classification models, often improving their performance.&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;SPAN&gt;The following new functions are introduced&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;_SYS_AFL.PAL_UMAP&lt;/SPAN&gt;​ with the most important &lt;SPAN&gt;parameters N_NEIGHBORS, MIN_DIST, N_COMPONENTS, DISTANCE_LEVEL&lt;/SPAN&gt;​&lt;/LI&gt;&lt;/UL&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;_SYS_AFL.PAL_TRUSTWORTHINESS&lt;/SPAN&gt;​, u&lt;SPAN&gt;sed to measure the structure similarity between original high dimensional space and embedded low dimensional space based on K nearest neighbors.&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Calculating pairwise distances&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Many algorithms, for example clustering algorithms utilize distance matrixes as a preprocessing step, often inbuild to the functions. While often there is the wish to decouple though the distance matrix calculation from the follow-up task like the actual clustering. Moreover, if decoupled custom calculated matrixes can be fed into algorithms as input.&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Most PAL clustering functions support to feed-in a pre-calculated similarity matrix&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;SPAN&gt;Now, a dedicated &lt;A href="https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/distance-md?version=LATEST&amp;amp;q=distance&amp;amp;locale=en-US" target="_blank" rel="noopener noreferrer"&gt;pairwise distance calculation&lt;/A&gt; function is provided &lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;It supports distance metrics like &lt;EM&gt;Manhattan, Euclidien, Minkowski, Chebyshey&lt;/EM&gt; as well as &lt;STRONG&gt;Levenshtein&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;The &lt;STRONG&gt;&lt;EM&gt;Levenshtein distance&lt;/EM&gt;&lt;/STRONG&gt; (or “edit distance”) is a distance metric specifically targeting distance between text-columns. &lt;/SPAN&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;It calculates the minimum number of single-character edits (insertions, deletions, or substitutions) needed to transform one word into another, acting as a measure of their similarity. A lower distance indicates a higher similarity.&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;SPAN&gt;Applicable use cases&lt;/SPAN&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;It is useful in data cleaning, table column similarity analysis between columns of the same data type.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;After calculating the column similarity across all data types, clustering like K-Means can be applied to group similar fields and propose mappings for fields within the same cluster&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Real Vector data type support&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;The following PAL functions have been enhanced to support columns of type real vector&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Spectral Clustering&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Cluster Assignment&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Decision tree&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Sampling&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;In addition the AutoML and pipeline function now also support columns of type half precision vector.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-608655705"&gt;&lt;SPAN&gt;Creating Vector Embeddings enhancements&lt;/SPAN&gt;&lt;/H2&gt;&lt;P&gt;&lt;SPAN&gt;The SAP HANA Database Vector Engine function VECTOR_EMBEDDING()&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;has added support for remote, SAP AI Core exposed embedding models. Detailed instruction are given in the documentation at&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;&lt;A href="https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-vector-engine-guide/creating-text-embeddings-with-sap-ai-core" target="_blank" rel="noopener noreferrer"&gt;Creating Text Embeddings with SAP AI Core | SAP Help Portal&lt;/A&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-412142200"&gt;&lt;SPAN&gt;Python ML client (hana-ml) enhancements&lt;/SPAN&gt;&lt;/H2&gt;&lt;P&gt;&lt;EM&gt;The full list of new methods and enhancements with hana_ml 2.26&amp;nbsp; is summarized in the &lt;/EM&gt;&lt;SPAN&gt;&lt;A href="https://help.sap.com/doc/cd94b08fe2e041c2ba778374572ddba9/2025_3_QRC/en-US/change_log.html" target="_blank" rel="noopener noreferrer"&gt;&lt;EM&gt;changelog for hana-ml 2.26&lt;/EM&gt;&lt;/A&gt; &lt;/SPAN&gt;&lt;EM&gt;as part of the documentation. The key enhancements in this release include&lt;/EM&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;New&amp;nbsp;Functions&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Added text tokenization API.&lt;/LI&gt;&lt;LI&gt;Added explainability support with IsolationForest Outlier Detection&lt;/LI&gt;&lt;LI&gt;Added constrained clustering API.&lt;/LI&gt;&lt;LI&gt;Added intermittent time series data test in time series report.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Enhancements&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Support time series SHAP visualizations for AutoML Timeseries model explanations&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;You can find an examples notebook illustrating the highlighted feature enhancements &lt;SPAN&gt;&lt;A href="https://github.com/SAP-samples/hana-ml-samples/blob/main/Python-API/pal/notebooks/25QRC03_2.26.ipynb" target="_blank" rel="nofollow noopener noreferrer"&gt;here 25QRC03_2.26.ipynb&lt;/A&gt;.&amp;nbsp; &lt;/SPAN&gt;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/new-machine-learning-nlp-and-ai-features-in-sap-hana-cloud-2025-q3/ba-p/14304443"/>
    <published>2026-01-09T12:54:46.437000+01:00</published>
  </entry>
</feed>
