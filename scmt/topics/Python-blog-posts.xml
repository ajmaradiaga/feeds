<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>https://raw.githubusercontent.com/ajmaradiaga/feeds/main/scmt/topics/Python-blog-posts.xml</id>
  <title>SAP Community - Python</title>
  <updated>2025-12-10T12:11:51.168182+00:00</updated>
  <link href="https://community.sap.com/t5/c-khhcw49343/Python/pd-p/f220d74d-56e2-487e-8e6c-a8cb3def2378" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <subtitle>Python blog posts in SAP Community</subtitle>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/exposing-sap-s4-onprem-data-to-external-system-odata-service-btp/ba-p/14151915</id>
    <title>Exposing SAP S4 Onprem data to external System || Odata Service || BTP || Destination-Connectivity</title>
    <updated>2025-07-24T09:09:11.979000+02:00</updated>
    <author>
      <name>Ace_D</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1501366</uri>
    </author>
    <content>&lt;H2 id="toc-hId-1735011379"&gt;&lt;STRONG&gt;Prerequisites from BTP Side&lt;/STRONG&gt;&lt;/H2&gt;&lt;OL&gt;&lt;LI&gt;&lt;STRONG&gt;SAP BTP Account&lt;/STRONG&gt;&lt;UL&gt;&lt;LI&gt;Access to SAP Business Technology Platform (BTP) with appropriate entitlements.&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;SAP Cloud Connector Setup&lt;/STRONG&gt;&lt;UL&gt;&lt;LI&gt;SAP Cloud Connector installed and configured to connect your on-premise S/4HANA system to SAP BTP.&lt;/LI&gt;&lt;LI&gt;Destination configured in BTP cockpit pointing to your on-premise system.&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Connectivity Service and Destination Service&lt;/STRONG&gt;&lt;UL&gt;&lt;LI&gt;SAP BTP Connectivity service and destination service instance on the BTP account.&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Cloud Foundry Environment&lt;/STRONG&gt;&lt;UL&gt;&lt;LI&gt;Cloud Foundry space set up in your BTP subaccount to deploy the application.&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&lt;STRONG&gt;Use Case:&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Lets take a simple use case for creating a RAP service on S4, which is a wrapper API call on the reprocess IDOC function module.&lt;/P&gt;&lt;P&gt;For this we will create a RAP service on the S4 box, than activate the service with the &lt;STRONG&gt;/IWFND/MAINT_SERVICE&amp;nbsp;&lt;/STRONG&gt;and activate the &lt;STRONG&gt;ICF Node as well.&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Note: In this blog we will not go through the steps of setting cloud connector and destination on the BTP account. We will assume that destination with cloud connector setup is already available on the BTP.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 1: Create and activate the RAP service on the S4 Box.&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;We will create a unmanaged rap scenario with custom entity and than we will create a service definition and top of service definition we will create service binding.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_0-1752479114889.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286063i6147300A58FD94EB/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_0-1752479114889.png" alt="Ace_D_0-1752479114889.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Lets create the class for the query implementation.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_1-1752479219667.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286066i8FE4DD17F2125A3B/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_1-1752479219667.png" alt="Ace_D_1-1752479219667.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;DIV class=""&gt;&amp;nbsp;&lt;/DIV&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_3-1752479284513.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286068i7DF33D859E64B0B5/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_3-1752479284513.png" alt="Ace_D_3-1752479284513.png" /&gt;&lt;/span&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_4-1752479314425.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286069i1ADC607EAFF20853/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_4-1752479314425.png" alt="Ace_D_4-1752479314425.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;We are calling the FM to reprocess the idoc and checking the relevant table to get the latest reprocessed idoc status for the same.&lt;/P&gt;&lt;P&gt;Now we will create a service definition and service binding for the custom entity created.&lt;/P&gt;&lt;P&gt;Service Definition&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_8-1752479854976.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286073i72B0728E2CDB8D9C/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_8-1752479854976.png" alt="Ace_D_8-1752479854976.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;And overall project will look something like this.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_5-1752479511558.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286070iB8D8CB8704C75416/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_5-1752479511558.png" alt="Ace_D_5-1752479511558.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Here i have created two types of service bindings v2 and v4 but we will be using only v2 for this.&lt;/P&gt;&lt;P&gt;So overall we created below 4 artifacts.&lt;/P&gt;&lt;P&gt;1. Custom entity&lt;/P&gt;&lt;P&gt;2. Class&lt;/P&gt;&lt;P&gt;3. Service definition&lt;/P&gt;&lt;P&gt;4. Service binding&lt;/P&gt;&lt;P&gt;Now to test the service we will publish the service first and than call the generated url&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_7-1752479816432.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286072iDC562CE467DBAAA5/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_7-1752479816432.png" alt="Ace_D_7-1752479816432.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;if you see the image, i have published the service and after publish we also got the custom entity that we have created, click on the &lt;STRONG&gt;service url&lt;/STRONG&gt; and it will open in browser asking for authentication of S4 user id and password.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_9-1752480025822.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286074iD29880ABB4A5E44A/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_9-1752480025822.png" alt="Ace_D_9-1752480025822.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Once the authentication is successful you should be able to see this.&lt;/P&gt;&lt;P&gt;Now lets open the SAP GUI so that i can show you the ICF node activation, which is generally taken care by basis team.&lt;/P&gt;&lt;P&gt;Open the Tcode:&amp;nbsp;&lt;STRONG&gt;/IWFND/MAINT_SERVICE&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_10-1752480290098.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286075i2C11D72626448B10/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_10-1752480290098.png" alt="Ace_D_10-1752480290098.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Here you can find your activated service binding and make sure all components of the service looks as it is in the image.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Assuming cloud connector and destination is setup on BTP.&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;With this we completed the S4 box setup, now its ready to communicate with other external systems.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 2: Creating the Destination service and Connectivity service on the BTP Account.&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Login in to BTP account and lets create 2 service instances with service keys.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_11-1752483153170.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286097iDCC3A3321CE1ECB0/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_11-1752483153170.png" alt="Ace_D_11-1752483153170.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;1.&amp;nbsp;&lt;STRONG&gt;Destination service&lt;/STRONG&gt;, this will help us to get the destination details, Destination service gets all the registered destinations on the BTP, from which we can filter out the destination that is up for our S4 onprem box.&lt;/P&gt;&lt;P&gt;Destination service will give us all the relevant things like user id passwd, location and etc. for that destination.&lt;/P&gt;&lt;P&gt;Service key for the destination service&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_12-1752483270245.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286098iBE180D64FD0F615E/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_12-1752483270245.png" alt="Ace_D_12-1752483270245.png" /&gt;&lt;/span&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_13-1752483330309.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286099i127A1F287FB75995/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_13-1752483330309.png" alt="Ace_D_13-1752483330309.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;2. &lt;STRONG&gt;Connectivity Service,&amp;nbsp;&lt;/STRONG&gt;Since we are trying to get the data out of onprem system we will have to use connectivity service from btp, which will provide us proxies, this proxies will be use to call the onprem odata url.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_14-1752483491742.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286106i3806395799EEF5CB/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_14-1752483491742.png" alt="Ace_D_14-1752483491742.png" /&gt;&lt;/span&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_15-1752483598535.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286107iCD11D07C5A492C3E/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_15-1752483598535.png" alt="Ace_D_15-1752483598535.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;It is important to paste here the service keys for better understanding since we will be using many things from the service key into our application&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 3: Lets create a python application to call the onprem service.&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Important things to consider here.&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;By using destination and connectivity service we cannot test the application on local system, we will have to deploy our app on CF to test the same.&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;We will use the flask requests and certain other libraries for the python programming&lt;/P&gt;&lt;P&gt;file: .env file to store all the secrets this secrets are from the service keys only.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_2-1752492372749.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286202iF6300349E7896528/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_2-1752492372749.png" alt="Ace_D_2-1752492372749.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;File: Requirements.txt&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_0-1752492015085.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286200i27F53BD3C09B09AE/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_0-1752492015085.png" alt="Ace_D_0-1752492015085.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;File: Runtime.txt&lt;/P&gt;&lt;P&gt;python-3.11.*&lt;/P&gt;&lt;P&gt;file: manifest.yaml&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_1-1752492157202.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286201iA56CB734CFA3967C/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_1-1752492157202.png" alt="Ace_D_1-1752492157202.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;file:idocapis.py (should be same as mentioned in Manifest.yml file&lt;/P&gt;&lt;P&gt;Load all the required libraries.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_3-1752492555784.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286203iEB454EAAAAA7E66C/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_3-1752492555784.png" alt="Ace_D_3-1752492555784.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;define the function to get the token&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_4-1752493129840.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286204i152BA582C8BA1F8B/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_4-1752493129840.png" alt="Ace_D_4-1752493129840.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Define function to get the list of all destinations on the BTP&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_5-1752493241781.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286205iA640B84C01B00E5E/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_5-1752493241781.png" alt="Ace_D_5-1752493241781.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Define the function to construct the URL for the odata call&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_6-1752493319173.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286206i83806D03EC8AE6C7/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_6-1752493319173.png" alt="Ace_D_6-1752493319173.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Define function to get token for the connectivity service&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_7-1752493411094.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286213i4C439E0B0047EB4E/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_7-1752493411094.png" alt="Ace_D_7-1752493411094.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Define function to call the odata service with connectivity things&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_8-1752493551879.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286242iDE3AED9EE0D95C35/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_8-1752493551879.png" alt="Ace_D_8-1752493551879.png" /&gt;&lt;/span&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_9-1752493573946.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286243i670226B5E2674B07/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_9-1752493573946.png" alt="Ace_D_9-1752493573946.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Now lets define the final route in flask to call this service&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_13-1752494450583.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286281i0C7C7F657EECF5CA/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_13-1752494450583.png" alt="Ace_D_13-1752494450583.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;One thing to note here the&amp;nbsp;&lt;STRONG&gt;RESOURCE variable will depend on the destination url of the onprem that is setup on BTP.&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_11-1752494105816.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286279i7A1D0EA83E29C50A/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_11-1752494105816.png" alt="Ace_D_11-1752494105816.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_12-1752494133009.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286280iBAC4EB399CB3AEFD/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_12-1752494133009.png" alt="Ace_D_12-1752494133009.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;STEP 4: Deployment&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Lets deploy the app on the CF by pushing the app to the cloud foundry from the BAS&lt;/P&gt;&lt;P&gt;Use command &lt;STRONG&gt;cf push&lt;/STRONG&gt; after successful authentication for your cloud foundry space.&lt;/P&gt;&lt;P&gt;After the deployment on the cloud we can get the url that is generated on the cloud.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_14-1752495094785.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286283iF188D4ACAB2AC66A/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_14-1752495094785.png" alt="Ace_D_14-1752495094785.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Testing the application:&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Lets create a small python program to call this API in local now to test it.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Ace_D_15-1752495325813.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/286284iA1C1678704489B7A/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Ace_D_15-1752495325813.png" alt="Ace_D_15-1752495325813.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;You should get response coming from your S4 onPrem system.&lt;/P&gt;&lt;P&gt;Thanks for staying till the end!!!&lt;span class="lia-unicode-emoji" title=":smiling_face_with_smiling_eyes:"&gt;ðŸ˜Š&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/exposing-sap-s4-onprem-data-to-external-system-odata-service-btp/ba-p/14151915"/>
    <published>2025-07-24T09:09:11.979000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-2/ba-p/14167025</id>
    <title>SAP Databricks: Building an Intelligent Enterprise with AI Unleashed â€“ Part 2</title>
    <updated>2025-07-30T15:18:04.435000+02:00</updated>
    <author>
      <name>jing_wen</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1923466</uri>
    </author>
    <content>&lt;P&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-1/ba-p/14166813" target="_self"&gt;&lt;SPAN&gt;Part 1 â€“ SQL analytics with SAP Data Products&lt;BR /&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-2/ba-p/14167025" target="_self"&gt;Part 2 â€“ Build and deploy Mosaic AI and Agent Tools&lt;/A&gt;&lt;BR /&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-how-to-use-automl-to-forecast-sales-data-part-3/ba-p/14174354" target="_self"&gt;Part 3 â€“ How to use AutoML to forecast sales data&lt;/A&gt;&lt;BR /&gt;&lt;/SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-3/ba-p/14174201" target="_self"&gt;&lt;SPAN&gt;Part 4 â€“ Connect SAP Data Products with non-SAP data from AWS S3&lt;BR /&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-4/ba-p/14178056" target="_self"&gt;Part 5 â€“ End-to-end integration: SAP Databricks, SAP Datasphere, and SAP Analytics Cloud&lt;/A&gt;&lt;BR /&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-create-inferences-for-application-integration-with-sap-build/ba-p/14186662" target="_self"&gt;Part 6 â€“ Create inferences and endpoints for application integration with SAP Build&lt;/A&gt;&lt;/SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-create-inferences-for-application-integration-with-sap-build/ba-p/14186662" target="_self"&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/A&gt;&lt;BR /&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-in-sap-business-data-cloud-a-typical-machine-learning/ba-p/14206612" target="_self"&gt;Part 7&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;â€“&amp;nbsp;SAP Databricks in SAP Business Data Cloud - A Typical Machine Learning Workflow&lt;/SPAN&gt;&lt;/A&gt;&lt;/P&gt;&lt;H3 id="toc-hId-1865165611" id="toc-hId-1865187747"&gt;SAP Databricks in SAP Business Data Cloud&amp;nbsp;&lt;/H3&gt;&lt;P&gt;In part 2 of this series, we move beyond SQL analytics and explore how to harness the full power of &lt;STRONG&gt;Mosaic AI&lt;/STRONG&gt; and &lt;STRONG&gt;Agent &lt;/STRONG&gt;&lt;STRONG&gt;T&lt;/STRONG&gt;&lt;STRONG&gt;ools&lt;/STRONG&gt; within SAP Databricks. These capabilities enable developers and data scientists to rapidly prototype and deploy custom AI agent functions that interact with SAP Data Productsâ€”unlocking new levels of automation, personalization, and decision intelligence across the enterprise.&lt;/P&gt;&lt;H3 id="toc-hId-1865165611" id="toc-hId-1668674242"&gt;The Value of AI Powered by SAP Data Products&lt;/H3&gt;&lt;P&gt;Many companies envision AI as a simple linear path:&lt;/P&gt;&lt;H2 id="toc-hId-1343078018"&gt;&lt;STRONG&gt;Data &lt;/STRONG&gt;&lt;STRONG&gt;â†’&lt;/STRONG&gt;&lt;STRONG&gt; AI &lt;/STRONG&gt;&lt;STRONG&gt;â†’&lt;/STRONG&gt;&lt;STRONG&gt; Value&lt;/STRONG&gt;&lt;/H2&gt;&lt;P&gt;In reality, achieving business value from AI is far more complex. It starts with &lt;STRONG&gt;data selection, sourcing, and synthesis&lt;/STRONG&gt;, followed by rigorous &lt;STRONG&gt;data engineering&lt;/STRONG&gt; tasks such as cleaning, normalization, model training, evaluation, and hyperparameter tuning. The real challengeâ€”and where many initiatives stallâ€”is in &lt;STRONG&gt;operationalizing&lt;/STRONG&gt; these models: deploying them in production, monitoring performance, and continuously retraining to maintain accuracy.&lt;/P&gt;&lt;P&gt;This is where &lt;STRONG&gt;SAP Databricks and SAP Data Products&lt;/STRONG&gt; play a critical role. By combining SAPâ€™s semantically rich, governed data with Databricksâ€™ powerful analytics and machine learning platform, organizations can bridge the gap between experimentation and enterprise-scale valueâ€”making AI not just possible, but sustainable and impactful.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="What AI actually is.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/293977iDAC814AF09AD0C3D/image-size/large?v=v2&amp;amp;px=999" role="button" title="What AI actually is.png" alt="What AI actually is.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Letâ€™s explore several key features of SAP Databricks and dive into two practical use cases:&lt;BR /&gt;Analyzing a&amp;nbsp;&lt;STRONG&gt;Customer Data Product &lt;/STRONG&gt;with&lt;STRONG&gt; LLM&lt;/STRONG&gt;, and integrating &lt;STRONG&gt;SAP BTP Document AI&lt;/STRONG&gt; with the &lt;STRONG&gt;SAP Databricks Playground&lt;/STRONG&gt;.&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;H3 id="toc-hId-1275647232"&gt;AI/ML Features in SAP Databricks&lt;/H3&gt;&lt;UL&gt;&lt;LI&gt;&lt;A href="https://docs.databricks.com/sap/en/machine-learning#ai-playground" target="_blank" rel="noopener nofollow noreferrer"&gt;AI Playground&lt;/A&gt;&amp;nbsp;for testing generative AI models from your Databricks workspace. You can prompt, compare and adjust settings such as system prompt and inference parameters.&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://docs.databricks.com/sap/en/machine-learning#ai-functions" target="_blank" rel="noopener nofollow noreferrer"&gt;AI Functions&lt;/A&gt;&amp;nbsp;that you can use to apply AI, like text translation or sentiment analysis, on your data that is stored on Databricks.&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://docs.databricks.com/sap/en/machine-learning#ai-gateway" target="_blank" rel="noopener nofollow noreferrer"&gt;Mosaic AI Gateway&lt;/A&gt;&amp;nbsp;for governing and monitoring access to supported generative AI models and their associated model serving endpoints.&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://docs.databricks.com/sap/en/machine-learning#model-serving" target="_blank" rel="noopener nofollow noreferrer"&gt;Mosaic AI Model Serving&lt;/A&gt;&amp;nbsp;for deploying LLMs.&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://docs.databricks.com/sap/en/machine-learning#vector-serach" target="_blank" rel="noopener nofollow noreferrer"&gt;Mosaic AI Vector Search&lt;/A&gt;&amp;nbsp;provides a queryable vector database that stores embedding vectors and can be configured to automatically sync to your knowledge base.&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://docs.databricks.com/sap/en/machine-learning#lakehouse-monitoring" target="_blank" rel="noopener nofollow noreferrer"&gt;Lakehouse Monitoring&lt;/A&gt;&amp;nbsp;for data monitoring and tracking model prediction quality and drift using automatic payload logging with inference tables.&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://docs.databricks.com/sap/en/machine-learning#mlflow" target="_blank" rel="noopener nofollow noreferrer"&gt;Managed MLflow&lt;/A&gt;&amp;nbsp;for AI agent and ML model lifecycle.&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://docs.databricks.com/sap/en/machine-learning#agent-framework" target="_blank" rel="noopener nofollow noreferrer"&gt;Mosaic AI Agent Framework&lt;/A&gt;&amp;nbsp;for building and deploying production-quality agents like Retrieval Augmented Generation (RAG) applications.&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://docs.databricks.com/sap/en/machine-learning#agent-eval" target="_blank" rel="noopener nofollow noreferrer"&gt;Mosaic AI Agent Evaluation&lt;/A&gt;&amp;nbsp;for evaluating the quality, cost, and latency of generative AI applications, including RAG applications and chains.&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://docs.databricks.com/sap/en/machine-learning#automl" target="_blank" rel="noopener nofollow noreferrer"&gt;AutoML&lt;/A&gt;&amp;nbsp;to simplify the process of applying machine learning to your datasets.&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://docs.databricks.com/sap/en/machine-learning#finetuning" target="_blank" rel="noopener nofollow noreferrer"&gt;Foundation Model Fine-tuning&lt;/A&gt;&amp;nbsp;for customizing a foundation model using your own data to optimize its performance for your specific application.&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://docs.databricks.com/sap/en/machine-learning#unity-catalog" target="_blank" rel="noopener nofollow noreferrer"&gt;Unity Catalog&lt;/A&gt;&amp;nbsp;for managing AI assets, including models and experiments.&lt;BR /&gt;&lt;BR /&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;H4 id="toc-hId-1208216446"&gt;SAP Databricks Model Serving&lt;/H4&gt;&lt;P&gt;Model Serving provides a unified interface to deploy, govern, and query AI models for real-time and batch inference. Each model you serve is available as a REST API that you can integrate into your web or client application. Deploy custom models (including scikit-learn, XGBoost, PyTorch, Hugging Face transformer models) and foundational models hosted outside of Databricks.&lt;BR /&gt;&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="SAP Databricks Model Serving.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/293982iB770BBA479D06C23/image-size/large?v=v2&amp;amp;px=999" role="button" title="SAP Databricks Model Serving.png" alt="SAP Databricks Model Serving.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H4 id="toc-hId-1011702941"&gt;&lt;SPAN&gt;SAP Databricks Vector Search&lt;/SPAN&gt;&lt;/H4&gt;&lt;P&gt;Specifically designed for RAG applications, Vector Search delivers similarity search results, enriching LLM queries with context and domain knowledge, and improving accuracy and quality of results.&lt;/P&gt;&lt;P&gt;Create a Vector Search Index using the &lt;STRONG&gt;&lt;A href="https://api-docs.databricks.com/python/vector-search/databricks.vector_search.html" target="_blank" rel="noopener nofollow noreferrer"&gt;Python SDK&lt;/A&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Vector Search.png" style="width: 454px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/293984i8DE956AF1C13DCA4/image-size/large?v=v2&amp;amp;px=999" role="button" title="Vector Search.png" alt="Vector Search.png" /&gt;&lt;/span&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;H3 id="toc-hId-686106717"&gt;&lt;STRONG&gt;SAP Databricks Playground&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;Databricks Playground provides a built-in integration with your functions.&lt;BR /&gt;Itâ€™ll analyze which functions are available, and call them to properly answer your question&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Open the Playground&lt;/LI&gt;&lt;LI&gt;Select a &lt;STRONG&gt;model&lt;/STRONG&gt; (like Llama)&lt;/LI&gt;&lt;LI&gt;Add the &lt;STRONG&gt;tools/functions&lt;/STRONG&gt; you want your model to leverage&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;Ask a question, and the playground will do the magic for you!&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="SAP Databricks Playground.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/293985iD3B18B397C90A44B/image-size/large?v=v2&amp;amp;px=999" role="button" title="SAP Databricks Playground.png" alt="SAP Databricks Playground.png" /&gt;&lt;/span&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;H4 id="toc-hId-618675931"&gt;SAP Databricks AI Gateway&lt;/H4&gt;&lt;P&gt;Mosaic AI Gateway offers unified access to AI/ML models through a single standard query interface. This eliminates the need to maintain separate systems to manage AI traffic. Enterprises can effortlessly switch between foundation and custom models.&lt;/P&gt;&lt;P&gt;AI Gateway includes &lt;STRONG&gt;usage tracking &lt;/STRONG&gt;and &lt;STRONG&gt;guardrail activation&lt;/STRONG&gt; for secure storage, sharing, and management.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="SAP Databricks AI Gateway.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/293986i23C2418E43142EF1/image-size/large?v=v2&amp;amp;px=999" role="button" title="SAP Databricks AI Gateway.png" alt="SAP Databricks AI Gateway.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H4 id="toc-hId-422162426"&gt;Use Case #1: Analyze Customer Data Product with LLM&lt;/H4&gt;&lt;P&gt;Using generative AI, you can seamlessly analyze customer records from the Customer Data Product. The LLM output, consisting of business insights, is added as a new column. The enriched data is saved as a Delta table and published to the BDC Catalog.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;AI prompt is built using key fields like Customer Name, Country, Industry, Tax Number, City, Postal Code, and business flags&lt;/LI&gt;&lt;LI&gt;The&amp;nbsp;&lt;STRONG&gt;databricks-meta-llama-3-3-70b-instruct&lt;/STRONG&gt;&amp;nbsp;LLM generates concise business analyses per record,&lt;BR /&gt;identifying potential issues or opportunities&lt;/LI&gt;&lt;LI&gt;Responses are evaluated using&amp;nbsp;&lt;STRONG&gt;MLflow Databricks Agent&lt;/STRONG&gt;, ensuring clarity (e.g., checking the presence of customer names).&lt;/LI&gt;&lt;LI&gt;Enriched data is saved as a Delta Table:&amp;nbsp;&lt;STRONG&gt;default.customerllm&lt;/STRONG&gt;&lt;/LI&gt;&lt;LI&gt;The Delta Table is published as a Data Product to the BDC Catalog using the&amp;nbsp;&lt;A href="https://pypi.org/project/sap-bdc-connect-sdk/" target="_self" rel="nofollow noopener noreferrer"&gt;&lt;STRONG&gt;sap-bdc-connect-sdk&lt;/STRONG&gt;&lt;/A&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="1-Analyze Customer Data Product with LLM.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/293988iA0E034BF16401B79/image-size/large?v=v2&amp;amp;px=999" role="button" title="1-Analyze Customer Data Product with LLM.png" alt="1-Analyze Customer Data Product with LLM.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="2-Analyze Customer Data Product with LLM.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/294555iF3DDA153FF40A0E3/image-size/large?v=v2&amp;amp;px=999" role="button" title="2-Analyze Customer Data Product with LLM.png" alt="2-Analyze Customer Data Product with LLM.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="3-Analyze Customer Data Product with LLM.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/293990i6C112D23E775BC2E/image-size/large?v=v2&amp;amp;px=999" role="button" title="3-Analyze Customer Data Product with LLM.png" alt="3-Analyze Customer Data Product with LLM.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="4-Analyze Customer Data Product with LLM.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/293991i7D6DE20DD5F37B9E/image-size/large?v=v2&amp;amp;px=999" role="button" title="4-Analyze Customer Data Product with LLM.png" alt="4-Analyze Customer Data Product with LLM.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="5-Agent Evaluation.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/293992i2DA12E7EF54C845D/image-size/large?v=v2&amp;amp;px=999" role="button" title="5-Agent Evaluation.png" alt="5-Agent Evaluation.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H4 id="toc-hId-225648921"&gt;&lt;SPAN&gt;&amp;nbsp;&lt;BR /&gt;Use Case #2:&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;SAP Document AI &amp;amp; AI-Powered Validation in SAP Databricks&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/H4&gt;&lt;P&gt;You can build an end-to-end pipeline for processing documents â€“ with ingestion, validation, and audit review using&amp;nbsp;&lt;STRONG&gt;SAP Document AI, SAP Databricks&lt;/STRONG&gt;,&amp;nbsp;&lt;STRONG&gt;MLflow&lt;/STRONG&gt;, and&amp;nbsp;&lt;STRONG&gt;Claude AI&lt;/STRONG&gt;.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Integration with BTP Document AI via API&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Combine the result.json files within SAP Databricks Experiments and convert them into a parquet file. Upload as a delta table into the Unity Catalog.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Use of MLflow and Claude-3-7-sonnet model&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;Connectivity to SAP Databricks Playground with Tools/Functions&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;This allows business users to reduce manual review time through automation, improve data quality and compliance with AI-driven checks, and create summaries for finance teams.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Tool Status Checker.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/293994i7D44745CBEE9C943/image-size/large?v=v2&amp;amp;px=999" role="button" title="Tool Status Checker.png" alt="Tool Status Checker.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Tool AI Validation.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/294556i5072E6DD597E9A3B/image-size/large?v=v2&amp;amp;px=999" role="button" title="Tool AI Validation.png" alt="Tool AI Validation.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Agent Tools.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/293997iB681D6445205AC0E/image-size/large?v=v2&amp;amp;px=999" role="button" title="Agent Tools.png" alt="Agent Tools.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Playground Output.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/293998i834F955A838D2313/image-size/large?v=v2&amp;amp;px=999" role="button" title="Playground Output.png" alt="Playground Output.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;In part 2 of this series, we've gone beyond SQL analytics to explore how &lt;STRONG&gt;Mosaic AI&lt;/STRONG&gt;&lt;STRONG&gt;&amp;nbsp;tools&amp;nbsp;&lt;/STRONG&gt;within &lt;STRONG&gt;SAP Databricks&lt;/STRONG&gt; empower teams to build production-grade, enterprise-ready AI solutions. By combining the semantic richness of &lt;STRONG&gt;SAP Data Products&lt;/STRONG&gt; with Databricks' unified data and AI platform, organizations can move from isolated experiments to &lt;STRONG&gt;operationalized AI&lt;/STRONG&gt; that drives real business value.&lt;/P&gt;&lt;P&gt;As you continue building with &lt;STRONG&gt;SAP Business Data Cloud and SAP Databricks&lt;/STRONG&gt;, the focus shifts from experimentation to &lt;STRONG&gt;governance, reusability, and scale&lt;/STRONG&gt;. This is the foundation of the intelligent enterpriseâ€”where &lt;STRONG&gt;trusted SAP data fuels impactful AI&lt;/STRONG&gt;, delivered seamlessly and responsibly.&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-2/ba-p/14167025"/>
    <published>2025-07-30T15:18:04.435000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/using-jupyter-in-sap-business-application-studio-my-notes/ba-p/14167294</id>
    <title>Using Jupyter in SAP Business Application Studio â€“ my notes</title>
    <updated>2025-08-01T17:29:38.575000+02:00</updated>
    <author>
      <name>Vitaliy-R</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/183</uri>
    </author>
    <content>&lt;P&gt;In my previous blog post&amp;nbsp;&lt;A class="" href="https://community.sap.com/t5/technology-blog-posts-by-sap/using-python-in-sap-business-application-studio-my-notes/ba-p/14155516" target="_blank"&gt;Using Python in SAP Business Application Studio â€“ my notes&lt;/A&gt;, I focused on Python.&lt;/P&gt;&lt;P&gt;This blog post focuses on running Jupyter notebooks in SAP Business Application Studio&amp;nbsp;&lt;SPAN&gt;(referred to as "BAS" below)&lt;/SPAN&gt;.&amp;nbsp;I assume you're not an absolute beginner with SAP Business Application Studioâ€”or at least you're familiar with Visual Studio Code. If you need a general introduction to navigating, coding, and executing code in notebooks using the Jupyter extension, then check:&amp;nbsp;&lt;A href="https://code.visualstudio.com/docs/datascience/jupyter-notebooks#_create-or-open-a-jupyter-notebook" target="_blank" rel="nofollow noopener noreferrer"&gt;https://code.visualstudio.com/docs/datascience/jupyter-notebooks#_create-or-open-a-jupyter-notebook&lt;/A&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;For the examples in this post, Iâ€™ll be using the SAP Business Application Studio, which is available in the SAP BTP Trial environment.&lt;/P&gt;&lt;H2 id="toc-hId-1736107166"&gt;Creating a virtual environment with the Python extension&lt;/H2&gt;&lt;P&gt;As already discussed in&amp;nbsp;&lt;A class="" href="https://community.sap.com/t5/technology-blog-posts-by-sap/persisting-python-environment-when-using-jupyter-notebooks-in-sap-business/ba-p/13549863" target="_blank"&gt;Persisting Python environment when using Jupyter notebooks in SAP Business Application Studio&lt;/A&gt;, one of the first things you might want to do, when working with the Python code in BAS, is to create a virtual environment. There, we discussed how to use the `&lt;FONT face="terminal,monaco" color="#000080"&gt;python -m venv&lt;/FONT&gt;` command to create it. Now, let's use the extension commands.&lt;/P&gt;&lt;P class="lia-indent-padding-left-30px" style="padding-left : 30px;"&gt;&lt;STRONG&gt;&lt;span class="lia-unicode-emoji" title=":nerd_face:"&gt;ðŸ¤“&lt;/span&gt;&amp;nbsp;Pro tip:&lt;/STRONG&gt; if you want to understand what is happening when extension commands are executed, then:&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;Open the Output panel (eg, &lt;STRONG&gt;View -&amp;gt; Output&lt;/STRONG&gt; from the menu).&lt;/LI&gt;&lt;LI&gt;Choose the output for the &lt;STRONG&gt;Python&lt;/STRONG&gt; extension, and then in &lt;STRONG&gt;Settings&lt;/STRONG&gt;, set the level to &lt;STRONG&gt;Info&lt;/STRONG&gt; and&amp;nbsp;&lt;STRONG&gt;Set As Default&lt;/STRONG&gt;:&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_0-1753987148253.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/294808iA00DF79EEB2DED50/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_0-1753987148253.png" alt="VitaliyR_0-1753987148253.png" /&gt;&lt;/span&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;Alternatively, if the Python extension has not yet been loaded, then open the command palette and run the command &lt;FONT face="terminal,monaco" color="#000080"&gt;&amp;gt;Python:&amp;nbsp;Show Output&lt;/FONT&gt;:&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_1-1753988104700.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/294816i1A41D1412BCD4050/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_1-1753988104700.png" alt="VitaliyR_1-1753988104700.png" /&gt;&lt;/span&gt;&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;Go to the command palette and run the command `Python: Create Environment...`...&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_2-1753988411673.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/294818iA62DBE8BF3AA3AD7/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_2-1753988411673.png" alt="VitaliyR_2-1753988411673.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;..and select the &lt;STRONG&gt;Venv&lt;/STRONG&gt; option to create a virtual environment `.venv` (the name cannot be modified, at least in the current version of the Python extension):&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_3-1753988600004.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/294819i8339A533DC5C36F0/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_3-1753988600004.png" alt="VitaliyR_3-1753988600004.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;...selecting the required base Python run-time installation (like &lt;FONT face="terminal,monaco" color="#000080"&gt;~/.asdf-inst/shims/python&lt;/FONT&gt;&lt;span class="lia-unicode-emoji" title=":disappointed_face:"&gt;ðŸ˜ž&lt;/span&gt;&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_5-1753989146997.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/294824iB54EFA8DE6BA8E9E/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_5-1753989146997.png" alt="VitaliyR_5-1753989146997.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;If you have set output on the Info level for the Python extension, then you should see (interesting) details on how the environment was created:&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_6-1753990222424.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/294829iFFA2B57EB06A3F49/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_6-1753990222424.png" alt="VitaliyR_6-1753990222424.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;One additional note:&lt;/STRONG&gt; the &lt;FONT face="terminal,monaco" color="#000080"&gt;.gitignore&lt;/FONT&gt;&amp;nbsp;file has been automatically created for the virtual environment:&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_7-1753990886473.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/294835iB005A7F1F2F0BC57/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_7-1753990886473.png" alt="VitaliyR_7-1753990886473.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H2 id="toc-hId-1539593661"&gt;Select the Jupyter kernel&lt;/H2&gt;&lt;P&gt;To execute code in your Jupyter notebook, you need to select a &lt;A href="https://docs.jupyter.org/en/latest/glossary.html#term-kernel" target="_self" rel="nofollow noopener noreferrer"&gt;Jupyter kernel&lt;/A&gt;&amp;nbsp;that will be used to execute the code.&lt;/P&gt;&lt;P&gt;Click &lt;STRONG&gt;Select Kernel&lt;/STRONG&gt; and then on &lt;STRONG&gt;Python Environments...&lt;/STRONG&gt; category:&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_1-1754052597730.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/295259i23FB8803DB7F5A0E/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_1-1754052597730.png" alt="VitaliyR_1-1754052597730.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;...which will show you the list of Python environments that the Python extension found in your BAS dev space. Click on your virtual environment created for your project, like &lt;STRONG&gt;&lt;FONT face="terminal,monaco" color="#333399"&gt;.venv&lt;/FONT&gt;&lt;/STRONG&gt;:&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_2-1754052967744.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/295262iB31F805BAEF302B6/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_2-1754052967744.png" alt="VitaliyR_2-1754052967744.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H2 id="toc-hId-1343080156"&gt;Execute code in a Jupyter notebook&lt;/H2&gt;&lt;P&gt;During the first execution of your code, Jupyter will check if the &lt;A href="https://ipython.readthedocs.io/en/stable/install/kernel_install.html#installing-the-ipython-kernel" target="_self" rel="nofollow noopener noreferrer"&gt;IPython Kernel&lt;/A&gt; executable (the Python package &lt;FONT face="terminal,monaco" color="#333399"&gt;ipykernel&lt;/FONT&gt;) is installed in the selected Python environment. If not, it will automatically install it, as can be seen in the Jupyter extension's output, and then start it:&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_3-1754056502781.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/295283i783E48892ACD6FDB/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_3-1754056502781.png" alt="VitaliyR_3-1754056502781.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Once started, the Kernel exposes 5 ports for communication with your notebook, which can be seen in the run-time specification in a file &lt;FONT face="terminal,monaco" color="#333399"&gt;~/.local/share/jupyter/runtime/kernel-*.json&lt;/FONT&gt;. While this information should not be required in most of the basic scenarios, you might find it overwhelming that BAS displays pop-up messages for each port every time the kernel runtime starts:&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_4-1754057368976.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/295297i85CDC3874404AD19/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_4-1754057368976.png" alt="VitaliyR_4-1754057368976.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;While information about open ports is relevant when developing front-end applications using SAP Business Application Studio, it might be useless and overwhelming when working with Jupyter notebooks. So, click on the settings icon and turn off notifications from the "Exposing router ports" extension&lt;SPAN&gt;&amp;nbsp;or disable this extension completely:&lt;BR /&gt;&lt;/SPAN&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_5-1754057622486.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/295325i29F6A95A91A5DBDC/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_5-1754057622486.png" alt="VitaliyR_5-1754057622486.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H2 id="toc-hId-1146566651"&gt;Limiting listed Python environments&lt;/H2&gt;&lt;P&gt;You might also find it quite overwhelming to deal with the list of all the global Python locations when setting the kernel for a notebook. To reduce this list, select the command &lt;STRONG&gt;Jupyter: Filter Kernels&lt;/STRONG&gt; from the command palette:&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_0-1754060969070.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/295350i759F9877C1C4BB3D/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_0-1754060969070.png" alt="VitaliyR_0-1754060969070.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;...which opens the setting &lt;STRONG&gt;&lt;FONT face="terminal,monaco" color="#333399"&gt;jupyter.kernels.excludePythonEnvironments&lt;/FONT&gt;&lt;/STRONG&gt;. Switch to the &lt;STRONG&gt;Workspace&lt;/STRONG&gt; tab and click &lt;STRONG&gt;Add Item&lt;/STRONG&gt; to specify which Python environments to exclude from the list. You might want to add the following items to the list:&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;/bin/python3
/usr/bin/python3
~/.asdf-inst/shims/python3.13
~/.asdf-inst/shims/python3
~/.asdf-inst/shims/python&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_1-1754061618539.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/295359i09FFA8A1E47B3F08/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_1-1754061618539.png" alt="VitaliyR_1-1754061618539.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P class="lia-indent-padding-left-30px" style="padding-left : 30px;"&gt;&lt;STRONG&gt;&lt;span class="lia-unicode-emoji" title=":nerd_face:"&gt;ðŸ¤“&lt;/span&gt;Pro tip:&lt;/STRONG&gt; these values can also be edited in the project's file &lt;FONT face="terminal,monaco" color="#000080"&gt;.vscode/settings.json&lt;/FONT&gt;.&lt;/P&gt;&lt;pre class="lia-code-sample language-javascript"&gt;&lt;code&gt;{
    "jupyter.kernels.excludePythonEnvironments": [
        "/bin/python3",
        "/usr/bin/python3",
        "~/.asdf-inst/shims/python3.13",
        "~/.asdf-inst/shims/python3",
        "~/.asdf-inst/shims/python"
    ]
}&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;Next time you need to pick the kernel for a Jupyter notebook, you should see only your project's virtual Python environment:&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_2-1754062063240.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/295361iB582616218E20475/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_2-1754062063240.png" alt="VitaliyR_2-1754062063240.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H4 id="toc-hId-1208218584"&gt;You should be ready to work with your Jupyter notebooks in SAP Business Application Studio!&lt;/H4&gt;&lt;P&gt;Please share your tips in the comments!&lt;/P&gt;&lt;P&gt;------&lt;/P&gt;&lt;P&gt;-Vitaliy, aka&amp;nbsp;&lt;A href="https://bsky.app/profile/sygyzmundovych.bsky.social" target="_self" rel="nofollow noopener noreferrer"&gt;@Sygyzmundovych&lt;/A&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/using-jupyter-in-sap-business-application-studio-my-notes/ba-p/14167294"/>
    <published>2025-08-01T17:29:38.575000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/using-conda-forge-in-sap-business-application-studio-my-notes/ba-p/14169956</id>
    <title>Using conda-forge in SAP Business Application Studio â€“ my notes</title>
    <updated>2025-08-01T23:07:46.509000+02:00</updated>
    <author>
      <name>Vitaliy-R</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/183</uri>
    </author>
    <content>&lt;P&gt;&lt;SPAN&gt;In my previous blog posts, I focused on&amp;nbsp;&lt;/SPAN&gt;&lt;A class="" href="https://community.sap.com/t5/technology-blog-posts-by-sap/using-python-in-sap-business-application-studio-my-notes/ba-p/14155516" target="_blank"&gt;Using Python in SAP Business Application Studio&lt;/A&gt;&lt;SPAN&gt;&amp;nbsp;and&amp;nbsp;&lt;A class="" href="https://community.sap.com/t5/technology-blog-posts-by-sap/using-jupyter-in-sap-business-application-studio-my-notes/ba-p/14167294" target="_blank"&gt;Using Jupyter in SAP Business Application Studio&lt;/A&gt;&amp;nbsp;with Python's virtual environments. But SAP Business Application Studio (referred to as "BAS" below) also allows you to work with the &lt;A href="https://en.wikipedia.org/wiki/Conda_(package_manager)" target="_self" rel="nofollow noopener noreferrer"&gt;Conda&lt;/A&gt; environments:&lt;BR /&gt;&lt;/SPAN&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_0-1754071912864.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/295447i44DA5805AC4D52AD/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_0-1754071912864.png" alt="VitaliyR_0-1754071912864.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Usually, Conda is used when one needs binary package management, and installation with &lt;FONT face="terminal,monaco" color="#333399"&gt;pip&lt;/FONT&gt;&amp;nbsp;fails because of the build requirements that cannot be successfully completed with the tools and authorizations a developer has in their BAS dev space.&lt;/P&gt;&lt;P class="lia-indent-padding-left-30px" style="padding-left : 30px;"&gt;&lt;span class="lia-unicode-emoji" title=":backhand_index_pointing_right:"&gt;ðŸ‘‰&lt;/span&gt;&amp;nbsp;At the time of writing this post, using Conda might require additional commercial licenses from Anaconda, Inc. if downloading their tools and/or packages from their `&lt;FONT face="terminal,monaco"&gt;default&lt;/FONT&gt;` &lt;A href="https://en.wikipedia.org/wiki/Conda_(package_manager)#Channels" target="_self" rel="nofollow noopener noreferrer"&gt;channel&lt;/A&gt;. This might be a valid case for you, but this blog post covers only the use of the community-driven `conda-forge` channel and the minimum tools required to use it. To the best of my knowledge, I discuss only the technical setup here, but you might need to check any license implications for the dependencies of a project you are working on.&lt;/P&gt;&lt;H2 id="toc-hId-1736173353"&gt;Install miniforge&lt;/H2&gt;&lt;P&gt;Miniforge is a minimal installer for Conda and Mamba with the conda-forge channel set as the default (and only) channel.&lt;/P&gt;&lt;P&gt;Follow the installation steps, for example from&amp;nbsp;&lt;A href="https://github.com/conda-forge/miniforge?tab=readme-ov-file#unix-like-platforms-macos-linux--wsl" target="_blank" rel="noopener nofollow noreferrer"&gt;https://github.com/conda-forge/miniforge?tab=readme-ov-file#unix-like-platforms-macos-linux--wsl&lt;/A&gt;&lt;/P&gt;&lt;P&gt;Download and run the installation script.&lt;/P&gt;&lt;pre class="lia-code-sample language-bash"&gt;&lt;code&gt;curl --location --remote-name --output-dir ~/tmp "https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh"

bash ~/tmp/Miniforge3-$(uname)-$(uname -m).sh -b&lt;/code&gt;&lt;/pre&gt;&lt;H2 id="toc-hId-1539659848"&gt;&amp;nbsp;Configure conda&lt;/H2&gt;&lt;P&gt;Activate conda and its &lt;FONT face="terminal,monaco" color="#333399"&gt;base&lt;/FONT&gt;&amp;nbsp;environment.&lt;/P&gt;&lt;pre class="lia-code-sample language-bash"&gt;&lt;code&gt;eval "$(/home/user/miniforge3/bin/conda shell.$(basename "${SHELL}") hook)"&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;Initialize conda (it will add the shell's integration).&lt;/P&gt;&lt;pre class="lia-code-sample language-bash"&gt;&lt;code&gt;conda init&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;To prevent the conda's &lt;FONT face="terminal,monaco" color="#333399"&gt;base&lt;/FONT&gt;&amp;nbsp;environment from being activated on startup (because it will be taken care of by the Python extension), run the following command:&lt;/P&gt;&lt;pre class="lia-code-sample language-bash"&gt;&lt;code&gt;conda config --set auto_activate_base false&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;For changes to take effect, close and re-open your current shell.&lt;/P&gt;&lt;P&gt;Update conda's &lt;FONT face="terminal,monaco" color="#333399"&gt;base&lt;/FONT&gt; environment to the latest packages from the community-maintained &lt;FONT face="terminal,monaco" color="#333399"&gt;conda-forge&lt;/FONT&gt;&amp;nbsp;channel.&lt;/P&gt;&lt;pre class="lia-code-sample language-bash"&gt;&lt;code&gt;conda update -n base --all --yes&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;You can also use the faster &lt;FONT face="terminal,monaco" color="#333399"&gt;mamba&lt;/FONT&gt;&amp;nbsp;command instead of &lt;FONT face="terminal,monaco" color="#333399"&gt;conda&lt;/FONT&gt;. Both are parts of the Miniforge installation.&lt;/P&gt;&lt;H2 id="toc-hId-1343146343"&gt;Point Python extension to your Conda executable&lt;/H2&gt;&lt;P&gt;In &lt;STRONG&gt;Terminal&lt;/STRONG&gt;, check the location of the &lt;FONT face="terminal,monaco" color="#333399"&gt;conda&lt;/FONT&gt;&amp;nbsp;command, which should be something like &lt;FONT face="terminal,monaco" color="#333399"&gt;/home/user/miniforge3/condabin/conda&lt;/FONT&gt;:&lt;/P&gt;&lt;pre class="lia-code-sample language-bash"&gt;&lt;code&gt;which conda&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;In &lt;STRONG&gt;Settings&lt;/STRONG&gt;, open &lt;FONT face="terminal,monaco" color="#333399"&gt;python.condaPath&lt;/FONT&gt;&amp;nbsp;and switch to the Remote tab. Input the value.&lt;/P&gt;&lt;H2 id="toc-hId-1146632838"&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_1-1754077236838.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/295487i523B13F658B4CF31/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_1-1754077236838.png" alt="VitaliyR_1-1754077236838.png" /&gt;&lt;/span&gt;&lt;/H2&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;H2 id="toc-hId-950119333"&gt;Create a &lt;FONT face="terminal,monaco" color="#333399"&gt;.conda&lt;/FONT&gt;&amp;nbsp;environment in your project&lt;/H2&gt;&lt;P&gt;Open your project folder in BAS.&lt;/P&gt;&lt;P&gt;From the command palette, start the command&amp;nbsp;&lt;STRONG&gt;Python: Create Environment...&lt;/STRONG&gt;, then &lt;STRONG&gt;Conda&lt;/STRONG&gt;&amp;nbsp;environment type...&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_0-1754071912864.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/295447i44DA5805AC4D52AD/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_0-1754071912864.png" alt="VitaliyR_0-1754071912864.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;...and select the required Python version from the list:&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_2-1754078227143.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/295497i93854803DCAD9C13/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_2-1754078227143.png" alt="VitaliyR_2-1754078227143.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;You should see the message that the environment is created and set as the active one for the project.&lt;/P&gt;&lt;P&gt;Open a new Terminal session, and you should see the &lt;FONT face="terminal,monaco" color="#333399"&gt;.conda&lt;/FONT&gt;&amp;nbsp;environment set.&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_3-1754078590314.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/295499iD3EBC13944398F9A/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_3-1754078590314.png" alt="VitaliyR_3-1754078590314.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;You can check the list of environments with the command:&lt;/P&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;pre class="lia-code-sample language-bash"&gt;&lt;code&gt;conda env list&lt;/code&gt;&lt;/pre&gt;&lt;H2 id="toc-hId-753605828"&gt;Install IPyKernel in your &lt;FONT face="terminal,monaco" color="#333399"&gt;.conda&lt;/FONT&gt; environment&lt;/H2&gt;&lt;P&gt;Unlike in a "venv" virtual environment, the Python extension might not automatically install IPyKernel in a conda environment. You need to install it manually.&lt;/P&gt;&lt;P&gt;In &lt;STRONG&gt;Terminal&lt;/STRONG&gt;, with the &lt;FONT face="terminal,monaco" color="#333399"&gt;.conda&lt;/FONT&gt;&amp;nbsp;environment set in the command line, run the command:&lt;/P&gt;&lt;pre class="lia-code-sample language-bash"&gt;&lt;code&gt;conda install ipykernel --yes&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;Check with the command&lt;/P&gt;&lt;pre class="lia-code-sample language-bash"&gt;&lt;code&gt;conda list ipykernel&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_4-1754079013971.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/295500i6624BBBD3246EFC5/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_4-1754079013971.png" alt="VitaliyR_4-1754079013971.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H2 id="toc-hId-557092323"&gt;Select the &lt;FONT face="terminal,monaco" color="#333399"&gt;.conda&lt;/FONT&gt; env in your notebook&lt;/H2&gt;&lt;P&gt;Now, you can open your project's Jupyter notebook, select the &lt;FONT face="terminal,monaco" color="#333399"&gt;.conda&lt;/FONT&gt; environment, and execute the code &lt;span class="lia-unicode-emoji" title=":nerd_face:"&gt;ðŸ¤“&lt;/span&gt;&lt;/P&gt;&lt;H2 id="toc-hId-360578818"&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="VitaliyR_5-1754079340647.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/295501iFC432ED27A836EC4/image-size/large?v=v2&amp;amp;px=999" role="button" title="VitaliyR_5-1754079340647.png" alt="VitaliyR_5-1754079340647.png" /&gt;&lt;/span&gt;&lt;/H2&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;H4 id="toc-hId-1208218584" id="toc-hId-422230751"&gt;You should be ready to work with conda-forge in SAP Business Application Studio now!&amp;nbsp;&lt;/H4&gt;&lt;P&gt;Please share your tips in the comments!&lt;/P&gt;&lt;H4 id="toc-hId-1208218584" id="toc-hId-225717246"&gt;In my other blog posts, I focused on:&lt;BR /&gt;&lt;span class="lia-unicode-emoji" title=":snake:"&gt;ðŸ&lt;/span&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/using-python-in-sap-business-application-studio-my-notes/ba-p/14155516" target="_self"&gt;Using Python in SAP Business Application Studio&lt;/A&gt;, and&lt;BR /&gt;ðŸª&amp;nbsp;&lt;A class="" href="https://community.sap.com/t5/technology-blog-posts-by-sap/using-jupyter-in-sap-business-application-studio-my-notes/ba-p/14167294" target="_blank"&gt;Using Jupyter in SAP Business Application Studio&lt;/A&gt;.&lt;/H4&gt;&lt;P&gt;------&lt;/P&gt;&lt;P&gt;-Vitaliy, aka&amp;nbsp;&lt;A href="https://bsky.app/profile/sygyzmundovych.bsky.social" target="_self" rel="nofollow noopener noreferrer"&gt;@Sygyzmundovych&lt;/A&gt;&amp;nbsp;&lt;/P&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/using-conda-forge-in-sap-business-application-studio-my-notes/ba-p/14169956"/>
    <published>2025-08-01T23:07:46.509000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/building-collaborative-microservices-in-python-with-fastapi-echo-amp/ba-p/14170025</id>
    <title>Building Collaborative Microservices in Python with FastAPI: Echo &amp; Reverse Agents (Beginner -Part1)</title>
    <updated>2025-08-02T10:18:39.981000+02:00</updated>
    <author>
      <name>Yogananda</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/75</uri>
    </author>
    <content>&lt;P&gt;Microservices encourage us to build distributed systems consisting of small, specialized components that communicate seamlessly. In this post, letâ€™s explore a simple yet illustrative example:&amp;nbsp;Agent-A and Agent-B, two FastAPI-based Python microservices that interact in real-time.&lt;/P&gt;&lt;P&gt;We'll see how&amp;nbsp;Agent-A receives a request, calls Agent-B, and returns a transformed responseâ€”demonstrating the foundations of service-to-service HTTP communication.&lt;/P&gt;&lt;H2 id="toc-hId-1736820012"&gt;The Objective&lt;/H2&gt;&lt;P&gt;We want to create:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Agent-B&lt;/STRONG&gt;:&amp;nbsp;A service that reverses text.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Agent-A&lt;/STRONG&gt;:&amp;nbsp;A service that takes a string, sends it to Agent-B for reversal, and returns the reversed string as a response.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;Both services are built with&amp;nbsp;&lt;A href="https://fastapi.tiangolo.com/" target="_blank" rel="noopener nofollow noreferrer"&gt;FastAPI&lt;/A&gt;&amp;nbsp;and use&amp;nbsp;&lt;A href="https://pydantic-docs.helpmanual.io/" target="_blank" rel="noopener nofollow noreferrer"&gt;Pydantic&lt;/A&gt;&amp;nbsp;for data validation.&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="2025-08-02_10-23-41.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/295526i46C7EF2439E02129/image-size/large?v=v2&amp;amp;px=999" role="button" title="2025-08-02_10-23-41.png" alt="2025-08-02_10-23-41.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H2 id="toc-hId-1540306507"&gt;Meet Agent-B: The Reverse Service&lt;/H2&gt;&lt;P&gt;&lt;SPAN&gt;Let's begin by looking at&amp;nbsp;&lt;/SPAN&gt;agent_b.py&lt;SPAN&gt;. Itâ€™s a minimal service that exposes one endpoint that takes a string and returns it reversed.&lt;/SPAN&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from fastapi import FastAPI
from pydantic import BaseModel
import os

PORT = int(os.getenv("AGENT_B_PORT", 8001))
app = FastAPI(title="Agent-B")

class ReverseRequest(BaseModel):
    text: str

class ReverseResponse(BaseModel):
    source: str
    reversed: str

@app.post("/reverse", response_model=ReverseResponse)
def reverse(req: ReverseRequest):
    return ReverseResponse(source="Agent-B", reversed=req.text[::-1])&lt;/code&gt;&lt;/pre&gt;&lt;H2 id="toc-hId-1343793002"&gt;Meet Agent-A: The Forwarding Echo Service&lt;/H2&gt;&lt;P&gt;Now, letâ€™s look at&amp;nbsp;&lt;STRONG&gt;agent_a.py&lt;/STRONG&gt;. While itâ€™s called an "echo" service, it doesnâ€™t simply echo back input. Instead, it&amp;nbsp;&lt;FONT color="#FF0000"&gt;&lt;STRONG&gt;forwards the input to Agent-B&lt;/STRONG&gt;&lt;/FONT&gt;, receives the reversed string, and returns that. This demonstrates microservice orchestration.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;agent_a.py&lt;/STRONG&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from fastapi import FastAPI
from pydantic import BaseModel
import httpx
import os

PORT = int(os.getenv("AGENT_A_PORT", 8000))
B_URL = os.getenv("AGENT_B_URL", "http://localhost:8001")
app = FastAPI(title="Agent-A")

class EchoRequest(BaseModel):
    text: str

class EchoResponse(BaseModel):
    source: str
    echoed: str

@app.post("/echo", response_model=EchoResponse)
async def echo(req: EchoRequest):
    # Forward the text to Agent-B
    async with httpx.AsyncClient() as client:
        r = await client.post(f"{B_URL}/reverse", json={"text": req.text})
        r.raise_for_status()
        reversed_text = r.json()["reversed"]
    # Respond with what Agent-B returned
    return EchoResponse(source="Agent-A", echoed=reversed_text)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;HR /&gt;&lt;H2 id="toc-hId-1147279497"&gt;Running Both Agents&lt;/H2&gt;&lt;OL&gt;&lt;LI&gt;&lt;P&gt;Start Agent-B:&lt;/P&gt;&lt;PRE&gt;uvicorn agent_b:app --port 8001&lt;/PRE&gt;&lt;/LI&gt;&lt;LI&gt;&lt;P&gt;Start Agent-A:&lt;/P&gt;&lt;PRE&gt;uvicorn agent_a:app --port 8000&lt;/PRE&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;/LI&gt;&lt;/OL&gt;&lt;H2 id="toc-hId-950765992"&gt;Testing the System&lt;/H2&gt;&lt;P&gt;Letâ€™s see the microservices in action with&amp;nbsp;curl:&lt;/P&gt;&lt;PRE&gt;curl -X POST http://localhost:8000/echo \
     -H "Content-Type: application/json" \
     -d '{"text":"hello A2A"}'&lt;/PRE&gt;&lt;P&gt;Response:&lt;/P&gt;&lt;PRE&gt;{
  "source": "Agent-A",
  "echoed": "A2A olleh"
}&lt;/PRE&gt;&lt;P&gt;&lt;STRONG&gt;What happened?&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;The request hits Agent-Aâ€™s&amp;nbsp;/echo.&lt;/LI&gt;&lt;LI&gt;Agent-A calls Agent-Bâ€™s&amp;nbsp;/reverse&amp;nbsp;with the payload.&lt;/LI&gt;&lt;LI&gt;Agent-B reverses the string.&lt;/LI&gt;&lt;LI&gt;Agent-A returns Agent-Bâ€™s output* as its own&amp;nbsp;"echoed"&amp;nbsp;field.&lt;/LI&gt;&lt;/UL&gt;&lt;H2 id="toc-hId-754252487"&gt;How to Extend This Example&lt;/H2&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Add Logging/Tracing:&lt;/STRONG&gt;&amp;nbsp;See the flow end-to-end.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Synchronous vs Asynchronous calls:&lt;/STRONG&gt;&amp;nbsp;Both are possible; here&amp;nbsp;httpx&amp;nbsp;with&amp;nbsp;async&amp;nbsp;enables efficient concurrency.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Error Handling&lt;/STRONG&gt;:&amp;nbsp;Add more robust logic in production.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Authentication&lt;/STRONG&gt;:&amp;nbsp;Secure the endpoints as needed.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;More Agents:&lt;/STRONG&gt;&amp;nbsp;Build a family of collaborating services!&lt;/LI&gt;&lt;/UL&gt;&lt;H2 id="toc-hId-557738982"&gt;Conclusion&lt;/H2&gt;&lt;P&gt;With just a few lines of Python and FastAPI, we built two collaborating microservices with clear responsibilities and simple interaction via HTTP. This pattern underpins many real-world distributed architectures, making it a great learning foundation for building maintainable and scalable systems.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Try extending these agents on your own use case!&lt;/STRONG&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/building-collaborative-microservices-in-python-with-fastapi-echo-amp/ba-p/14170025"/>
    <published>2025-08-02T10:18:39.981000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/artificial-intelligence-blogs-posts/hands-on-tutorial-sap-databricks/ba-p/14156999</id>
    <title>Hands-on Tutorial: SAP Databricks</title>
    <updated>2025-08-04T09:17:31.648000+02:00</updated>
    <author>
      <name>AndreasForster</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/14188</uri>
    </author>
    <content>&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="000 logos white.png" style="width: 584px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291532iF242B7265DB0E43A/image-size/large?v=v2&amp;amp;px=999" role="button" title="000 logos white.png" alt="000 logos white.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;With &lt;A href="https://www.sap.com/products/data-cloud/databricks.html" target="_blank" rel="noopener noreferrer"&gt;SAP Databricks&lt;/A&gt; we now have a dedicated environment for Data Scientists within the SAP Business Data Cloud. This blog gives a practical introduction to this bespoke Databricks edition by implementing a bare bones demand forecast.&amp;nbsp;SAP Databricks includes important functionality beyond what is explained in this entry-level tutorial., for example data sharing, experiment tracking or AutoML.&lt;BR /&gt;&lt;BR /&gt;The integration of SAP Databricks is adding a new option to the SAP Business Technology Platform (BTP), giving customers more choice when creating custom extensions or applications. Existing components on the BTP, which you might already be familiar with, remain very relevant and strategic, such as:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;A href="https://www.sap.com/suisse/products/artificial-intelligence/ai-core.html" target="_self" rel="noopener noreferrer"&gt;SAP AI Core&lt;/A&gt;, which provides for example access to a &lt;A href="https://me.sap.com/notes/3437766" target="_self" rel="noopener noreferrer"&gt;long list of Large Language Models&lt;/A&gt; through its &lt;A href="https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/generative-ai-hub-in-sap-ai-core-7db524ee75e74bf8b50c167951fe34a5" target="_self" rel="noopener noreferrer"&gt;Generative AI Hub.&lt;/A&gt;&amp;nbsp;Some of the models are even hosted on SAP's own physical infrastructure, giving increased security (currently Mistral and Aleph Alpha).&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://www.sap.com/products/artificial-intelligence/ai-foundation-os/document-ai.html" target="_self" rel="noopener noreferrer"&gt;SAP Document AI&lt;/A&gt;, which extracts information from documents such as PDFs, Excel, images, ...&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://www.sap.com/products/data-cloud/hana.html" target="_self" rel="noopener noreferrer"&gt;SAP HANA Cloud&lt;/A&gt; and &lt;A href="https://www.sap.com/products/data-cloud/datasphere.html" target="_self" rel="noopener noreferrer"&gt;SAP Datasphere&lt;/A&gt;, with their built-in multi-model capabilities, ie Machine Learning, embeddings generation for Text, Vector engine, Graph engine, Geospatial&amp;nbsp;&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://www.sap.com/products/artificial-intelligence/joule-studio.html" target="_self" rel="noopener noreferrer"&gt;SAP Build, Joule Studio&lt;/A&gt; for adding new skills to our digital assistant Joule, and the ability to create new AI agents on the roadmap. This &lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/how-sap-s-ai-agent-architecture-enables-unprecedented-automation-and/ba-p/14158296" target="_self"&gt;blog&lt;/A&gt;&amp;nbsp;(series) on SAP's AI Agent Architecture by our CTO and Chief AI Officer&amp;nbsp;Philipp Herzig&amp;nbsp;&lt;SPAN&gt;gives an excellent overview of the wider picture.&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;SAP Databricks can also be used as development environment for the above components. &lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/unlocking-sap-ai-foundation-capabilities-in-sap-databricks-a-technical-deep/ba-p/14162430" target="_self"&gt;This blog&lt;/A&gt; by &lt;a href="https://community.sap.com/t5/user/viewprofilepage/user-id/434167"&gt;@san_tran&lt;/a&gt;&amp;nbsp;for instance shows a SAP Databricks project that integrates with Large Language Models on SAP AI Core.&amp;nbsp;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;And remember to check on the AI/ML functionality that has already been built into SAP standard applications, or what is in the pipeline to be released. Both released and planned functionality is shown in the &lt;A href="https://roadmaps.sap.com/board?FT=AI&amp;amp;FT=GEN_AI&amp;amp;range=FIRST-LAST" target="_self" rel="noopener noreferrer"&gt;Roadmap Explorer&lt;/A&gt;.&lt;/P&gt;&lt;P&gt;&lt;FONT color="#FF0000"&gt;Note: All data, code and images that are used in this blog can be downloaded from this repository: &lt;A href="https://github.com/SAP-samples/mee-samples/tree/main/Hands-on%20Tutorial%20SAP%20Databricks" target="_self" rel="nofollow noopener noreferrer"&gt;Hands-on Tutorial SAP Databricks&lt;/A&gt;. In that repository you find the individual files but also the whole project exported as Databricks Archive.&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;Before we get going, big thanks to Stojan Maleschlijski for all the great collaboration, including exploring the SAP Databricks capabilities together!&amp;nbsp;&lt;a href="https://community.sap.com/t5/user/viewprofilepage/user-id/39047"&gt;@stojanm&lt;/a&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="toc-hId-1606077867"&gt;Table of contents&lt;/H1&gt;&lt;UL&gt;&lt;LI&gt;&lt;A href="#background" target="_self" rel="nofollow noopener noreferrer"&gt;Use Case&amp;nbsp;&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="#architecture" target="_self" rel="nofollow noopener noreferrer"&gt;Architecture&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="#prerequisites" target="_self" rel="nofollow noopener noreferrer"&gt;Prerequisites&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="#uploaddata" target="_self" rel="nofollow noopener noreferrer"&gt;Upload data&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="#eda" target="_self" rel="nofollow noopener noreferrer"&gt;Explore the data&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="#forecast" target="_self" rel="nofollow noopener noreferrer"&gt;Create a forecast with SAP Databricks&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="#schedule" target="_self" rel="nofollow noopener noreferrer"&gt;Schedule a forecast with SAP Databricks&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="#goodtoknow" target="_self" rel="nofollow noopener noreferrer"&gt;Good to know&lt;/A&gt;&lt;UL&gt;&lt;LI&gt;Base file&lt;/LI&gt;&lt;LI&gt;Lineage &amp;amp; Table usage insights&lt;/LI&gt;&lt;LI&gt;Visuals&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="#summary" target="_self" rel="nofollow noopener noreferrer"&gt;Summary&lt;/A&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="background" id="toc-hId-1409564362"&gt;Use Case&lt;/H1&gt;&lt;P&gt;This blog aims to give a first introduction for carrying out a Machine Learning project on SAP Databricks. Trying to simulate a demand forecast, we will predict how many nights people spend in a hotel in Switzerland. We use some granular data that is kindly shared by the &lt;A href="https://opendata.swiss/de/dataset/hotellerie-ankunfte-und-logiernachte-der-geoffneten-betriebe-nach-jahr-monat-tourismusregion-un46/resource/dc676b65-69dc-437c-b0f0-4ef703be9ae2" target="_self" rel="nofollow noopener noreferrer"&gt;Swiss Statistics department&lt;/A&gt;.&lt;/P&gt;&lt;P&gt;A Data Scientist (you) creates a monthly forecast and sets up a schedule for the forecast to be updated every month.&amp;nbsp; This data would then become part of a business process, maybe you want to provide the data to business users in a dashboard.&amp;nbsp;This tutorial however focusses solely on creating and scheduling the forecast in SAP Databricks.&lt;/P&gt;&lt;P&gt;It's a simple time-series forecasting example, but the concept is still extremely relevant for so many different business requirements. Instead of forecasting how many visitors are staying overnight, you might need to forecast your sales quantities, your cash flow or even for topics that don't come immediately to mind. One customer for instance is using time-series forecasting to improve the data quality of external data that is loaded into the system. If a new value is outside the predicted range, there might be a data quality issue and IT can follow up, whether the data is correct or whether indeed a data quality issue cropped up.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="architecture" id="toc-hId-1213050857"&gt;Architecture&lt;/H1&gt;&lt;P&gt;In a &lt;STRONG&gt;productive&lt;/STRONG&gt; system a typical architecture and data flow would look like this.&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;A Data Product is pushed into the Business Data Cloud Object Store&lt;/LI&gt;&lt;LI&gt;This Data Product is shared with SAP Databricks&lt;/LI&gt;&lt;LI&gt;SAP Databricks creates and saves a forecast as DeltaTable into the Object Store&lt;/LI&gt;&lt;LI&gt;This table is registered as Custom Data Product in the Business Data Cloud&lt;/LI&gt;&lt;LI&gt;SAP Datasphere installs the Custom Data Product, making it accessible for further data modelling and visualisation in SAP Analytics Cloud&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="100 architecture prod.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291617iCD8EC574E5DB872A/image-size/large?v=v2&amp;amp;px=999" role="button" title="100 architecture prod.png" alt="100 architecture prod.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;The architecture of this hands-on exercise is focused purely on getting some familiarity with SAP Databricks. The only interface we will be using is SAP Databricks. We use it to upload the data and to go through the steps of a Machine Learning project.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="114 hands-on architecture.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291641i1B8E5C353594A19C/image-size/large?v=v2&amp;amp;px=999" role="button" title="114 hands-on architecture.png" alt="114 hands-on architecture.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="prerequisites" id="toc-hId-1016537352"&gt;Prerequisites&lt;/H1&gt;&lt;P&gt;Getting started is pretty easy.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;You will just need to have access to an instance of SAP Databricks. This could be the &lt;A href="https://www.sap.com/products/data-cloud/trial.html" target="_self" rel="noopener noreferrer"&gt;SAP Business Data Cloud trial&lt;/A&gt;.&lt;/LI&gt;&lt;LI&gt;Some familiarity with SQL and especially Python would be very useful.&amp;nbsp;&lt;/LI&gt;&lt;LI&gt;And maybe just some curiosity to try something new.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="uploaddata" id="toc-hId-820023847"&gt;Upload data&lt;/H1&gt;&lt;P&gt;Start by bringing the historic data, on which we want to train a model, into the system. Upload the dataset &lt;A href="https://github.com/SAP-samples/mee-samples/blob/main/Hands-on%20Tutorial%20SAP%20Databricks/OVERNIGHTSTAYS.csv" target="_self" rel="nofollow noopener noreferrer"&gt;OVERNIGHTSTAYS.csv&lt;/A&gt; with the Graphical User Interface of SAP Databricks. Follow the steps shown in this screencam. In case that you are working with the trial, then you need to change the Catalog drop-down as shown to "workspace".&lt;/P&gt;&lt;P&gt;&lt;EM&gt;HINT: In case the upload fails with the message "Table with same name already exists", then another user has already created the table in this shared environment. Due to access rights you won't be able to see that other user's table. Just change the table name&amp;nbsp; in your upload to something unique and remember to use this name when accessing the table further on.&lt;BR /&gt;&lt;/EM&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="010 data upload.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291462i85AFDC42927C8AE0/image-size/large?v=v2&amp;amp;px=999" role="button" title="010 data upload.gif" alt="010 data upload.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;The data is uploaded. it is showing in the Catalog as (Delta)Table. Physically it is stored in the Object Store. The table's Overview tab lists the column names and their data types:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="200 table.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291821i2442B47235562A8F/image-size/large?v=v2&amp;amp;px=999" role="button" title="200 table.png" alt="200 table.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Move to the table's "Sample Data" tab to see some of the uploaded rows.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="210 sample data.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291822i8812DF688B48DC5A/image-size/large?v=v2&amp;amp;px=999" role="button" title="210 sample data.png" alt="210 sample data.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;The first lines shows that in January 2022 there were 392.805 nights spent by Swiss residents in the area of GraubÃ¼nden. In case you are not that familiar with Swiss geography, GraubÃ¼nden is the largest Canton in Switzerland, it's in the Alps and includes beautiful places like St. Moritz and Davos.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="eda" id="toc-hId-623510342"&gt;Explore the data&amp;nbsp;&lt;/H1&gt;&lt;P&gt;You have two options to explore the data in SAP Databricks.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Either query the DeltaTable directly with SQL&lt;/LI&gt;&lt;LI&gt;Or use a Notebook, in which you can use both SQL or Python&lt;/LI&gt;&lt;/UL&gt;&lt;H2 id="toc-hId-556079556"&gt;&lt;FONT color="#000000"&gt;Data exploration with the SQL Editor&lt;/FONT&gt;&lt;/H2&gt;&lt;P&gt;Let's look closer into the data. You can start with some SQL statements. You can go into the SQL Editor and type in your own syntax. The screencam shows two simple examples. We learn for instance that we have data from January 2022 to May 2025 to work with.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;SELECT * from overnightstays&lt;/LI&gt;&lt;LI&gt;SELECT min(MONTH), max(month) from overnightstays&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="1 SQL Editor.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291823i07AAB646DC970612/image-size/large?v=v2&amp;amp;px=999" role="button" title="1 SQL Editor.gif" alt="1 SQL Editor.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Or describe what you are looking for and have SAP Databrick's write the SQL for you! Click that small red-ish star icon to toggle on the &lt;A href="https://www.databricks.com/product/databricks-assistant" target="_self" rel="nofollow noopener noreferrer"&gt;Databricks Assistant&lt;/A&gt;.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="230 assistant toggle.png" style="width: 793px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291825i58D63393E550F4C2/image-size/large?v=v2&amp;amp;px=999" role="button" title="230 assistant toggle.png" alt="230 assistant toggle.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Just try out what you are interested in. I was wondering for example:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;which regions are in overnightstays?&lt;/LI&gt;&lt;LI&gt;use table overnightstays to determine how many overnight stays were they by region, sort the results&lt;/LI&gt;&lt;LI&gt;use table overnightstays to summarise overnightstays by countryofresidence&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="2 sql assistant.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291827i03309ED58F155C9F/image-size/large?v=v2&amp;amp;px=999" role="button" title="2 sql assistant.gif" alt="2 sql assistant.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;It turns out that most overnight stays in Switzerland are by Swiss residents, followed by German residents. Maybe surprisingly residents from the United States are a close third.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="240 overnight by countryofresidence.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291828iAF52C2350418D6FE/image-size/large?v=v2&amp;amp;px=999" role="button" title="240 overnight by countryofresidence.png" alt="240 overnight by countryofresidence.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-359566051"&gt;&lt;FONT color="#000000"&gt;Data exploration with a Notebook&lt;/FONT&gt;&lt;/H2&gt;&lt;P&gt;&lt;FONT color="#000000"&gt;Now continue the data exploration in a Notebook. &lt;A href="https://docs.databricks.com/aws/en/notebooks/" target="_self" rel="nofollow noopener noreferrer"&gt;Databricks Notebooks&lt;/A&gt; are quite special in that they can contain more than one scripting language. SAP Databricks supports both SQL as well as Python. The Notebook created in this section can also be downloaded from the &lt;A href="https://github.com/SAP-samples/mee-samples/tree/main/Hands-on%20Tutorial%20SAP%20Databricks" target="_self" rel="nofollow noopener noreferrer"&gt;repository&lt;/A&gt;.&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT color="#000000"&gt;Begin by creating a folder in which we will save our Notebooks. Go into the "Workspace" section, which is where such files are kept together. Within your user's workspace create a folder called "Demand forecast".&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT color="#000000"&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="250 create folder.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291832iC5DC43480B5680ED/image-size/large?v=v2&amp;amp;px=999" role="button" title="250 create folder.gif" alt="250 create folder.gif" /&gt;&lt;/span&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;FONT color="#000000"&gt;In that new folder create a first Notebook as shown in the next screencam. Rename it to "010 Data exploration". And add a short header in Markdown at the top. Markdown is a common way to add comments and context to the code in a Notebook. Adding "#" character at the beginning of a line formats the text as top-level heading. Two&amp;nbsp;"#" characters make it a second-level heading, and so on. Without any&amp;nbsp;"#" character the following text is formatted just normally as plain text.&amp;nbsp;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT color="#000000"&gt;To add the Markdown, you need to change the cell's language selector to "Markdown" as shown in the screencam. Enter your text. When done, click outside the cell and the formatting kicks in.&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT color="#000000"&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="260 create notebook.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291833i095F807AD2D1B795/image-size/large?v=v2&amp;amp;px=999" role="button" title="260 create notebook.gif" alt="260 create notebook.gif" /&gt;&lt;/span&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Since the cells can also execute SQL code, add the most recent SQL statement from the SQL Editor.&lt;/P&gt;&lt;pre class="lia-code-sample language-sql"&gt;&lt;code&gt;SELECT
  COUNTRYOFRESIDENCE,
  SUM(OVERNIGHTSTAYS) AS total_overnightstays
FROM
  workspace.default.overnightstays
GROUP BY
  COUNTRYOFRESIDENCE
ORDER BY
  total_overnightstays DESC&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;FONT color="#000000"&gt;The screencam shows how a new cell can be added to the Notebook, by hovering with the mouse at the bottom of the cell above. Select "Code" as cell type. Change the cell's language selector to "SQL", paste the code and run it with the blue play button. You will see the same result as before in the "SQL Editor".&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT color="#000000"&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="270 notebook with sql.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/291834iD2FD9AF968D6F360/image-size/large?v=v2&amp;amp;px=999" role="button" title="270 notebook with sql.gif" alt="270 notebook with sql.gif" /&gt;&lt;/span&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Let's switch to Python for the data exploration so that you have tried all the options. Create a new Code cell, its language selector might already be on "Python" by default. Begin by loading the data into a PySpark DataFrame using the table's fully qualified path.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;overnightstays_sdf = spark.read.table("workspace.default.overnightstays") &lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Aggregate the data by month and look at the results. The display command initially shows the data as table, but it also comes with a Graphical User interface to quickly create a plot. Follow the steps in the screencam to create a line chart, to see how the numbers have evolved over time. There is a clear pattern, that especially in July and August the numbers are at their highest.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import pyspark.sql.functions as F
display(overnightstays_sdf.groupBy("month").agg(F.sum("overnightstays").alias("overnightstays")))&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="280 display chart.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292292iF1C7C59E9B75F071/image-size/large?v=v2&amp;amp;px=999" role="button" title="280 display chart.gif" alt="280 display chart.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Now explore, whether we can visually get a feel for an overall trend in the data, whether the numbers tend to go up or down over time. Have the Assistant do the work of writing the code with this request:&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Create a new DataFrame that has the year in a new column. aggregate by year and show the result in a plotly chart&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Plotly is a very common Python charting library. And indeed, when looking at the yearly totals, the numbers are increasing for the years.&amp;nbsp; The year 2025 is not meaningful yet in this chart as the dataset only contains January to May for that year.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="290 yearly totals.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292297i05056079B6F0B3FE/image-size/large?v=v2&amp;amp;px=999" role="button" title="290 yearly totals.gif" alt="290 yearly totals.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="forecast" id="toc-hId-33969827"&gt;Create a forecast with SAP Databricks&lt;/H1&gt;&lt;P&gt;During the above data exploration we saw that the data has a trend (numbers are increasing over time) and some seasonality (ie numbers are largest in the summer). Let's train a Machine Learning that picks up on such patterns and can estimate future values.&amp;nbsp;&lt;FONT color="#000000"&gt;The Notebook created in this section can also be downloaded from the &lt;A href="https://github.com/SAP-samples/mee-samples/tree/main/Hands-on%20Tutorial%20SAP%20Databricks" target="_self" rel="nofollow noopener noreferrer"&gt;repository&lt;/A&gt;.&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;In case you are unsure about how to implement the following 4 code blocks in a new notebook called "020 Demand forecast", then this screencam will guide you along. It shows in a quick scroll through what the output should look like.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="400 forecast prep.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292310iC793B83B80A6B323/image-size/large?v=v2&amp;amp;px=999" role="button" title="400 forecast prep.gif" alt="400 forecast prep.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Read the historic data again into a PySpark DataFrame.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;overnightstays_sdf = spark.read.table("workspace.default.overnightstays") &lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Since we want to create a monthly forecast on the total values, aggregate the history by month. Since we are working with the PySpark DataFrame, the built-in Spark engine is doing the work.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import pyspark.sql.functions as F
overnightstaysmonthly_sdf = overnightstays_sdf.groupBy("month").agg(F.sum("overnightstays").alias("overnightstays"))
display(overnightstaysmonthly_sdf)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;The data needs to be prepared further, so that the Python package called Prophet can train a time-series model on it.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# Convert Spark DataFrame to Pandas DataFrame (the time series algorithm requires a Pandas DataFrame)
overnightstaysmonthly_df = overnightstaysmonthly_sdf.toPandas()

# Sort the DataFrame by date
overnightstaysmonthly_df = overnightstaysmonthly_df.sort_values('month')

# Rename the columns, as required by the time-series algoritm Prophet
overnightstaysmonthly_df = overnightstaysmonthly_df.rename(columns={'month': 'ds', 'overnightstays': 'y'})&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;We want to use the Prophet package for the forecast, which still needs to be installed. Here we install it directly from the Notebook. However, in the "Good to know" section below you see a more elegant way of using additional Python package, by creating your own customised Base environment.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;pip install prophet==1.1.7&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Everything is now in place to finally train the time-series model. The following block of code contains multiple steps. It is useful to run these steps together under the "with mlflow.start_run() section as this allows to log information about the training run in SAP Databricks.&amp;nbsp; The following code:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Trains the time series model&lt;/LI&gt;&lt;LI&gt;Creates a DataFrame with the future 12 months that are to be forecasted&lt;/LI&gt;&lt;LI&gt;Creates a forecast for the known past and the future 12 months&lt;/LI&gt;&lt;LI&gt;Calculates&amp;nbsp; and logs the model's accuracy on the known history&lt;/LI&gt;&lt;LI&gt;Plots and logs the known history against the predicted values&lt;/LI&gt;&lt;LI&gt;Logs the number of records that were used during training&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;Yes, in a real project this code would be even more elaborate. For example you may want to try different model configurations to get even better forecast accuracy. And the accuracy should ideally be calculated on a hold-out sample. Currently the code uses the same data for training as well as for checking the model's accuracy. The accuracy should really be calculated on data the model has never seen before.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;EM&gt;HINT: In case the following code gives the error "MLflow not available", please check in the Environment settings on the right, that the "Environment version" is set to 2. These environments are explained a bit further down in the "Good to know" section further below. For some people it defaults to version 1 (which doesn't include the mlflow library), for others it defaults to the required version 2.&lt;/EM&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import pandas as pd
from prophet import Prophet
from sklearn.metrics import mean_absolute_percentage_error
import matplotlib.pyplot as plt
import mlflow, mlflow.tracking._model_registry.utils

mlflow.tracking._model_registry.utils._get_registry_uri_from_spark_session = lambda: "databricks-uc"

with mlflow.start_run():

    # Initialize the Prophet model
    model = Prophet()

    # Fit the model
    model.fit(overnightstaysmonthly_df)

    # Create a DataFrame that contains all dates for which a prediction is required, the future 12 months but also the known past for comparison
    datestopredict_df = model.make_future_dataframe(periods=12, freq='MS')

    # Forecast the future and known past
    forecast_df = model.predict(datestopredict_df)

    # Plot the predictions together with known past
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.plot(forecast_df['ds'], forecast_df['yhat'], label='Forecast', color='red')
    ax.plot(overnightstaysmonthly_df['ds'], overnightstaysmonthly_df['y'], label='Historical Data')
    ax.fill_between(forecast_df['ds'], forecast_df['yhat_lower'], forecast_df['yhat_upper'], color='red', alpha=0.3)
    ax.set_title('Demand forecast')
    ax.set_xlabel('Month')
    ax.set_ylabel('Total Overnightstays')
    ax.legend()
    ax.grid(True)
    plt.show()

    # Calculate and log model accuracy, here MAPE on training data
    overnightstaysmonthly_df[["ds"]] = overnightstaysmonthly_df[["ds"]].apply(pd.to_datetime)
    anctualsandpredicted_df = overnightstaysmonthly_df[['ds', 'y']].merge(forecast_df[['ds', 'yhat']], on='ds', how='inner', suffixes=('_left', '_right'))
    prophet_mape = mean_absolute_percentage_error(anctualsandpredicted_df['y'], anctualsandpredicted_df['yhat'])
    mlflow.log_metric("mape", prophet_mape)

    # Log the chart in the Unity Catalog / Experiment
    mlflow.log_figure(fig, "Forecast.png") 

    # Log the size of the training dataset
    mlflow.log_metric("rowcount_training", overnightstaysmonthly_df.shape[0])&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Run the above code and we see our prediction!&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="410 forecast.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292311i9103A5F02A8E234B/image-size/large?v=v2&amp;amp;px=999" role="button" title="410 forecast.gif" alt="410 forecast.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;To persist the forecast, save it as DeltaTable. From here the data can be made available to SAP Datasphere and SAP Analytics Cloud. That part is outside the scope of this tutorial, please check the &lt;A href="https://help.sap.com/docs/SAP_BUSINESS_DATA_CLOUD/3708ee482fc441ef8bb91711b1629109/c4464c041dec43b58a32501c6a6dda3a.html?locale=en-US" target="_self" rel="noopener noreferrer"&gt;documentation&lt;/A&gt;.&lt;/P&gt;&lt;P&gt;&lt;EM&gt;HINT: In case the saving fails with the message "PERMISSION_DENIED", then another user has already created that table and you are not allowed to overwrite it. Just change the name of the table you are creating to proceed.&amp;nbsp;&lt;/EM&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;forecast_sdf = spark.createDataFrame(forecast_df)
forecast_sdf.write.mode("overwrite").saveAsTable("workspace.default.overnightstays_forecast")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;The forecast is saved as DeltaTable, that means you can see it in the Catalog!&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="420 forecast in catalog.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292312iF8375CFED009E0F8/image-size/large?v=v2&amp;amp;px=999" role="button" title="420 forecast in catalog.gif" alt="420 forecast in catalog.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;The information that was logged during the training is now available in the Experiments section. The model's MAPE (Median Absolute Percentage Error) is at 1.4%, 41 records were used during training and we can see the chart that compares actuals to predictions. Especially when trying out different settings for the Machine Learning algorithm, this tracking becomes very useful, for instance to select which configuration to use for the prediction.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="430 catalog.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292313i0DFA2CC5C05A03F6/image-size/large?v=v2&amp;amp;px=999" role="button" title="430 catalog.gif" alt="430 catalog.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="schedule" id="toc-hId--162543678"&gt;Schedule&lt;/H1&gt;&lt;P&gt;Everything we need for our demand forecast is implemented and we can run it manually. That means we are also good to have it automated with a schedule. Straight from the notebook you can define the scheduling logic, ie the recurring interval and whether you want any notifications. The notebooks can also be triggered through APIs if you prefer.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="500 schedule.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292322i2DBBC49C66338C25/image-size/large?v=v2&amp;amp;px=999" role="button" title="500 schedule.gif" alt="500 schedule.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="goodtoknow" id="toc-hId-410682900"&gt;Good to know&lt;/H1&gt;&lt;P&gt;A few things that might be good to know.&lt;/P&gt;&lt;H2 id="toc-hId--79233612"&gt;&amp;nbsp;&lt;/H2&gt;&lt;H2 id="toc-hId--275747117"&gt;Base file&lt;/H2&gt;&lt;P&gt;In the above code the Python package Prophet is installed directly from within a Notebook. That's ok, but not ideal for ongoing use. If you were repeatedly re-running all cells from the Notebook, then Python will try to re-install the package every time. It's generally better to centrally specify the packages you know you will be using long term. This also ensures consistency across projects and developers. In SAP Databricks you can specify reusable / shareable lists of Python packages that are to be installed through Base environments.&lt;/P&gt;&lt;P&gt;Create a Base environment by creating a file called&amp;nbsp;base_env_prophet.yaml with this content:&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;dependencies:
  - prophet==1.1.7&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="600 create base env file.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292444i2F3BB0DE70432780/image-size/large?v=v2&amp;amp;px=999" role="button" title="600 create base env file.gif" alt="600 create base env file.gif" /&gt;&lt;/span&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;With this base file in place, you can&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Step 1: Remove the pip install command from the "020 Demand forecast" Notebook&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="660 1 Delete cells.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292457iE2926E77E522CDA1/image-size/large?v=v2&amp;amp;px=999" role="button" title="660 1 Delete cells.gif" alt="660 1 Delete cells.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Step 2: Specify that the Notebook will use the base file instead, to determine which packages to install. Here you can also select the Environment version. These environment versions are documented&amp;nbsp;&lt;A href="https://docs.databricks.com/aws/en/release-notes/serverless/environment-version/" target="_self" rel="nofollow noopener noreferrer"&gt;here&lt;/A&gt;. After selecting a different base file or after changing the environment version you need hit "Apply" and confirm for the change to take effect.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="660 2 Apply base file.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292459i632B6A73035CB9CF/image-size/large?v=v2&amp;amp;px=999" role="button" title="660 2 Apply base file.gif" alt="660 2 Apply base file.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;To test out that the Package is correctly installed through the base file, clear the environment as shown in the screencam and run the Python code in the cells.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="620 run with base file.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292470i7C64B17F50E7A505/image-size/large?v=v2&amp;amp;px=999" role="button" title="620 run with base file.gif" alt="620 run with base file.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId--472260622"&gt;Lineage &amp;amp; Table usage insights&lt;/H2&gt;&lt;P&gt;Now that the table "overnightstays" has been used a few times, check out the lineage, which shows where the table is used and the statistics on how heavily that table is used.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="550 lineage and insights.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292347iEE1C9D6942221C23/image-size/large?v=v2&amp;amp;px=999" role="button" title="550 lineage and insights.gif" alt="550 lineage and insights.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId--668774127"&gt;Visuals&lt;/H2&gt;&lt;P&gt;From the distance most Notebooks look very much alike at first glance. I heard some great feedback that some visual / image on top of the notebook would help to differentiate. In the repository you find &lt;A href="https://github.com/SAP-samples/mee-samples/tree/main/Hands-on%20Tutorial%20SAP%20Databricks/Images" target="_self" rel="nofollow noopener noreferrer"&gt;a few examples&lt;/A&gt;, in case you would like to use those to distinguish between Data Exploration, AI/ML Sandboxing and AI/ML Deployment.&lt;/P&gt;&lt;P&gt;Upload any images that you want to use into the Workspace. To keep things tidy I am creating a separate folder for them.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="650 image upload.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292681i472440DD7D4505B1/image-size/large?v=v2&amp;amp;px=999" role="button" title="650 image upload.gif" alt="650 image upload.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;These images can then be shown in a Markup cell.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="660 image display.gif" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/292683i5E97FE69DBC59287/image-size/large?v=v2&amp;amp;px=999" role="button" title="660 image display.gif" alt="660 image display.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="summary" id="toc-hId--571884625"&gt;Summary&lt;/H1&gt;&lt;P&gt;SAP Databricks is adding a dedicated Data Science environment to the SAP landscape. After this hands-on experience you hopefully have a first feel for how a Data Scientist can enrich the data in SAP Business Data Cloud with Machine Learning.&lt;/P&gt;&lt;P&gt;For your own projects, you will want to check out the documentation:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;A href="https://help.sap.com/docs/business-data-cloud/sap-databricks/introducing-sap-databricks" target="_self" rel="noopener noreferrer"&gt;SAP Databricks documentation by SAP&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://docs.databricks.com/sap/en/" target="_self" rel="nofollow noopener noreferrer"&gt;SAP Databricks documentation by Databricks&lt;/A&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;I hope you enjoyed getting hands-on with SAP Databricks!&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/artificial-intelligence-blogs-posts/hands-on-tutorial-sap-databricks/ba-p/14156999"/>
    <published>2025-08-04T09:17:31.648000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-4/ba-p/14174201</id>
    <title>SAP Databricks: Building an Intelligent Enterprise with AI Unleashed â€“ Part 4</title>
    <updated>2025-08-07T08:54:53.397000+02:00</updated>
    <author>
      <name>jing_wen</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1923466</uri>
    </author>
    <content>&lt;P&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-1/ba-p/14166813" target="_self"&gt;&lt;SPAN&gt;Part 1 â€“ SQL analytics with SAP Data Products&lt;BR /&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-2/ba-p/14167025" target="_self"&gt;Part 2 â€“ Build and deploy Mosaic AI and Agent Tools&lt;/A&gt;&lt;BR /&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-how-to-use-automl-to-forecast-sales-data-part-3/ba-p/14174354" target="_self"&gt;Part 3 â€“ How to use AutoML to forecast sales data&lt;/A&gt;&lt;BR /&gt;&lt;/SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-3/ba-p/14174201" target="_self"&gt;&lt;SPAN&gt;Part 4 â€“ Connect SAP Data Products with non-SAP data from AWS S3&lt;BR /&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-4/ba-p/14178056" target="_self"&gt;Part 5 â€“ End-to-end integration: SAP Databricks, SAP Datasphere, and SAP Analytics Cloud&lt;/A&gt;&lt;BR /&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-create-inferences-for-application-integration-with-sap-build/ba-p/14186662" target="_self"&gt;Part 6 â€“ Create inferences and endpoints for application integration with SAP Build&lt;/A&gt;&lt;/SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-create-inferences-for-application-integration-with-sap-build/ba-p/14186662" target="_self"&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/A&gt;&lt;BR /&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-in-sap-business-data-cloud-a-typical-machine-learning/ba-p/14206612" target="_self"&gt;Part 7&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;â€“&amp;nbsp;SAP Databricks in SAP Business Data Cloud - A Typical Machine Learning Workflow&lt;/SPAN&gt;&lt;/A&gt;&lt;/P&gt;&lt;H3 id="toc-hId-1865165611" id="toc-hId-1866023751"&gt;SAP Databricks in SAP Business Data Cloud&amp;nbsp;&lt;/H3&gt;&lt;P&gt;In today's data-driven world, gaining a 360-degree view of your enterprise requires combining data from all corners of your business. SAP systems house governed business data. Meanwhile, Amazon S3 often contains unstructured or external dataâ€”everything from web logs and partner data to machine learning features and fraud detection inputs.&lt;/P&gt;&lt;H3 id="toc-hId-1669510246"&gt;&lt;STRONG&gt;Step-by-Step Playbook: Connecting AWS S3 to SAP Databricks&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;In this part of our blog series, weâ€™ll walk you through &lt;STRONG&gt;connecting SAP Databricks to an Amazon S3 bucket&lt;/STRONG&gt; â€” empowering you to unify data from diverse sources and accelerate your analytics and AI workflows.&lt;/P&gt;&lt;P&gt;By the end of this guide, youâ€™ll have:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;A &lt;STRONG&gt;Unity Catalog external location&lt;/STRONG&gt; pointing to your S3 bucket&lt;/LI&gt;&lt;LI&gt;A &lt;STRONG&gt;registered table&lt;/STRONG&gt; in Unity Catalog&lt;/LI&gt;&lt;LI&gt;A &lt;STRONG&gt;ready-to-run SAP Databricks notebook&lt;/STRONG&gt; to analyze your unified data&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;We will be covering the following steps in this guide:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Stepâ€¯1 â€“ Create the External Location (AWS Quickstart)&lt;/LI&gt;&lt;LI&gt;Stepâ€¯2 â€“ Generate a Personal Access Token (PAT) in SAPâ€¯Databricks&lt;/LI&gt;&lt;LI&gt;Stepâ€¯3 â€“ Identify (or create) the target S3 bucket&lt;/LI&gt;&lt;LI&gt;Stepâ€¯4 â€“ Configure the AWS IAM Role &amp;amp; Policies&lt;/LI&gt;&lt;LI&gt;Stepâ€¯5 â€“ Launch the AWS Quickstart CloudFormation Stack&lt;/LI&gt;&lt;LI&gt;Stepâ€¯6 â€“ Provide the PAT to the CloudFormation Stack&lt;/LI&gt;&lt;LI&gt;Stepâ€¯7 â€“ Verify CloudFormation Completion&lt;/LI&gt;&lt;LI&gt;Stepâ€¯8 â€“ Test the External Location connection in SAPâ€¯Databricks&lt;/LI&gt;&lt;LI&gt;Stepâ€¯9â€“ Create a Unity Catalog table from the S3 data&lt;/LI&gt;&lt;LI&gt;Stepâ€¯10â€¯â€“ Work with SAP Data Products and non-SAP data from S3 within the SAP Databricks Notebook environment&amp;nbsp;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;Let's dive in.&lt;/P&gt;&lt;H4 id="toc-hId-1602079460"&gt;&lt;STRONG&gt;Prerequisites&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;You must create the S3 bucket that you want to use as an external location before you create the external location object in&amp;nbsp;SAP Databricks. More details documented &lt;A href="https://docs.databricks.com/sap/en/external-locations" target="_blank" rel="noopener nofollow noreferrer"&gt;here&lt;/A&gt;:&lt;/LI&gt;&lt;/UL&gt;&lt;UL&gt;&lt;UL&gt;&lt;LI&gt;The AWS CloudFormation template supports only S3 buckets.&lt;/LI&gt;&lt;LI&gt;The name of an S3 bucket that you want users to read from and write to cannot use dot notation (for example,&amp;nbsp;incorrect.bucket.name.notation). For more bucket naming guidance, see the&amp;nbsp;&lt;A href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html" target="_blank" rel="noopener nofollow noreferrer"&gt;AWS bucket naming rules&lt;/A&gt;.&lt;/LI&gt;&lt;LI&gt;Avoid using a path in S3 that is already defined as an external location in another&amp;nbsp;Unity Catalog&amp;nbsp;metastore. You can safely read data in a single external S3 location from more than one metastore, but concurrent writes to the same S3 location from multiple metastores can lead to consistency issues.&lt;/LI&gt;&lt;/UL&gt;&lt;/UL&gt;&lt;UL&gt;&lt;LI&gt;If you don't use the AWS CloudFormation template to create the external location, you must first create a storage credential in&amp;nbsp;SAP Databricks&amp;nbsp;that gives access to the cloud storage location path. See the following AWS doc:&amp;nbsp;&lt;A href="https://docs.databricks.com/aws/connect/unity-catalog/cloud-storage/storage-credentials" target="_blank" rel="noopener nofollow noreferrer"&gt;Create a storage credential&lt;/A&gt;.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;If you use the AWS CloudFormation flow, that storage credential is created for you.&lt;/P&gt;&lt;H4 id="toc-hId-1405565955"&gt;&lt;STRONG&gt;Permissions requirements&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;You must have the&amp;nbsp;&lt;STRONG&gt;CREATE EXTERNAL LOCATION&lt;/STRONG&gt;&amp;nbsp;privilege on both the metastore and the storage credential referenced in the external location. Metastore admins have&amp;nbsp;CREATE EXTERNAL LOCATION&amp;nbsp;on the metastore by default.&lt;/LI&gt;&lt;LI&gt;If you are using the AWS CloudFormation template, you must also have the&amp;nbsp;&lt;STRONG&gt;CREATE STORAGE CREDENTIAL&amp;nbsp;&lt;/STRONG&gt;privilege on the metastore. Metastore admins have&amp;nbsp;CREATE STORAGE CREDENTIAL&amp;nbsp;on the metastore by default.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;This guide assumes you're using the &lt;STRONG&gt;SAP Databricks Quickstart on AWS&lt;/STRONG&gt;, which automates much of the setup. Advanced users may opt for a manual setup with your IAM role (ARN), including the encryption algorithm of SSE-S3 or SSE-KMS.&lt;/P&gt;&lt;H4 id="toc-hId-1209052450"&gt;&lt;STRONG&gt;Step 1 â€“ Create the External Location&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;In&amp;nbsp;&lt;STRONG&gt;SAP Databricks Unity Catalog,&amp;nbsp;&lt;/STRONG&gt;navigate to "Create an external location".&lt;/LI&gt;&lt;LI&gt;Use the &lt;STRONG&gt;Quickstart&lt;/STRONG&gt; wizard to create your external location.&lt;/LI&gt;&lt;LI&gt;Alternatively, advanced users can manually configure an &lt;STRONG&gt;AWS IAM Role&lt;/STRONG&gt; and specify the &lt;STRONG&gt;S3 bucket path&lt;/STRONG&gt;.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="1.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297594i52AB164F42A7AC08/image-size/large?v=v2&amp;amp;px=999" role="button" title="1.png" alt="1.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="2.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297596i873B39FF94B3F71D/image-size/large?v=v2&amp;amp;px=999" role="button" title="2.png" alt="2.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H4 id="toc-hId-1012538945"&gt;&lt;STRONG&gt;Step 2 â€“ Generate a Personal Access Token (PAT)&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;In SAP Databricks, generate a &lt;STRONG&gt;PAT&lt;/STRONG&gt; that will be used during the AWS CloudFormation setup.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="3 - AWS Quickstart.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297597i207587AA1288ACF2/image-size/large?v=v2&amp;amp;px=999" role="button" title="3 - AWS Quickstart.png" alt="3 - AWS Quickstart.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H4 id="toc-hId-816025440"&gt;&lt;STRONG&gt;Step 3 â€“ Identify Your S3 Bucket&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;Meanwhile, choose or create the &lt;STRONG&gt;S3 bucket&lt;/STRONG&gt; that holds your non-SAP data.&lt;/LI&gt;&lt;LI&gt;Example: Fraud detection data (&lt;A href="https://github.com/aws-samples/aws-fraud-detector-samples/tree/master/data" target="_blank" rel="noopener nofollow noreferrer"&gt;AWS sample data&lt;/A&gt;).&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="4 - AWS S3 Bucket.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297598i925D069B74492B1E/image-size/large?v=v2&amp;amp;px=999" role="button" title="4 - AWS S3 Bucket.png" alt="4 - AWS S3 Bucket.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H4 id="toc-hId-619511935"&gt;&lt;STRONG&gt;Step 4 â€“ Configure Your AWS IAM Role &amp;amp; Policies&lt;/STRONG&gt;&lt;/H4&gt;&lt;P&gt;Ensure the IAM role used by &lt;STRONG&gt;SAP Databricks&lt;/STRONG&gt; has the following permissions to enable secure and functional access to your AWS data environment:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;AmazonS3FullAccess&lt;/STRONG&gt;&lt;BR /&gt;Grants full access to all Amazon S3 buckets and objects within the account.&lt;BR /&gt;&lt;EM&gt;Used to read/write data from S3, list buckets, and manage objects.&lt;/EM&gt;&lt;EM&gt;&lt;BR /&gt;&lt;/EM&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;AmazonS3TablesFullAccess&lt;/STRONG&gt;&lt;BR /&gt;Grants access to AWS Glue tables and data lake metadata.&lt;BR /&gt;&lt;EM&gt;Supports Unity Catalog integration with external tables stored in S3.&lt;/EM&gt;&lt;EM&gt;&lt;BR /&gt;&lt;/EM&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;AmazonS3ObjectLambdaExecutionRolePolicy&lt;/STRONG&gt;&lt;BR /&gt;Enables the role to invoke AWS Lambda functions when accessing S3 objects.&lt;BR /&gt;&lt;EM&gt;Required only if using S3 Object Lambda to dynamically transform or filter data before it is consumed by Databricks.&lt;BR /&gt;&lt;/EM&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;IAMFullAccess&lt;/STRONG&gt;&lt;BR /&gt;Grants full access to manage IAM resources such as roles, policies, users, and groups.&lt;BR /&gt;&lt;EM&gt;Recommended only for administrative or automated setup environments (e.g., CloudFormation). For production, follow the principle of least privilege and only allow specific IAM actions such as &lt;/EM&gt;&lt;EM&gt;iam:PassRole&lt;/EM&gt;&lt;EM&gt; or &lt;/EM&gt;&lt;EM&gt;iam:AssumeRole&lt;/EM&gt;&lt;EM&gt;.&lt;BR /&gt;&lt;/EM&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;EM&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="4 - AmazonS3Access .png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297601iF40C9B86D2A955E1/image-size/large?v=v2&amp;amp;px=999" role="button" title="4 - AmazonS3Access .png" alt="4 - AmazonS3Access .png" /&gt;&lt;/span&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="4 - IAMFullAccess.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297602iB42FDD37280BEBA9/image-size/large?v=v2&amp;amp;px=999" role="button" title="4 - IAMFullAccess.png" alt="4 - IAMFullAccess.png" /&gt;&lt;/span&gt;&lt;/EM&gt;&lt;/P&gt;&lt;H4 id="toc-hId-422998430"&gt;&lt;STRONG&gt;Step 5 â€“ Launch the AWS Quickstart CloudFormation Stack&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;Launch the Quickstart template from within SAP Databricks, and it will direct you to the AWS Quickstart CloudFormation Stack.&lt;/LI&gt;&lt;LI&gt;Note the Personal Access Token (PAT) as you'll be prompted to input it in AWS.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="5 - Launch in Quickstart.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297608iE93B69E4AD46AAAB/image-size/large?v=v2&amp;amp;px=999" role="button" title="5 - Launch in Quickstart.png" alt="5 - Launch in Quickstart.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H4 id="toc-hId-226484925"&gt;&lt;STRONG&gt;Step 6 â€“ Provide Your Personal Access Token to the Stack&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;Input the &lt;STRONG&gt;PAT&lt;/STRONG&gt; generated in the previous step to allow SAP Databricks access.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="5 - Quickstart.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297609i2CF23993EB954E0A/image-size/large?v=v2&amp;amp;px=999" role="button" title="5 - Quickstart.png" alt="5 - Quickstart.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H4 id="toc-hId--467745675"&gt;&lt;STRONG&gt;Step 7 â€“ Verify Stack Completion&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;Monitor the CloudFormation Stack â€“ the status will change from &lt;STRONG&gt;CREATE_IN_PROGRESS&lt;/STRONG&gt; to &lt;STRONG&gt;CREATE_COMPLETE&lt;/STRONG&gt;.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="6 - Quickstart Successful Create_Complete.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297610i6971C5330C84E07B/image-size/large?v=v2&amp;amp;px=999" role="button" title="6 - Quickstart Successful Create_Complete.png" alt="6 - Quickstart Successful Create_Complete.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H4 id="toc-hId--664259180"&gt;&lt;STRONG&gt;Step 8 â€“ Test the External Location Connection&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;In SAP Databricks, return to your external location under Catalog Explorer and &lt;STRONG&gt;click â€œTest Connectionâ€&lt;/STRONG&gt;.&lt;/LI&gt;&lt;LI&gt;Verify external location and credential permissions for &lt;STRONG&gt;Read&lt;/STRONG&gt;, &lt;STRONG&gt;Write&lt;/STRONG&gt;, and more.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="7 - SAP Databricks External Data.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297611iF09BB1A9C1062738/image-size/large?v=v2&amp;amp;px=999" role="button" title="7 - SAP Databricks External Data.png" alt="7 - SAP Databricks External Data.png" /&gt;&lt;/span&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="8 - S3 Files.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297612i5FE943658CF2A33D/image-size/large?v=v2&amp;amp;px=999" role="button" title="8 - S3 Files.png" alt="8 - S3 Files.png" /&gt;&lt;/span&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="9 - SAP Databricks Test Connection.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297613i87F80F84500A569B/image-size/large?v=v2&amp;amp;px=999" role="button" title="9 - SAP Databricks Test Connection.png" alt="9 - SAP Databricks Test Connection.png" /&gt;&lt;/span&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="10 - Validate Configuration.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297614iBE4F3B7D83C5AE50/image-size/large?v=v2&amp;amp;px=999" role="button" title="10 - Validate Configuration.png" alt="10 - Validate Configuration.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H4 id="toc-hId--860772685"&gt;&lt;STRONG&gt;Step 9 â€“&amp;nbsp;&lt;/STRONG&gt;&lt;STRONG&gt;Create a Unity Catalog Table from S3 Data&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;Select a file in your S3 bucket and &lt;STRONG&gt;register it as a table&lt;/STRONG&gt; in Unity Catalog.&lt;/LI&gt;&lt;LI&gt;Navigate to Unity Catalog to confirm your new table appears and is accessible for querying&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="11 - Create Table.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297616i7A6F34A35E5E0B9B/image-size/large?v=v2&amp;amp;px=999" role="button" title="11 - Create Table.png" alt="11 - Create Table.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="12 - Select Table.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297618iFF0A10EA29EB96D9/image-size/large?v=v2&amp;amp;px=999" role="button" title="12 - Select Table.png" alt="12 - Select Table.png" /&gt;&lt;/span&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="13 - Preview &amp;amp; Create Table.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297619i46A13DCAFE21360F/image-size/large?v=v2&amp;amp;px=999" role="button" title="13 - Preview &amp;amp; Create Table.png" alt="13 - Preview &amp;amp; Create Table.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="14 - Table in UC.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297620i25D3A182C447310E/image-size/large?v=v2&amp;amp;px=999" role="button" title="14 - Table in UC.png" alt="14 - Table in UC.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H4 id="toc-hId--1057286190"&gt;&lt;STRONG&gt;Step 10 â€“ Work with SAP Data Products and non-SAP S3 Data within the Notebook environment&lt;/STRONG&gt;&lt;/H4&gt;&lt;UL&gt;&lt;LI&gt;Use the SAP Databricks notebook environment to run analyses on your newly unified data.&lt;/LI&gt;&lt;LI&gt;You can join SAP Data Products with external S3 data for richer insights, all within the same environment.&lt;/LI&gt;&lt;LI&gt;For basic visualizations, harness the power of the notebook environment. For comprehensive dashboarding, SAP Analytics Cloudâ€”part of the SAP Business Data Cloudâ€”offers pre-built templates, robust architecture, and seamless integration with enterprise data, enabling scalable and insightful analytics.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Notebook Dashboard.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/297622i085B94E22DC7BDB5/image-size/large?v=v2&amp;amp;px=999" role="button" title="Notebook Dashboard.png" alt="Notebook Dashboard.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;By connecting AWS S3 to SAP Databricks, you can seamlessly unify external and SAP business data for more powerful analytics and AI workflows. This integration enables easy access, management, and analysis of diverse datasets within SAP Business Data Cloud.&amp;nbsp;&lt;/P&gt;&lt;P&gt;In the &lt;STRONG&gt;next part&lt;/STRONG&gt;, we will explore &lt;STRONG&gt;end-to-end integration between SAP Databricks, SAP Datasphere, and SAP Analytics Cloud&lt;/STRONG&gt;, completing the unified analytics journey from data ingestion to actionable insights.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-4/ba-p/14174201"/>
    <published>2025-08-07T08:54:53.397000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/building-agents-for-a-simple-microservice-architecture-with-fastapi-part-2/ba-p/14176702</id>
    <title>ðŸš€Building Agents  for a Simple Microservice Architecture with FastAPI (Part 2)</title>
    <updated>2025-08-10T09:01:21.368000+02:00</updated>
    <author>
      <name>Yogananda</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/75</uri>
    </author>
    <content>&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Previous Blog :&amp;nbsp;&amp;nbsp;&lt;/STRONG&gt;&lt;SPAN class=""&gt;&lt;A class="" href="https://community.sap.com/t5/technology-blog-posts-by-sap/building-collaborative-microservices-in-python-with-fastapi-echo-amp/ba-p/14170025" target="_blank"&gt;Building Collaborative Microservices in Python with FastAPI: Echo &amp;amp; Reverse Agents (Beginner -Part1)&lt;/A&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;EM&gt;Microservices are a powerful way to design scalable and maintainable applications. &lt;/EM&gt;&lt;/P&gt;&lt;P&gt;&lt;EM&gt;In this blog, we will explore a minimal yet effective microservice setup using&amp;nbsp;&lt;STRONG&gt;FastAPI&lt;/STRONG&gt;, perfect for learning and experimentation. This will help to you build better Microservices and deploy in SAP BTP - Kyma&lt;/EM&gt;&lt;/P&gt;&lt;H3 id="toc-hId-1866088139"&gt;Sample Use Case&lt;/H3&gt;&lt;P&gt;A client sends a city name to the Weather Agent. The agent fetches enrichment data from the Data Enricher, generates fake weather data, and returns a combined report. This mimics real-world API composition and data aggregation.&lt;/P&gt;&lt;H3 id="toc-hId-1669574634"&gt;Overview&lt;/H3&gt;&lt;P&gt;It consists of two core services:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Fake Weather Agent&amp;nbsp;(&lt;FONT color="#FF6600"&gt;weather_agent.py&lt;/FONT&gt;)&lt;/LI&gt;&lt;LI&gt;Data Enricher&amp;nbsp;(&lt;FONT color="#FF6600"&gt;data_enricher.py&lt;/FONT&gt;)&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;A shell script (&lt;FONT color="#FF6600"&gt;run.sh&lt;/FONT&gt;) is included to launch both services on separate ports, simulating a real-world microservice environment.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Yogananda_0-1754809227004.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/298973i1E5B429726C6F429/image-size/large?v=v2&amp;amp;px=999" role="button" title="Yogananda_0-1754809227004.png" alt="Yogananda_0-1754809227004.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId-1473061129"&gt;&lt;span class="lia-unicode-emoji" title=":sun_behind_rain_cloud:"&gt;ðŸŒ¦&lt;/span&gt;ï¸ 1. Fake Weather Agent (&lt;FONT color="#FF6600"&gt;weather_agent.py&lt;/FONT&gt;)&lt;/H3&gt;&lt;P&gt;&lt;FONT color="#3366FF"&gt;Purpose&lt;/FONT&gt;:&amp;nbsp; &amp;nbsp;Generates a fake weather report for a given city.&lt;/P&gt;&lt;P&gt;&lt;FONT color="#3366FF"&gt;API Endpoint:&amp;nbsp;&amp;nbsp;&lt;/FONT&gt;&lt;FONT color="#FF6600"&gt;POST /weather&lt;/FONT&gt;&amp;nbsp;â€” Accepts a JSON payload with a city name.&lt;/P&gt;&lt;P&gt;&lt;FONT color="#3366FF"&gt;How It Works:&lt;/FONT&gt;&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;Receives a city name from the client.&lt;/LI&gt;&lt;LI&gt;Optionally calls the&amp;nbsp;Data Enricher&amp;nbsp;service to fetch additional info (e.g., population, country).&lt;/LI&gt;&lt;LI&gt;Generates random weather data:&lt;UL&gt;&lt;LI&gt;Temperature&lt;/LI&gt;&lt;LI&gt;Condition (e.g., sunny, rainy)&lt;/LI&gt;&lt;LI&gt;Humidity&lt;/LI&gt;&lt;LI&gt;Wind speed&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;LI&gt;Returns a combined weather report, enriched with city metadata if available.&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&lt;FONT color="#3366FF"&gt;Tech Stack:&lt;/FONT&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;FastAPI for API development&lt;/LI&gt;&lt;LI&gt;Pydantic for data validation&lt;/LI&gt;&lt;LI&gt;httpx for asynchronous HTTP calls&lt;/LI&gt;&lt;/UL&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import httpx
import os
import random

PORT = int(os.getenv("PORT", 8002))
TARGET = os.getenv("TARGET_URL", "http://localhost:8003")   # downstream agent

app = FastAPI(title="Fake-Weather-Agent")

class Location(BaseModel):
    city: str

class WeatherReport(BaseModel):
    source: str
    city: str
    temperature: float   # Â°C
    condition: str
    humidity: int        # %
    wind_kmh: float

CONDITIONS = ["Sunny", "Cloudy", "Rain", "Snow", "Thunderstorm"]

@app.post("/weather", response_model=WeatherReport)
async def get_weather(loc: Location):
    """Generate a fake weather report for the given city."""
    # Optionally call another agent (e.g. a â€œdata-enrichmentâ€ service)
    async with httpx.AsyncClient() as client:
        try:
            r = await client.post(
                f"{TARGET}/enrich",
                json={"city": loc.city}
            )
            r.raise_for_status()
            extra = r.json()
        except Exception:
            extra = {}

    return WeatherReport(
        source="Fake-Weather-Agent",
        city=loc.city,
        temperature=round(random.uniform(-10, 40), 1),
        condition=random.choice(CONDITIONS),
        humidity=random.randint(20, 95),
        wind_kmh=round(random.uniform(0, 40), 1),
        **extra
    )&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-1276547624"&gt;&lt;span class="lia-unicode-emoji" title=":cityscape:"&gt;ðŸ™&lt;/span&gt;ï¸ 2. Data Enricher (&lt;FONT color="#FF6600"&gt;data_enricher.py&lt;/FONT&gt;)&lt;/H3&gt;&lt;P&gt;&lt;FONT color="#3366FF"&gt;Purpose&lt;/FONT&gt;:&amp;nbsp;Provides additional metadata about a city.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;API Endpoint:&amp;nbsp;&lt;/STRONG&gt;&lt;FONT color="#FF6600"&gt;POST /enrich&lt;/FONT&gt;&amp;nbsp;â€” Accepts a JSON payload with a city name.&lt;/P&gt;&lt;P&gt;&lt;FONT color="#3366FF"&gt;How It Works:&lt;/FONT&gt;&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;Looks up the city in a fake in-memory database.&lt;/LI&gt;&lt;LI&gt;Returns population and country if found.&lt;/LI&gt;&lt;LI&gt;If not found, returns default placeholder values.&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&lt;FONT color="#3366FF"&gt;Tech Stack:&lt;/FONT&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;FastAPI&lt;/LI&gt;&lt;LI&gt;Pydantic&lt;/LI&gt;&lt;/UL&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Data-Enricher")

class EnrichRequest(BaseModel):
    city: str

class EnrichResponse(BaseModel):
    population: int
    country: str

FAKE_DB = {
    "london": {"population": 9_000_000, "country": "UK"},
    "paris":  {"population": 2_100_000, "country": "France"},
    "tokyo":  {"population": 14_000_000, "country": "Japan"},
}

@app.post("/enrich", response_model=EnrichResponse)
def enrich(req: EnrichRequest):
    city = req.city.lower()
    if city not in FAKE_DB:
        return EnrichResponse(population=0, country="Unknown")
    return FAKE_DB[city]&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-1080034119"&gt;&lt;span class="lia-unicode-emoji" title=":desktop_computer:"&gt;ðŸ–¥&lt;/span&gt;ï¸ 3. Running the Services (&lt;FONT color="#FF6600"&gt;run.sh&lt;/FONT&gt;)&lt;/H3&gt;&lt;P&gt;&lt;FONT color="#3366FF"&gt;Purpose:&amp;nbsp;&lt;/FONT&gt;Starts both services using&amp;nbsp;uvicorn, FastAPIâ€™s ASGI server.&lt;BR /&gt;A shell script (&lt;A title="" href="vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/code/electron-browser/workbench/workbench.html" target="_blank" rel="noopener nofollow noreferrer"&gt;run.sh&lt;/A&gt;) is provided to run both services on different ports.&lt;/P&gt;&lt;P&gt;&lt;FONT color="#3366FF"&gt;How It Works&lt;/FONT&gt;:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Launches&amp;nbsp;Fake Weather Agent&amp;nbsp;on port&amp;nbsp;&lt;FONT color="#FF6600"&gt;8002&lt;/FONT&gt;&lt;/LI&gt;&lt;LI&gt;Launches&amp;nbsp;Data Enricher&amp;nbsp;on port&amp;nbsp;&lt;FONT color="#FF6600"&gt;8003&lt;/FONT&gt;&lt;/LI&gt;&lt;LI&gt;Each service runs in its own terminal window&lt;/LI&gt;&lt;/UL&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# Terminal 1
uvicorn fake_weather:app --port 8002 --reload

# Terminal 2
uvicorn data_enricher:app --port 8003 --reload&lt;/code&gt;&lt;/pre&gt;&lt;H2 id="toc-hId-754437895"&gt;Key Points :&amp;nbsp;&lt;/H2&gt;&lt;UL&gt;&lt;LI&gt;Microservice Communication:&lt;BR /&gt;The Weather Agent calls the Data Enricher via HTTP to demonstrate service-to-service communication.&lt;/LI&gt;&lt;LI&gt;Extensibility:&lt;BR /&gt;Easy to add more enrichment services or expand the fake database.&lt;/LI&gt;&lt;LI&gt;FastAPI Features:&lt;BR /&gt;Shows how to use Pydantic models, async endpoints, and response models.&lt;/LI&gt;&lt;LI&gt;Local Development:&amp;nbsp;&amp;nbsp;&lt;BR /&gt;Simple to run both services locally for testing and learning.&lt;/LI&gt;&lt;/UL&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/building-agents-for-a-simple-microservice-architecture-with-fastapi-part-2/ba-p/14176702"/>
    <published>2025-08-10T09:01:21.368000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/enterprise-resource-planning-blog-posts-by-members/easy-way-to-move-zeroes-in-sap-btp-abap-steampunk-js-amp-python/ba-p/14176847</id>
    <title>Easy way to move zeroes in SAP BTP ABAP(Steampunk), JS &amp; Python</title>
    <updated>2025-08-10T15:27:33.552000+02:00</updated>
    <author>
      <name>kallolathome</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/14879</uri>
    </author>
    <content>&lt;H2 id="toc-hId-962976781" id="toc-hId-1737006510"&gt;Introduction&lt;/H2&gt;&lt;P&gt;&lt;SPAN&gt;This is part of the&amp;nbsp;&lt;/SPAN&gt;&lt;A href="https://blogs.sap.com/2022/12/20/easy-way-to-write-algorithms-in-abap-series-01/" target="_blank" rel="noopener noreferrer"&gt;&lt;STRONG&gt;Easy way to write algorithms in ABAP: Series 01&lt;/STRONG&gt;&lt;/A&gt;&lt;SPAN&gt;. For more algorithms, please check the main blog-post.&lt;/SPAN&gt;&lt;/P&gt;&lt;H2 id="toc-hId-766463276" id="toc-hId-1540493005"&gt;Problem&lt;/H2&gt;&lt;P&gt;Given an integer array&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;nums, move all&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;0's to the end of it while maintaining the relative order of the non-zero elements.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Note&lt;/STRONG&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;that you must do this in-place without making a copy of the array.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Example 1:&lt;/STRONG&gt;&lt;/P&gt;&lt;PRE&gt;&lt;STRONG&gt;Input:&lt;/STRONG&gt; nums = [0,1,0,3,12]
&lt;STRONG&gt;Output:&lt;/STRONG&gt; [1,3,12,0,0]&lt;/PRE&gt;&lt;P&gt;&lt;STRONG&gt;Example 2:&lt;/STRONG&gt;&lt;/P&gt;&lt;PRE&gt;&lt;STRONG&gt;Input:&lt;/STRONG&gt; nums = [0]
&lt;STRONG&gt;Output:&lt;/STRONG&gt; [0]&lt;/PRE&gt;&lt;P&gt;&lt;STRONG&gt;Constraints:&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;1 &amp;lt;= nums.length &amp;lt;= 104&lt;/LI&gt;&lt;LI&gt;-231 &amp;lt;= nums[i] &amp;lt;= 231 - 1&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Follow up:&lt;/STRONG&gt;&lt;SPAN&gt;&amp;nbsp;Could you minimize the total number of operations done?&lt;/SPAN&gt;&lt;/P&gt;&lt;H2 id="toc-hId-569949771" id="toc-hId-1343979500"&gt;Solution&lt;/H2&gt;&lt;P&gt;&lt;SPAN&gt;Time Complexity: &lt;STRONG&gt;O(n)&lt;/STRONG&gt;&lt;BR /&gt;Space Complexity: &lt;STRONG&gt;O(1)&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;H3 id="toc-hId-502518985" id="toc-hId-1276548714"&gt;ABAP&lt;/H3&gt;&lt;pre class="lia-code-sample language-abap"&gt;&lt;code&gt;CLASS zmove_zeroes DEFINITION
  PUBLIC
  FINAL
  CREATE PUBLIC .

  PUBLIC SECTION.
    INTERFACES if_oo_adt_classrun.

  PROTECTED SECTION.
  PRIVATE SECTION.
    " Define a table type for integers
    TYPES ty_nums TYPE STANDARD TABLE OF i WITH EMPTY KEY.

    " Method to move zeroes in-place
    METHODS moveZeroes
      CHANGING lt_nums TYPE ty_nums.

ENDCLASS.

CLASS zmove_zeroes IMPLEMENTATION.

  METHOD if_oo_adt_classrun~main.
    " Initialize the number array with some zeroes and non-zeroes
    DATA(lt_nums) = VALUE ty_nums( ( 0 ) ( 1 ) ( 0 ) ( 3 ) ( 12 ) ).

    " Output the array before moving zeroes
    out-&amp;gt;write( |Array before moving zeroes: | ).
    LOOP AT lt_nums INTO DATA(lv_num).
      out-&amp;gt;write( lv_num ).
    ENDLOOP.

    " Call the method to move zeroes to the end
    moveZeroes( CHANGING lt_nums = lt_nums ).

    " Output the array after moving zeroes
    out-&amp;gt;write( |Array after moving zeroes: | ).
    LOOP AT lt_nums INTO lv_num.
      out-&amp;gt;write( lv_num ).
    ENDLOOP.

  ENDMETHOD.

  METHOD moveZeroes.

    DATA(lv_count) = 0. " Counter for non-zero elements

    " First pass: Move all non-zero elements to the front
    LOOP AT lt_nums ASSIGNING FIELD-SYMBOL(&amp;lt;lf_num&amp;gt;).
      IF &amp;lt;lf_num&amp;gt; &amp;lt;&amp;gt; 0.
        " Place the non-zero element at the next available position
        lt_nums[ lv_count + 1 ] = &amp;lt;lf_num&amp;gt;.
        lv_count += 1.
      ENDIF.
    ENDLOOP.

    " Second pass: Fill the rest of the array with zeroes
    WHILE lv_count &amp;lt; lines( lt_nums ).
      lt_nums[ lv_count + 1 ] = 0.
      lv_count += 1.
    ENDWHILE.

  ENDMETHOD.

ENDCLASS.&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-306005480" id="toc-hId-1080035209"&gt;JavaScript&lt;/H3&gt;&lt;pre class="lia-code-sample language-javascript"&gt;&lt;code&gt;function moveZeroes(nums) {
    let left = 0;
    for (let right = 0; right &amp;lt; nums.length; right++) {
        if (nums[right] !== 0) {
            // Swap nums[left] and nums[right]
            let temp = nums[left];
            nums[left] = nums[right];
            nums[right] = temp;
            left++;
        }
    }
    return nums;
}&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-883521704"&gt;&amp;nbsp;&lt;/H3&gt;&lt;H3 id="toc-hId-502518985" id="toc-hId-687008199"&gt;Python&lt;/H3&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;def moveZeroes(nums):
    left = 0
    for right in range(len(nums)):
        if nums[right] != 0:
            nums[left], nums[right] = nums[right], nums[left]
            left += 1
    return nums&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;N.B: For ABAP, I am using SAP BTP ABAP Environment 2309 Release.&lt;/SPAN&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;SPAN&gt;Happy Coding!&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN class="lia-unicode-emoji"&gt;&lt;span class="lia-unicode-emoji" title=":slightly_smiling_face:"&gt;ðŸ™‚&lt;/span&gt;&lt;/SPAN&gt;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/enterprise-resource-planning-blog-posts-by-members/easy-way-to-move-zeroes-in-sap-btp-abap-steampunk-js-amp-python/ba-p/14176847"/>
    <published>2025-08-10T15:27:33.552000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/sap-datasphere-automation-creating-database-user-in-sap-datasphere-using/ba-p/14176606</id>
    <title>SAP Datasphere Automation : Creating Database user in SAP Datasphere using Datasphere CLI &amp; Python</title>
    <updated>2025-08-12T08:06:51.294000+02:00</updated>
    <author>
      <name>shubham521</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/845865</uri>
    </author>
    <content>&lt;H3 id="toc-hId-1866087182"&gt;Introduction&lt;/H3&gt;&lt;P&gt;One way to access datasphere hana database is via database users. Each database user is linked to one space (except database analysis user). We can create database user vai GUI however, its a long process and prone to errors. In this blog, i will share my work on how i automated the process using SAP Datasphere CLI and Python without the need to login into datasphere GUI.&lt;/P&gt;&lt;H3 id="toc-hId-1669573677"&gt;Old Process:&lt;/H3&gt;&lt;P&gt;If you want to create a database user via GUI, you need to perform the below steps.&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;Login into the datasphere tenant. Make sure you have the required application roles to perform the task.&lt;/LI&gt;&lt;LI&gt;Go to Space management and select the space. Scroll down and click create in database user.&amp;nbsp;&lt;/LI&gt;&lt;LI&gt;Provide the username and deploy the space. Save the password and share it with the user.&amp;nbsp;&lt;/LI&gt;&lt;/OL&gt;&lt;H3 id="toc-hId-1473060172"&gt;Automated Process&lt;/H3&gt;&lt;P&gt;To overcome the long process, i have divided the python script to perform all these task in sequential manner.&amp;nbsp;&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;&lt;STRONG&gt;Datasphere CLI Setup&lt;/STRONG&gt; : Set the CLI and login into datasphere&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Reading the metadata&lt;/STRONG&gt;: Read the space metadata using SAP Datasphere CLI &lt;STRONG&gt;space read&lt;/STRONG&gt; command&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Modifying JSON&lt;/STRONG&gt; : Enhance the output JSON with the new user details and deploy it using the &lt;STRONG&gt;space create&lt;/STRONG&gt; command&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Password Reset&lt;/STRONG&gt; : Reset the database user password and save it in a file for sharing.&lt;/LI&gt;&lt;/OL&gt;&lt;H3 id="toc-hId-1276546667"&gt;Set up SAP Datasphere CLI (First time only)&lt;/H3&gt;&lt;P&gt;To start working with Datasphere CLI, we need to perform few one time configuration like setting host, login in and setting cache. If you want to login via a different tenant, you need to change the CLIENT_ID and CLIENT_SECRETS as per the tenant.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;datasphere config set host {"host_url"}
datasphere login --client-id {"client_id"} --client-secret {"client_secret"}
datasphere config set cache --client-id {"client_id"} --client-secret {"client_secret"}&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;Once you are done with the initial setup of CLI, we can start working on the main code which will create database users.&lt;/P&gt;&lt;H3 id="toc-hId-1080033162"&gt;Reading space metadata&lt;/H3&gt;&lt;P&gt;Since we do not have a direct command to create DB user, we extract the space metadata JSON using CLI. This command output the space metadata JSON into console. I have stored this into a variable that i will use later in modifying the JSON&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;space_Json = datasphere space read -space "{space"} --definations&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-883519657"&gt;Modifying the JSON and deploying it in datasphere&amp;nbsp;&lt;/H3&gt;&lt;P&gt;To create a new databased user in space, we need to add the new user details in the dbuser object of the JSON. We can store the space metadata JSON into our local machine and modify it or we can modify it on the go using tempfiles. I have used the later approach as it eliminates the need of JSON management&amp;nbsp;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;#Concatenating the space and username
new_db_user = f"{space}#{username}"

#Appending the new_db_user details into the space metadata JSON
space_JSON[space]["spaceDefinition"]["dbusers"][new_db_user] = {
"ingestion":{
        "auditing":{
          "dppRead":{
            "retentionPeriod":21
            "isAuditPolicyActive":True
          },
         
        }
      },
      "consumption":{
        "consumptionWithGrant":false,
        "spaceSchemaAccess":True,
        "scriptServerAccess":false,
        "localSchemaAccess":false,
        "hdiGrantorForCupsAccess":false
}&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;Once the JSON is modified, write the new JSON into a temporary file. I have stored the file path into a variable called tmp_file_path. I will push this file path to datasphere tenant.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;#Pushing the modified JSON to datasphere tenant
datasphere space create --file-path "{tmp_file_path}" -- force-defination-deployment &lt;/code&gt;&lt;/pre&gt;&lt;P&gt;If you want to delete a database user, remove the entry from the JSON and run the same command and add&amp;nbsp;&lt;SPAN&gt;&lt;STRONG&gt;--enforce-database-user-deletion&lt;/STRONG&gt; in the end. If this command is not added, then the deletion will not work.&lt;/SPAN&gt;&lt;/P&gt;&lt;H3 id="toc-hId-687006152"&gt;Password Reset&lt;/H3&gt;&lt;P&gt;Once the database user is created, we need to reset the password to store it in a local file for sharing. Below is the command to perform that action&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;datasphere dbusers password reset --space{"space"} --databaseuser {"new_db_user"}&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;(&lt;STRONG&gt;Tip: Add a 30 seconds wait time between creating the database user and resetting the password commands because the space deployment takes some time and we cannot reset the password before that&lt;/STRONG&gt;).&lt;/P&gt;&lt;P&gt;I have also added an automatic email creation in my workflow and inserting database user details into a local table for future analysis.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Screenshot 2025-08-09 at 20.56.29.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/298901iDFDFD74E03AF67EA/image-size/large?v=v2&amp;amp;px=999" role="button" title="Screenshot 2025-08-09 at 20.56.29.png" alt="Screenshot 2025-08-09 at 20.56.29.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId-490492647"&gt;Conclusion:&lt;/H3&gt;&lt;P&gt;With this automation, we can create database users in datasphere tenant without even login into datasphere. This workflow provides a more robust way to handle creation and maintenance database users.&amp;nbsp;&lt;/P&gt;&lt;P&gt;I would love to know your thoughts on this workflow in the comments below. Lets explore more opportunities of automation using datasphere CLI.&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/sap-datasphere-automation-creating-database-user-in-sap-datasphere-using/ba-p/14176606"/>
    <published>2025-08-12T08:06:51.294000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/predictive-stock-transfer-amp-automatic-purchase-re-order-plant-to-plant/ba-p/14170063</id>
    <title>Predictive Stock Transfer &amp; Automatic Purchase Re-Order: Plant-to-Plant A2A Orchestration</title>
    <updated>2025-08-16T14:04:54.954000+02:00</updated>
    <author>
      <name>Yogananda</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/75</uri>
    </author>
    <content>&lt;P&gt;&lt;STRONG&gt;Use-case: Predictive Stock Transfer &amp;amp; Automatic Purchase Re-Order (Plant-to-Plant A2A scenario driven by real-time APIs)&lt;/STRONG&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Yogananda_0-1755345825205.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/301623i7ED9C4CC9FB704BB/image-size/large?v=v2&amp;amp;px=999" role="button" title="Yogananda_0-1755345825205.png" alt="Yogananda_0-1755345825205.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;A fast-moving material in Plant A is kept in stock by an end-to-end, automated flow that &lt;/SPAN&gt;&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;&lt;SPAN&gt;checks forecast demand, &lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;looks for internal surplus in nearby plants, and &lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;automatically creates stock transfers or purchase requisitions as needed. &lt;/SPAN&gt;&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&lt;STRONG&gt;Prerequisities and needed&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;SAP S/4HANA Cloud (Inventory &amp;amp; MRP) â€“ On-hand stock, stock in transit, MRP items&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;SAP Integrated Business Planning (IBP) â€“ Demand forecast for FG-100 at Plant A&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;SAP Extended Warehouse Management (EWM) â€“ Real-time on-hand stock incl. quarantine&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;SAP Ariba â€“ Supplier catalog and pricing for external options&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;External supplier catalog (Ariba-like) â€“ External pricing and lead times&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;SPAN&gt;SAP Integration Suite / SAP Event Mesh â€“ Orchestrates the end-to-end flow and publishes events&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Sequence â€“ how the APIs interact end-to-end&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 0 â€“ Trigger&lt;/STRONG&gt;&lt;BR /&gt;A nightly iFlow in SAP Integration Suite (or SAP Event Mesh) starts the orchestration.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 1 â€“ Demand forecast&lt;/STRONG&gt;&lt;BR /&gt;GET /api/ibp/v1/demandplanning/forecast?material=FG-100&amp;amp;plant=A&amp;amp;weeks=4&lt;BR /&gt;â†’ Returns 1,200 pcs forecasted demand.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 2 â€“ Current &amp;amp; projected stock in Plant A&lt;/STRONG&gt;&lt;BR /&gt;GET /sap/opu/odata/sap/API_MATERIAL_STOCK_SRV/MaterialStock?material=FG-100&amp;amp;plant=A&lt;BR /&gt;â†’ 180 pcs unrestricted, 40 pcs blocked.&lt;BR /&gt;GET /sap/opu/odata/sap/API_MRP_COCKPIT_SRV/MrpItems?material=FG-100&amp;amp;plant=A&lt;BR /&gt;&lt;SPAN&gt;Result: confirmed receipts 600 pcs&lt;/SPAN&gt;&lt;BR /&gt;â†’ Confirmed receipts 600 pcs, so net shortage = 1,200 â€“ 180 â€“ 600 = 420 pcs.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 3 â€“ Locate surplus stock in the network&lt;/STRONG&gt;&lt;BR /&gt;Parallel calls (one per plant):&lt;BR /&gt;GET /sap/opu/odata/sap/API_MATERIAL_STOCK_SRV/MaterialStock?material=FG-100&amp;amp;plant=B&lt;BR /&gt;â†’ 300 pcs unrestricted.&lt;BR /&gt;GET /sap/opu/odata/sap/API_MATERIAL_STOCK_SRV/MaterialStock?material=FG-100&amp;amp;plant=C&lt;BR /&gt;â†’ 250 pcs unrestricted.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 4 â€“ Check ATP (available-to-transfer) incl. transit time&lt;/STRONG&gt;&lt;BR /&gt;POST /sap/opu/odata/sap/API_ATP_CHECK_SRV/CheckAvailability&lt;BR /&gt;Body:&lt;/P&gt;&lt;pre class="lia-code-sample language-json"&gt;&lt;code&gt;{ 
"material": "FG-100", 
"plant": "B", 
"demandQty": 300, 
"requiredDate": "2024-06-25" 
}&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;â†’ Confirms 300 pcs can be delivered by 2024-06-23.&lt;BR /&gt;Same for Plant C â†’ 120 pcs available by 2024-06-24.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 5 â€“ Decide cheapest internal option&lt;/STRONG&gt;&lt;BR /&gt;Cost service (custom REST on S/4):&lt;BR /&gt;POST /internal/transferCost&lt;/P&gt;&lt;pre class="lia-code-sample language-json"&gt;&lt;code&gt;{ 
"fromPlants": ["B","C"], 
"toPlant": "A", 
"qty": [300,120] 
}&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;BR /&gt;â†’ Plant B has the lowest freight cost (â‚¬0.05/pc vs â‚¬0.07/pc).&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 6 â€“ Create stock transport order (STO)&lt;/STRONG&gt;&lt;BR /&gt;POST /sap/opu/odata/sap/API_STOCK_TRANSPORT_ORDER_SRV/A_StockTransportOrder&lt;BR /&gt;Body:&lt;/P&gt;&lt;pre class="lia-code-sample language-json"&gt;&lt;code&gt;{ 
"SupplyingPlant": "B", 
"ReceivingPlant": "A", 
"Material": "FG-100", 
"OrderQuantity": 300, 
"DeliveryDate": "2024-06-23" 
}&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;â†’ STO 4500012345 created.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 7 â€“ Remaining uncovered quantity&lt;/STRONG&gt;&lt;BR /&gt;Shortage after internal transfer = 420 â€“ 300 = 120 pcs.&lt;BR /&gt;Call Ariba to get best external price:&lt;BR /&gt;GET /v2/suppliers/catalog?material=FG-100&amp;amp;qty=120&amp;amp;currency=EUR&lt;BR /&gt;â†’ Supplier S-987 offers â‚¬2.30/pc, lead time 7 days.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 8 â€“ Create purchase requisition in S/4&lt;/STRONG&gt;&lt;BR /&gt;POST /sap/opu/odata/sap/API_PURCHASEREQ_PROCESS_SRV/A_PurchaseRequisitionHeader&lt;BR /&gt;Body:&lt;/P&gt;&lt;pre class="lia-code-sample language-json"&gt;&lt;code&gt;{ 
"Material": "FG-100", 
"Plant": "A", 
"Quantity": 120, 
"DeliveryDate": "2024-06-27", 
"SupplierHint": "S-987" 
}&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;â†’ PR 1000123456 created.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Step 9 â€“ Notify planners&lt;/STRONG&gt;&lt;BR /&gt;Publish event to SAP Event Mesh topic /business/plantA/stockReplenished&lt;BR /&gt;Payload:&lt;/P&gt;&lt;pre class="lia-code-sample language-json"&gt;&lt;code&gt;{ 
"material": "FG-100", 
"sto": "4500012345", 
"pr": "1000123456", 
"status": "covered" 
}&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;STRONG&gt;Outcome&lt;/STRONG&gt;&lt;BR /&gt;Plant A will receive 300 pcs from Plant B via an automatically created STO and 120 pcs via a purchase requisition with the cheapest external supplierâ€”no manual intervention, no stock-out, and minimal freight cost.&lt;/P&gt;&lt;P&gt;complete code&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;#!/usr/bin/env python3
"""
End-to-end A2A flow:
1. Read demand forecast (IBP)
2. Check stock &amp;amp; MRP in Plant A
3. Search surplus stock in Plants B/C
4. Run ATP check
5. Create STO (cheapest internal)
6. Create PR for remaining qty (Ariba best price)
"""

import os, json, math
from datetime import datetime, timedelta
from dotenv import load_dotenv
from requests import Session
from requests_oauthlib import OAuth2Session
from oauthlib.oauth2 import BackendApplicationClient

load_dotenv()

# ---------- CONFIG ----------
MATERIAL = "FG-100"
PLANT_A  = "A"
PLANTS_SURPLUS = ["B", "C"]
DEMAND_WEEKS   = 4
TRANSPORT_DAYS = 2
# ----------------------------

s = Session()

# ---------- 0.  OAUTH TOKEN for S/4 ----------
def s4_token():
    url = f"{os.getenv('S4_HOST')}/oauth/token"
    r = s.post(url,
               auth=(os.getenv('S4_USER'), os.getenv('S4_PASSWORD')),
               data={'grant_type':'client_credentials'})
    r.raise_for_status()
    return r.json()['access_token']

s.headers.update({'Authorization': f"Bearer {s4_token()}"})

# ---------- 1.  IBP â€“ demand forecast ----------
def ibp_forecast():
    url = (f"{os.getenv('S4_HOST')}/sap/opu/odata/sap/"
           f"API_DEMAND_PLANNING_SRV/DemandForecast")
    params = {
        "$filter": f"Material eq '{MATERIAL}' and Plant eq '{PLANT_A}'",
        "$select": "DemandQuantity",
        "$format": "json"
    }
    r = s.get(url, params=params)
    r.raise_for_status()
    total = sum(item['DemandQuantity'] for item in r.json()['d']['results'])
    return total

demand_qty = ibp_forecast()
print("Demand forecast:", demand_qty)

# ---------- 2.  Stock &amp;amp; MRP ----------
def plant_stock(plant):
    url = (f"{os.getenv('S4_HOST')}/sap/opu/odata/sap/"
           f"API_MATERIAL_STOCK_SRV/MaterialStock")
    params = {
        "$filter": f"Material eq '{MATERIAL}' and Plant eq '{plant}' and "
                   f"StorageLocation ne ''",
        "$format": "json"
    }
    r = s.get(url, params=params)
    r.raise_for_status()
    return sum(item['UnrestrictedStockQuantity']
               for item in r.json()['d']['results'])

def plant_receipts(plant):
    url = (f"{os.getenv('S4_HOST')}/sap/opu/odata/sap/"
           f"API_MRP_COCKPIT_SRV/MrpItems")
    params = {
        "$filter": f"Material eq '{MATERIAL}' and Plant eq '{plant}' and "
                   f"MrpElementCategory eq 'AR'",
        "$format": "json"
    }
    r = s.get(url, params=params)
    r.raise_for_status()
    return sum(item['Quantity'] for item in r.json()['d']['results'])

stock_A = plant_stock(PLANT_A)
receipts_A = plant_receipts(PLANT_A)
shortage = max(0, demand_qty - stock_A - receipts_A)
print("Shortage:", shortage)
if shortage == 0:
    print("No action needed.")
    exit()

# ---------- 3.  Surplus stock ----------
surplus = {}
for p in PLANTS_SURPLUS:
    surplus[p] = plant_stock(p)
print("Surplus:", surplus)

# ---------- 4.  ATP check ----------
def atp_ok(plant, qty, req_date):
    url = (f"{os.getenv('S4_HOST')}/sap/opu/odata/sap/"
           f"API_ATP_CHECK_SRV/CheckAvailability")
    body = {
        "Material": MATERIAL,
        "Plant": plant,
        "DemandQuantity": str(qty),
        "RequiredDate": req_date.isoformat()
    }
    r = s.post(url, json=body)
    r.raise_for_status()
    return r.json()['d']['ConfirmedQuantity'] == str(qty)

best_internal = None
needed = shortage
for plant, qty in surplus.items():
    take = min(qty, needed)
    req = datetime.utcnow().date() + timedelta(days=TRANSPORT_DAYS)
    if atp_ok(plant, take, req):
        best_internal = (plant, take)
        break
if best_internal:
    plant, qty = best_internal
    print(f"Best internal: {qty} from {plant}")

    # ---------- 5.  Create STO ----------
    url = (f"{os.getenv('S4_HOST')}/sap/opu/odata/sap/"
           f"API_STOCK_TRANSPORT_ORDER_SRV/A_StockTransportOrder")
    body = {
        "SupplyingPlant": plant,
        "ReceivingPlant": PLANT_A,
        "Material": MATERIAL,
        "OrderQuantity": str(qty),
        "DeliveryDate": req.isoformat()
    }
    r = s.post(url, json=body)
    r.raise_for_status()
    sto = r.json()['d']['StockTransportOrder']
    print("STO created:", sto)
    shortage -= qty

# ---------- 6.  Ariba â€“ best external price ----------
if shortage &amp;gt; 0:
    client = BackendApplicationClient(client_id=os.getenv('ARIBA_CLIENT_ID'))
    oauth = OAuth2Session(client=client)
    token = oauth.fetch_token(
        token_url=os.getenv('ARIBA_TOKEN_URL'),
        client_id=os.getenv('ARIBA_CLIENT_ID'),
        client_secret=os.getenv('ARIBA_CLIENT_SECRET')
    )
    url = "https://api.ariba.com/v2/suppliers/catalog"
    params = {
        "material": MATERIAL,
        "qty": shortage,
        "currency": "EUR"
    }
    r = oauth.get(url, params=params)
    r.raise_for_status()
    best = min(r.json()['offers'], key=lambda x: float(x['price']))
    print("Best supplier:", best['supplier'], best['price'])

    # ---------- 7.  Create PR ----------
    url = (f"{os.getenv('S4_HOST')}/sap/opu/odata/sap/"
           f"API_PURCHASEREQ_PROCESS_SRV/A_PurchaseRequisitionHeader")
    body = {
        "Material": MATERIAL,
        "Plant": PLANT_A,
        "Quantity": str(shortage),
        "DeliveryDate": (datetime.utcnow().date()
                         + timedelta(days=int(best['leadtime']))).isoformat(),
        "SupplierHint": best['supplier']
    }
    r = s.post(url, json=body)
    r.raise_for_status()
    pr = r.json()['d']['PurchaseRequisition']
    print("PR created:", pr)

print("Flow finished.")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/predictive-stock-transfer-amp-automatic-purchase-re-order-plant-to-plant/ba-p/14170063"/>
    <published>2025-08-16T14:04:54.954000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/sap-datasphere-amp-python-one-click-to-export-data-of-multiple-views-in/ba-p/14180664</id>
    <title>SAP Datasphere &amp; Python : One click to export data of multiple views in Excel/CSV</title>
    <updated>2025-08-19T08:22:39.531000+02:00</updated>
    <author>
      <name>vikasparmar88</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1528256</uri>
    </author>
    <content>&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Introduction&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Currently, SAP Datasphere only allows data export through Analytical Models. That means for every fact view, one has to create a separate Analytical Model just to download the data. Itâ€™s not ideal, especially when you have many views. Exporting them one by one becomes slow and repetitive.&lt;/P&gt;&lt;P&gt;To solve this,&amp;nbsp; Python script was developed that connects to SAP Datasphere, runs a query on each view from list, and saves the results as Excel files in a local drive path which is predefined. Now&amp;nbsp; just run the script, and it exports everything in one goâ€”no manual effort needed.&lt;/P&gt;&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;Requirements&amp;nbsp;&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;Before running the script, install the following Python packages:&lt;/P&gt;&lt;P&gt;These packages enable secure connectivity to SAP Datasphere and support efficient data handling.&lt;/P&gt;&lt;PRE&gt;pip install hdbcli
pip install sqlalchemy
pip install sqlalchemy-hana&lt;/PRE&gt;&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;Creating a Database User in SAP Datasphere&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;To enable Python connectivity, create a database user:&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;Navigate to&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;STRONG&gt;Space Management&lt;/STRONG&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;in SAP Datasphere&lt;/LI&gt;&lt;LI&gt;Select the relevant space&lt;/LI&gt;&lt;LI&gt;Click on&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;STRONG&gt;Database Access&lt;/STRONG&gt;&lt;/LI&gt;&lt;LI&gt;Create a new user with read access&lt;/LI&gt;&lt;LI&gt;Copy the host, port, username, and password&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;Helpful links :&amp;nbsp;&lt;A title="Create DB User in Datasphere Space" href="https://developers.sap.com/tutorials/data-warehouse-cloud-intro8-create-databaseuser..html" target="_blank" rel="noopener noreferrer"&gt;Create DB User in Datasphere Space&lt;/A&gt;&amp;nbsp;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Python&amp;nbsp;Script&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;PRE&gt;# &lt;span class="lia-unicode-emoji" title=":package:"&gt;ðŸ“¦&lt;/span&gt; Install required packages (run these in your terminal or notebook)
# pip install hdbcli              # SAP HANA database client
# pip install sqlalchemy          # SQL toolkit and ORM for Python
# pip install sqlalchemy-hana     # SAP HANA dialect for SQLAlchemy

# &lt;span class="lia-unicode-emoji" title=":books:"&gt;ðŸ“š&lt;/span&gt; Import necessary libraries
import pandas as pd               # For data manipulation and Excel export
from hdbcli import dbapi          # SAP HANA DBAPI for direct connection
import warnings                   # To suppress unnecessary warnings
import os                         # For file path and directory handling

# &lt;span class="lia-unicode-emoji" title=":prohibited:"&gt;ðŸš«&lt;/span&gt; Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# &lt;span class="lia-unicode-emoji" title=":locked_with_key:"&gt;ðŸ”&lt;/span&gt; Define SAP Datasphere connection parameters
# &lt;span class="lia-unicode-emoji" title=":backhand_index_pointing_right:"&gt;ðŸ‘‰&lt;/span&gt; Replace the placeholders below with your actual connection details
db_user = '&amp;lt;your_database_user&amp;gt;'           # User with access to target schema
db_password = '&amp;lt;your_secure_password&amp;gt;'     # Password (handle securely)
db_host = '&amp;lt;your_datasphere_host_url&amp;gt;'     # Host URL (e.g., xyz.hanacloud.ondemand.com)
db_port = 443                               # Default HTTPS port for SAP HANA Cloud
db_schema = '&amp;lt;your_schema_name&amp;gt;'           # Target schema containing views

# &lt;span class="lia-unicode-emoji" title=":file_folder:"&gt;ðŸ“&lt;/span&gt; Ensure output folder exists for Excel exports
output_folder = r'C:\Datasphere\Excel export'  # Update path as needed
os.makedirs(output_folder, exist_ok=True)

# &lt;span class="lia-unicode-emoji" title=":clipboard:"&gt;ðŸ“‹&lt;/span&gt; Define list of views to extract data from
# &lt;span class="lia-unicode-emoji" title=":backhand_index_pointing_right:"&gt;ðŸ‘‰&lt;/span&gt; Add or modify view names based on your schema
view_list = ['VIEW_1', 'VIEW_2']  # Example views

try:
    # &lt;span class="lia-unicode-emoji" title=":globe_with_meridians:"&gt;ðŸŒ&lt;/span&gt; Establish secure connection to SAP Datasphere
    connection = dbapi.connect(
        address=db_host,
        port=db_port,
        user=db_user,
        password=db_password,
        encrypt=True,
        sslValidateCertificate=True
    )
    print("&lt;span class="lia-unicode-emoji" title=":white_heavy_check_mark:"&gt;âœ…&lt;/span&gt; Connected to SAP Datasphere")

    cursor = connection.cursor()

    # &lt;span class="lia-unicode-emoji" title=":repeat_button:"&gt;ðŸ”&lt;/span&gt; Loop through each view and export its data
    for view_name in view_list:
        try:
            # &lt;span class="lia-unicode-emoji" title=":bar_chart:"&gt;ðŸ“Š&lt;/span&gt; Construct and execute SQL query
            sql_query = f'SELECT * FROM "{db_schema}"."{view_name}"'
            print(f"&lt;span class="lia-unicode-emoji" title=":bar_chart:"&gt;ðŸ“Š&lt;/span&gt; Executing query: {sql_query}")
            cursor.execute(sql_query)

            # &lt;span class="lia-unicode-emoji" title=":inbox_tray:"&gt;ðŸ“¥&lt;/span&gt; Fetch results and convert to DataFrame
            rows = cursor.fetchall()
            columns = [desc[0] for desc in cursor.description]
            df = pd.DataFrame(rows, columns=columns)

            # &lt;span class="lia-unicode-emoji" title=":outbox_tray:"&gt;ðŸ“¤&lt;/span&gt; Export DataFrame to Excel
            output_path = os.path.join(output_folder, f'{view_name}.xlsx')
            df.to_excel(output_path, index=False)
            print(f"&lt;span class="lia-unicode-emoji" title=":white_heavy_check_mark:"&gt;âœ…&lt;/span&gt; Data from '{view_name}' saved to: {output_path}")

        except dbapi.Error as view_err:
            print(f"&lt;span class="lia-unicode-emoji" title=":cross_mark:"&gt;âŒ&lt;/span&gt; Error querying '{view_name}': {view_err}")

except dbapi.Error as db_err:
    print(f"&lt;span class="lia-unicode-emoji" title=":cross_mark:"&gt;âŒ&lt;/span&gt; Database error: {db_err}")
except Exception as ex:
    print(f"&lt;span class="lia-unicode-emoji" title=":warning:"&gt;âš ï¸&lt;/span&gt; Unexpected error: {ex}")
finally:
    # &lt;span class="lia-unicode-emoji" title=":locked:"&gt;ðŸ”’&lt;/span&gt; Ensure connection is closed gracefully
    if 'connection' in locals():
        connection.close()
        print("&lt;span class="lia-unicode-emoji" title=":locked:"&gt;ðŸ”’&lt;/span&gt; Connection closed")&lt;/PRE&gt;&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Script Capabilities&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Establishes secure connection to SAP Datasphere&lt;/LI&gt;&lt;LI&gt;Executes queries on each listed view&lt;/LI&gt;&lt;LI&gt;Saves data from each view into a separate Excel file&lt;/LI&gt;&lt;LI&gt;Stores all files in a defined folder&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;Only connection details and view names need to be updated. The script handles the rest.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;FONT size="5"&gt;&lt;STRONG&gt;One-Click Execution with a .bat File&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;To simplify execution, create a&amp;nbsp;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;STRONG&gt;RunExport.bat&lt;/STRONG&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;file to run the Python script with a double-click.&lt;/P&gt;&lt;P&gt;Double-clicking the file will automatically export all views to Excel without opening a terminal&lt;/P&gt;&lt;PRE&gt; off
REM Activate Python and run the Export script

REM Change to the script directory
cd /d "C:\Datasphere\Excel export"

REM Run the Python script
python Export.py

REM Pause to keep the window open (optional)
pause&lt;/PRE&gt;&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;Example&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="4"&gt;Before Execution&lt;/FONT&gt;&lt;/STRONG&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="11.jpeg" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/300844iD619132276E2474C/image-size/large?v=v2&amp;amp;px=999" role="button" title="11.jpeg" alt="11.jpeg" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Double Click on "RunExcel.bat" file&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="2.jpeg" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/300845iCD8108A14F47A5B1/image-size/large?v=v2&amp;amp;px=999" role="button" title="2.jpeg" alt="2.jpeg" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;SPAN&gt;Post Execution&lt;/SPAN&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="3.jpg" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/300847iD739F0EA7A4E9615/image-size/large?v=v2&amp;amp;px=999" role="button" title="3.jpg" alt="3.jpg" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;Conclusion&lt;/STRONG&gt;&lt;/FONT&gt;&lt;BR /&gt;This automation simplifies data exports from SAP Datasphere, especially when working with multiple views.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;It reduces manual effort&lt;/LI&gt;&lt;LI&gt;improves consistency and saves time.&lt;/LI&gt;&lt;LI&gt;ideal for recurring tasks or scheduled jobs.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;For setup support or customization, feel free to connect.&lt;/P&gt;&lt;P&gt;Thanks&lt;/P&gt;&lt;P&gt;Vikas Parmar&lt;/P&gt;&lt;P&gt;&lt;a href="https://community.sap.com/t5/c-khhcw49343/SAP+Datasphere/pd-p/73555000100800002141" class="lia-product-mention" data-product="16-1"&gt;SAP Datasphere&lt;/a&gt;&amp;nbsp;&lt;a href="https://community.sap.com/t5/c-khhcw49343/SAP+Business+Data+Cloud/pd-p/73554900100700003531" class="lia-product-mention" data-product="1249-1"&gt;SAP Business Data Cloud&lt;/a&gt;&amp;nbsp;&lt;a href="https://community.sap.com/t5/c-khhcw49343/Python/pd-p/f220d74d-56e2-487e-8e6c-a8cb3def2378" class="lia-product-mention" data-product="126-1"&gt;Python&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/sap-datasphere-amp-python-one-click-to-export-data-of-multiple-views-in/ba-p/14180664"/>
    <published>2025-08-19T08:22:39.531000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-in-sap-business-data-cloud-a-typical-machine-learning/ba-p/14206612</id>
    <title>SAP Databricks in SAP Business Data Cloud â€“ a Typical Machine Learning Workflow</title>
    <updated>2025-09-04T04:16:17.227000+02:00</updated>
    <author>
      <name>js2</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/41060</uri>
    </author>
    <content>&lt;P&gt;With SAP Databricks you have access to an amazing set of capabilities to work with your BDC Data Products and other data.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_0-1756949550337.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308823i50F7383D319ECA1F/image-size/large?v=v2&amp;amp;px=999" role="button" title="js2_0-1756949550337.png" alt="js2_0-1756949550337.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;This blog post is part of a series exploring SAP Databricks in SAP Business Data Cloud:&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-1/ba-p/14166813" target="_self"&gt;&lt;SPAN&gt;Part 1 â€“ SQL analytics with SAP Data Products&lt;BR /&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-2/ba-p/14167025" target="_self"&gt;Part 2 â€“ Build and deploy Mosaic AI and Agent Tools&lt;/A&gt;&lt;BR /&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-how-to-use-automl-to-forecast-sales-data-part-3/ba-p/14174354" target="_self"&gt;Part 3 â€“ How to use AutoML to forecast sales data&lt;/A&gt;&lt;BR /&gt;&lt;/SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-3/ba-p/14174201" target="_self"&gt;&lt;SPAN&gt;Part 4 â€“ Connect SAP Data Products with non-SAP data from AWS S3&lt;BR /&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-building-an-intelligent-enterprise-with-ai-unleashed-part-4/ba-p/14178056" target="_self"&gt;Part 5 â€“ End-to-end integration: SAP Databricks, SAP Datasphere, and SAP Analytics Cloud&lt;/A&gt;&lt;BR /&gt;&lt;/SPAN&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-create-inferences-for-application-integration-with-sap-build/ba-p/14186662" target="_self"&gt;Part 6 â€“ Create inferences for application integration with SAP Build&amp;nbsp;&lt;/A&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Part 7 -&amp;nbsp;SAP Databricks in SAP Business Data Cloud â€“ a Typical Machine Learning Workflow&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;In this blog post weâ€™ll look at the typical workflow you would undertake when trying to train a machine learning model.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Visualise and understand your data&lt;/LI&gt;&lt;LI&gt;Optimise for hyperparameters to tune your model&lt;/LI&gt;&lt;LI&gt;Explore hyperparameter sweep results with MLflow&lt;/LI&gt;&lt;LI&gt;Register the best performing model in MLflow&lt;/LI&gt;&lt;LI&gt;Apply the registered model with batch inference and Databricks Model Serving&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;Weâ€™ll use a classic machine learning dataset to predict whether a wine is of high quality or not (&lt;STRONG&gt;&lt;EM&gt;a data classification problem&lt;/EM&gt;&lt;/STRONG&gt;). Of course you have access to a range of SAP Data Products, but by using this dataset you donâ€™t even need a connected S/4HANA system to follow along. The dataset also comes built-in with SAP Databricks.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-1759168994"&gt;Wine Quality Classification&lt;/H2&gt;&lt;P&gt;We will train a binary classification model to predict the quality of Portuguese "Vinho Verde" wine based on the wine's physicochemical properties.&lt;/P&gt;&lt;P&gt;The dataset is from the UCI Machine Learning Repository, presented in Modelling wine preferences by data mining from physicochemical properties [Cortez et al., 2009]. And the good news is that this dataset comes preloaded with your Databricks system.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Letâ€™s create a new notebook in our SAP Databricks system and in a new cell we will install some module dependencies.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;%pip install --upgrade -Uqqq mlflow&amp;gt;=3.0 xgboost hyperopt
%restart_python&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;Installs:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;The latest &lt;STRONG&gt;&lt;A href="https://mlflow.org/" target="_blank" rel="nofollow noopener noreferrer"&gt;mlflow&lt;/A&gt;&lt;/STRONG&gt; for experiment tracking and general MLOps&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;A href="https://xgboost.readthedocs.io/en/stable/" target="_blank" rel="nofollow noopener noreferrer"&gt;xgboost&lt;/A&gt;&lt;/STRONG&gt; being a fantastic and very popular machine learning model architecture (it dominates many Kaggle competitions)&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;A href="https://hyperopt.github.io/hyperopt/" target="_blank" rel="nofollow noopener noreferrer"&gt;hyperopt&lt;/A&gt;&lt;/STRONG&gt; is a python library used for hyperparameter optimisation. It intelligently searches for the optimal hyperparameters to use when training a machine learning model.&lt;/LI&gt;&lt;LI&gt;The `&lt;STRONG&gt;%restart_python&lt;/STRONG&gt;` magic command it necessary in Databricks notebooks because they use long running Python processes and this ensures that the system path and any newly installed python packages are being used.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Next, we create a cell to connect MLFlow to the Databricks Unity Catalog (it would otherwise use an sqlite data store) and create a few constants that will be used later:&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import mlflow
mlflow.set_registry_uri("databricks-uc")

CATALOG_NAME = "workspace"
SCHEMA_NAME = "default"
MODEL_NAME = "wine_quality_classifier"&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-1562655489"&gt;Read and Understand the DATA&lt;/H2&gt;&lt;P&gt;Read the white wine quality and red wine quality CSV datasets and merge them into a single DataFrame. &lt;EM&gt;Note theses datasets come with your Databricks system&lt;/EM&gt;.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import pandas as pd

white_wine = pd.read_csv("/databricks-datasets/wine-quality/winequality-white.csv", sep=";")
red_wine = pd.read_csv("/databricks-datasets/wine-quality/winequality-red.csv", sep=";")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Merge the two DataFrames into a single dataset, with a new binary feature "is_red" that indicates whether the wine is red or white.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;red_wine['is_red'] = 1
white_wine['is_red'] = 0

data = pd.concat([red_wine, white_wine], axis=0)

# cast to float as a best practice (avoids dtype issues with NaN's later)
data["is_red"] = data["is_red"].astype("float32")

# Remove spaces from column names
data.rename(columns=lambda x: x.replace(' ', '_'), inplace=True)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_1-1756949711124.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308824i39D3D0E1BF31443D/image-size/large?v=v2&amp;amp;px=999" role="button" title="js2_1-1756949711124.png" alt="js2_1-1756949711124.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Now we have one dataset with a mix of white and red wines.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H3 id="toc-hId-1495224703"&gt;Visualize data&lt;/H3&gt;&lt;P&gt;Before training a model, explore the dataset using popular charting libraries: Seaborn and Matplotlib.&lt;/P&gt;&lt;P&gt;Plot a histogram of the dependent variable, quality.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import seaborn as sns
sns.displot(data.quality, kde=False)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_2-1756949850129.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308825i87E79215C173B70F/image-size/large?v=v2&amp;amp;px=999" role="button" title="js2_2-1756949850129.png" alt="js2_2-1756949850129.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Looks like quality scores are normally distributed between 3 and 9. Define a wine as high quality if it has quality &amp;gt;= 7.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;high_quality = (data.quality &amp;gt;= 7)
data.quality = high_quality

# cast to float as a best practice (avoids dtype issues with NaN's later)
data["quality"] = data["quality"].astype("float32")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Box plots are useful for identifying correlations between features and a binary label. Create box plots for each feature to compare high-quality and low-quality wines. Significant differences in the box plots indicate good predictors of quality.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import matplotlib.pyplot as plt

dims = (3, 4)

f, axes = plt.subplots(dims[0], dims[1], figsize=(25, 15))
axis_i, axis_j = 0, 0
for col in data.columns:
  if col == 'is_red' or col == 'quality':
    continue # Box plots cannot be used on indicator variables
  sns.boxplot(x=high_quality, y=data[col], ax=axes[axis_i, axis_j])
  axis_j += 1
  if axis_j == dims[1]:
    axis_i += 1
    axis_j = 0&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_3-1756949850144.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308826iF8D13D55625B9536/image-size/large?v=v2&amp;amp;px=999" role="button" title="js2_3-1756949850144.png" alt="js2_3-1756949850144.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;In the above box plots, a few variables stand out as good univariate predictors of quality.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;In the alcohol box plot, the median alcohol content of high-quality wines is greater than even the 75th quantile of low-quality wines. High alcohol content is correlated with quality.&lt;/LI&gt;&lt;LI&gt;In the density box plot, low quality wines have a greater density than high quality wines. Density is inversely correlated with quality.&lt;/LI&gt;&lt;/UL&gt;&lt;H2 id="toc-hId-1169628479"&gt;&amp;nbsp;&lt;/H2&gt;&lt;H2 id="toc-hId-973114974"&gt;Preprocess data&lt;/H2&gt;&lt;P&gt;Before training a model, check for missing values and split the data into training and validation sets.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;data.isna().any()&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_4-1756950017285.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308827i2A0200F5FB07390B/image-size/medium?v=v2&amp;amp;px=400" role="button" title="js2_4-1756950017285.png" alt="js2_4-1756950017285.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;There are no missing values.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P class="lia-indent-padding-left-60px" style="padding-left : 60px;"&gt;&lt;EM&gt;Note: Often you will need to take advantage of &lt;STRONG&gt;feature engineering&lt;/STRONG&gt; at this step. This is where you can build new features as combinations of your existing dataâ€¦ for example multiplying two existing feature columns together to create a new column may enable the model to find better patterns in the data.&lt;/EM&gt;&lt;/P&gt;&lt;P class="lia-indent-padding-left-60px" style="padding-left : 60px;"&gt;&lt;EM&gt;For time series data it can be very helpful to generate a new feature column called â€œQtrâ€ for example to show which qtr of the year that data point is in based on a date. You will need to experiment with thisâ€¦&lt;/EM&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-776601469"&gt;Prepare the dataset to train a baseline model&lt;/H2&gt;&lt;P&gt;Split the input data into 3 sets:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Train (60% of the dataset used to train the model)&lt;/LI&gt;&lt;LI&gt;Validation (20% of the dataset used to tune the hyperparameters)&lt;/LI&gt;&lt;LI&gt;Test (20% of the dataset used to report the true performance of the model on an unseen dataset)&lt;/LI&gt;&lt;/UL&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from sklearn.model_selection import train_test_split

X = data.drop(["quality"], axis=1)
y = data.quality

# Split out the training data
X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.6, random_state=123)

# Split the remaining data equally into validation and test
X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=0.5, random_state=123)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-580087964"&gt;Train a baseline model&lt;/H2&gt;&lt;P&gt;This task seems well suited to a &lt;STRONG&gt;random forest classifier&lt;/STRONG&gt;, since the output is binary and there may be interactions between multiple variables.&lt;/P&gt;&lt;P&gt;Build a simple classifier using scikit-learn and use MLflow to keep track of the model's accuracy and save the model for later use.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P class="lia-indent-padding-left-60px" style="padding-left : 60px;"&gt;&lt;EM&gt;Note: When this cell is executed an MLFlow Experiment will be created by default (automatically) using the full path of this Notebook. In production experiments its best practice to set the Experiment name with&amp;nbsp;mlflow.set_experiment()&amp;nbsp;because you may work on the problem over multiple Notebooks and/or users.&lt;/EM&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import mlflow.pyfunc
import mlflow.sklearn
import numpy as np
import sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
from mlflow.models.signature import infer_signature
from mlflow.utils.environment import _mlflow_conda_env
import cloudpickle
import time

# The predict method of sklearn's RandomForestClassifier returns a binary classification (0 or 1). 
# The following code creates a wrapper function, SklearnModelWrapper, that uses 
# the predict_proba method to return the probability that the observation belongs to each class. 

class SklearnModelWrapper(mlflow.pyfunc.PythonModel):
  def __init__(self, model):
    self.model = model
    
  def predict(self, context, model_input):
    return self.model.predict_proba(model_input)[:,1]

# mlflow.start_run creates a new MLflow run to track the performance of this model. 
# Within the context, you call mlflow.log_param to keep track of the parameters used, and
# mlflow.log_metric to record metrics like accuracy.
with mlflow.start_run(run_name='rf_baseline_n10'):
  n_estimators = 10
  model = RandomForestClassifier(n_estimators=n_estimators, random_state=np.random.RandomState(123))
  model.fit(X_train, y_train)

  # predict_proba returns [prob_negative, prob_positive], so slice the output with [:, 1]
  predictions_test = model.predict_proba(X_test)[:,1]
  auc_score = roc_auc_score(y_test, predictions_test)
  mlflow.log_param('n_estimators', n_estimators)
  # Use the area under the ROC curve as a metric.
  mlflow.log_metric('auc', auc_score)
  wrappedModel = SklearnModelWrapper(model)
  
  # MLflow contains utilities to create a conda environment used to serve models.
  # The necessary dependencies are added to a conda.yaml file which is logged along with the model.
  conda_env = _mlflow_conda_env(
        additional_conda_deps=None,
        additional_pip_deps=["cloudpickle=={}".format(cloudpickle.__version__), "scikit-learn=={}".format(sklearn.__version__)],
        additional_conda_channels=None,
    )

  # Here we log the model and register it to Unity Catalog in one go.
  # MLflow automatically generates model signatures when you provide
  # an `input_example` during model logging. This works for all model 
  # flavors and is the recommended approach for most use cases.
  # By registering this model to Unity Catalog, you can easily reference
  # the model from anywhere within Databricks.
  # 
  sample_input = X_train.head(5)

  model_version = mlflow.pyfunc.log_model(
    name="rf_baseline",
    python_model=wrappedModel,
    conda_env=conda_env,
    input_example=sample_input,
    registered_model_name=MODEL_NAME,
  )&lt;/code&gt;&lt;/pre&gt;&lt;P class="lia-indent-padding-left-60px" style="padding-left : 60px;"&gt;&lt;EM&gt;Note how the trained model is registered when logging it to MLFlow. This can be done separately as we will see later. If you are running many experiments there is no need to register every model but only the best model.&lt;/EM&gt;&lt;/P&gt;&lt;P&gt;Review the learned feature importances output by the model. As illustrated by the previous boxplots, alcohol and density are important in predicting quality.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;feature_importances = pd.DataFrame(model.feature_importances_, index=X_train.columns.tolist(), columns=['importance'])
feature_importances.sort_values('importance', ascending=False)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_5-1756950458175.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308828i525ABF330AAFB999/image-size/medium?v=v2&amp;amp;px=400" role="button" title="js2_5-1756950458175.png" alt="js2_5-1756950458175.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;You logged the Area Under the ROC Curve (AUC) to MLflow. Click the Experiment icon in the right sidebar to display the Experiment Runs sidebar. The model achieved an AUC of 0.854. A random classifier would have an AUC of 0.5, and higher AUC values are better.&lt;/P&gt;&lt;P&gt;The ROC AUC is a good evaluation metric for binary classification problems like we have here (is good quality / is not good quality).&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Next, assign this model the "Best" tag, and load it into this notebook from Unity Catalog.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from mlflow.tracking import MlflowClient

client = MlflowClient()
client.set_registered_model_alias(MODEL_NAME, "Best", model_version.registered_model_version)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;In Unity Catalog, the model version now has the tag "Best". You can now refer to the model using the path&amp;nbsp;&lt;FONT face="terminal,monaco"&gt;models:/{model_name}@Best&lt;/FONT&gt;.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_6-1756950552130.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308830iB8AEA9EA4051F02D/image-size/medium?v=v2&amp;amp;px=400" role="button" title="js2_6-1756950552130.png" alt="js2_6-1756950552130.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-383574459"&gt;Experiment with a new model&lt;/H2&gt;&lt;P&gt;The random forest model performed well even &lt;EM&gt;without&lt;/EM&gt; hyperparameter tuning.&lt;/P&gt;&lt;P&gt;Let's now try and do better and use the xgboost library to train a more accurate model. Run a hyperparameter sweep to train multiple models in parallel, using Hyperopt and Trials. As before, MLflow tracks the performance of each parameter configuration.&lt;/P&gt;&lt;P class="lia-indent-padding-left-60px" style="padding-left : 60px;"&gt;&lt;EM&gt;Note: We must use Trials and not SparkTrials in SAP Databricks, because SparkTrials tries to access the underlying JVM which is not supported on serverless compute.&lt;/EM&gt;&lt;/P&gt;&lt;P&gt;We use the validation dataset here for hyperparameter search.&lt;/P&gt;&lt;P class="lia-indent-padding-left-60px" style="padding-left : 60px;"&gt;&lt;FONT color="#FF0000"&gt;&lt;EM&gt;Note this training cell takes over 20mins to complete!&lt;/EM&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from hyperopt import fmin, tpe, hp, Trials, STATUS_OK
from hyperopt.pyll import scope
from math import exp
import mlflow.xgboost
import numpy as np
import xgboost as xgb

search_space = {
  'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),
  'learning_rate': hp.loguniform('learning_rate', -3, 0),
  'reg_alpha': hp.loguniform('reg_alpha', -5, -1),
  'reg_lambda': hp.loguniform('reg_lambda', -6, -1),
  'min_child_weight': hp.loguniform('min_child_weight', -1, 3),
  'objective': 'binary:logistic',
  'seed': 123, # Set a seed for deterministic training
}

def train_model(params):
  # With MLflow autologging, hyperparameters and the trained model are automatically logged to MLflow.
  mlflow.xgboost.autolog()
  with mlflow.start_run(nested=True):
    train = xgb.DMatrix(data=X_train, label=y_train)
    validation = xgb.DMatrix(data=X_val, label=y_val)
    # Pass in the validation set so xgb can track an evaluation metric. XGBoost terminates training when the evaluation metric
    # is no longer improving.
    booster = xgb.train(params=params, dtrain=train, num_boost_round=1000,
                        evals=[(validation, "validation")], early_stopping_rounds=50)
    validation_predictions = booster.predict(validation)
    auc_score = roc_auc_score(y_val, validation_predictions)
    mlflow.log_metric('auc', auc_score)

    # Don't register the model in one-step here - let the hyperparameter search find the best one first.
    #signature = infer_signature(X_train, booster.predict(train))
    #mlflow.xgboost.log_model(booster, name="xgboost", signature=signature)
    sample_input = X_train.head(5)
    mlflow.xgboost.log_model(booster, name="xgboost", input_example=sample_input)
    
    # Set the loss to -1*auc_score so fmin maximizes the auc_score
    return {'status': STATUS_OK, 'loss': -1*auc_score, 'booster': booster.attributes()}

# Use Trials instead of SparkTrials
trials = Trials()

# Run fmin within an MLflow run context so that each hyperparameter configuration is logged as a child run of a parent
# run called "xgboost_models" .
with mlflow.start_run(run_name='xgboost_models'):
  best_params = fmin(
    fn=train_model, 
    space=search_space, 
    algo=tpe.suggest, 
    max_evals=96,
    trials=trials,
  )&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-187060954"&gt;Use MLflow to view the results&lt;/H2&gt;&lt;P&gt;Open up the &lt;EM&gt;Experiments&lt;/EM&gt; sidebar to see the MLflow runs. Click on Date next to the down arrow to display a menu, and select 'auc' to display the runs sorted by the auc metric. The highest auc value is ~0.90.&amp;nbsp;&lt;STRONG&gt;Remember that this is against the validation data&lt;/STRONG&gt;.&lt;/P&gt;&lt;P class="lia-indent-padding-left-60px" style="padding-left : 60px;"&gt;&lt;EM&gt;Hyperparameter tuning (runs) score against the validation dataset!&lt;/EM&gt;&lt;/P&gt;&lt;P&gt;MLflow tracks the parameters and performance metrics of each run. Click the External Link icon at the top of the Experiment Runs sidebar to navigate to the MLflow Runs Table.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId--9452551"&gt;Update the best version of the&amp;nbsp;&lt;FONT face="terminal,monaco"&gt;wine_quality_classifier&lt;/FONT&gt;&amp;nbsp;model&lt;/H2&gt;&lt;P&gt;Earlier, you saved the baseline model to Unity Catalog with the name&amp;nbsp;&lt;FONT face="terminal,monaco"&gt;wine_quality_classifier&lt;/FONT&gt;. Now you can update&amp;nbsp;&lt;FONT face="terminal,monaco"&gt;wine_quality_classifier&lt;/FONT&gt;&amp;nbsp;to a more accurate model from the hyperparameter sweep. Because you used MLflow to log the model produced by each hyperparameter configuration, you can use MLflow to identify the best performing run and save the model from that run to Unity Catalog.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;best_run = mlflow.search_runs(order_by=['metrics.auc DESC']).iloc[0]
print(f'AUC of Best Run: {best_run["metrics.auc"]}')&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_7-1756950758865.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308831i3D6D3AA368E57F2C/image-size/medium?v=v2&amp;amp;px=400" role="button" title="js2_7-1756950758865.png" alt="js2_7-1756950758865.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;new_model_version = mlflow.register_model(f"runs:/{best_run.run_id}/model", MODEL_NAME)

# Registering the model takes a few seconds, so add a small delay
import time
time.sleep(15)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Click&amp;nbsp;&lt;STRONG&gt;Models&lt;/STRONG&gt;&amp;nbsp;in the left sidebar to see that the&amp;nbsp;&lt;FONT face="terminal,monaco"&gt;wine_quality_classifier&lt;/FONT&gt;&amp;nbsp;model now has a new versions. Assign the "Best" alias to the new version.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from mlflow.tracking import MlflowClient

client = MlflowClient()
client.set_registered_model_alias(MODEL_NAME, "Best", new_model_version.version)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Clients that call&amp;nbsp;&lt;FONT face="terminal,monaco"&gt;load_model()&lt;/FONT&gt;&amp;nbsp;using the "Best" alias now get the new model.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&amp;gt;&amp;gt; &lt;/STRONG&gt;&lt;STRONG&gt;Let's get the AUC score against the Test data&lt;/STRONG&gt;&lt;STRONG&gt;:&lt;/STRONG&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;model = mlflow.pyfunc.load_model(f"models:/{MODEL_NAME}@Best")

from sklearn.metrics import roc_auc_score
print(f'AUC: {roc_auc_score(y_test, model.predict(X_test))}')&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_8-1756950758865.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308832i435B28BDC047F3F0/image-size/medium?v=v2&amp;amp;px=400" role="button" title="js2_8-1756950758865.png" alt="js2_8-1756950758865.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;The new version achieved a better score (AUC = 0.90) on the test set.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-141288301"&gt;Batch inference&lt;/H2&gt;&lt;P&gt;There are many scenarios where you might want to evaluate a model on a corpus of new data. For example, you may have a fresh batch of data or may need to compare the performance of two models on the same corpus of data.&lt;/P&gt;&lt;P&gt;Evaluate the model on data stored in a Delta table, using Spark to run the computation in parallel.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# To simulate a new corpus of data, save the existing X_train data to a Delta table. 
# In the real world, this would be a new batch of data.
spark_df = spark.createDataFrame(X_train)

table_name = f"{CATALOG_NAME}.{SCHEMA_NAME}.wine_data"

(spark_df
  .write
  .format("delta")
  .mode("overwrite")
  .option("overwriteSchema",True)
  .saveAsTable(table_name)
)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Load the model into a Spark UDF, so it can be applied to the Delta table.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;apply_model_udf = mlflow.pyfunc.spark_udf(spark, f"models:/{MODEL_NAME}@Best")&lt;/code&gt;&lt;/pre&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# Read the "new data" from the Unity Catalog table
new_data = spark.read.table(f"{CATALOG_NAME}.{SCHEMA_NAME}.wine_data")&lt;/code&gt;&lt;/pre&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from pyspark.sql.functions import struct

# Apply the model to the new data
udf_inputs = struct(*(X_train.columns.tolist()))

new_data = new_data.withColumn(
  "prediction",
  apply_model_udf(udf_inputs)
)

display(new_data)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_9-1756951022597.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308833iDD624907B34C7B23/image-size/large?v=v2&amp;amp;px=999" role="button" title="js2_9-1756951022597.png" alt="js2_9-1756951022597.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Each row now has an associated prediction. Note that the&amp;nbsp;&lt;STRONG&gt;xgboost&lt;/STRONG&gt;&amp;nbsp;function is using the objective "binary:logistic" so the predictions shown here are probabilities.&lt;/P&gt;&lt;P&gt;We also add a is_good_quality column:&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from pyspark.sql.functions import col, when

new_data = new_data.withColumn("prediction", col("prediction")[0])

new_data = new_data.withColumn(
  "is_good_quality",
  when(col("prediction") &amp;gt; 0.5, True).otherwise(False)
)
display(new_data)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_10-1756951022605.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308834i46308B60261C3565/image-size/large?v=v2&amp;amp;px=999" role="button" title="js2_10-1756951022605.png" alt="js2_10-1756951022605.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Overwrite the table with the new columns:&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;(new_data
  .write
  .format("delta")
  .mode("overwrite")
  .option("overwriteSchema", True)
  .saveAsTable(table_name)
)

# Enable Change Data Feed for the table
# Seems that we can only add this option via SQL!!
spark.sql(f"ALTER TABLE {table_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId--55225204"&gt;Serve the model&lt;/H2&gt;&lt;P&gt;To productionize the model for low latency predictions, use Mosaic AI Model Serving to deploy the model to an endpoint. The following cell shows how to use the MLflow Deployments SDK to create a model serving endpoint (which can also be done view the Serving menu on the left).&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;First of all, letâ€™s just show the current modelâ€™s name and best version&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# Get the model vesion we tagged as &lt;a href="https://community.sap.com/t5/user/viewprofilepage/user-id/1725027"&gt;@Best&lt;/a&gt;
from mlflow.tracking import MlflowClient
best_ver = MlflowClient().get_model_version_by_alias(MODEL_NAME, "Best").version
print(MODEL_NAME, best_ver)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_11-1756951367897.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308835i39C1C658C1D6029A/image-size/medium?v=v2&amp;amp;px=400" role="button" title="js2_11-1756951367897.png" alt="js2_11-1756951367897.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Check if any versions of this model are already being served and delete them&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from mlflow.deployments import get_deploy_client

client = get_deploy_client("databricks")
endpoints = client.list_endpoints()

deployed = False
deployed_versions = []
for ep in endpoints:
    ep_detail = client.get_endpoint(ep["name"])
    for entity in ep_detail.get("config", {}).get("served_models", []):
        if entity.get("model_name") == MODEL_NAME or entity.get("model_name").endswith(MODEL_NAME):
            deployed = True
            deployed_versions.append(str(entity.get("model_version")))
            # Delete the serving endpoint if the model is already deployed
            client.delete_endpoint(ep["name"])

if deployed_versions:
    deployed_versions_str = ", ".join(deployed_versions)
else:
    deployed_versions_str = ""

display(spark.createDataFrame([{"model_name": MODEL_NAME, "deployed": deployed, "deployed_versions": deployed_versions_str, "action": "deleting..."}]))&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_12-1756951367899.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308836iCF48545499B7C401/image-size/large?v=v2&amp;amp;px=999" role="button" title="js2_12-1756951367899.png" alt="js2_12-1756951367899.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;EM&gt;&lt;FONT color="#FF0000"&gt;Creating the endpoint can take 5+ minutes...&lt;/FONT&gt;&lt;/EM&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# the "name" property can't include special chars so we drop the catalog and schema from the model_name

from mlflow.deployments import get_deploy_client

client = get_deploy_client("databricks")
endpoint = client.create_endpoint(
    name="wine-model-endpoint",
    config={
        "served_entities": [
            {
                "name": MODEL_NAME,
                "entity_name": f"{CATALOG_NAME}.{SCHEMA_NAME}.{MODEL_NAME}",
                "entity_version": best_ver,
                "workload_size": "Small",
                "scale_to_zero_enabled": True
            }
        ],
      }
)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId--251738709"&gt;Test the Model Serving Endpoint&lt;/H2&gt;&lt;P&gt;Navigate to User Settings -&amp;gt; Developer and create an Access Token for calling the serving endpoint.&lt;/P&gt;&lt;P&gt;Ensure the model is being served as this can take 5-10 mins.&lt;/P&gt;&lt;P&gt;In the below cells the notebook will:&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;Ask for your access token&lt;/LI&gt;&lt;LI&gt;Setup a payload (the required inputs for your model)&lt;/LI&gt;&lt;LI&gt;Call the model serving endpoint!&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from getpass import getpass
token = getpass("ðŸ”‘  Paste your Databricks token: ")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Setup the api call request payload with sample wine quality data:&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;payload = {
  "dataframe_split": {
    "columns": [
      "fixed_acidity", "volatile_acidity", "citric_acid", "residual_sugar",
      "chlorides", "free_sulfur_dioxide", "total_sulfur_dioxide",
      "density", "pH", "sulphates", "alcohol", "is_red"
    ],
    "data": [
      [7.3, 0.19, 0.27, 1.6, 0.027, 35, 136, 0.99248, 3.38, 0.54, 11, 0],
      [7.8, 0.88, 0.00, 2.6, 0.098, 25, 67, 0.9968, 3.20, 0.68, 9.8, 1]
    ]
  }&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Use the python requests package to make an api call to the SAP Databricks Model Serving Endpoint.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P class="lia-indent-padding-left-30px" style="padding-left : 30px;"&gt;&lt;FONT color="#FF0000"&gt;&lt;EM&gt;Make sure to update the endpoint uri below to match your current SAP Databricks system!&lt;/EM&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import os, json, requests

url   = "https://&amp;lt;uri&amp;gt;.cloud.databricks.com/serving-endpoints/wine-model-endpoint/invocations"

resp = requests.post(
    url,
    headers={
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json"
    },
    data=json.dumps(payload),
    timeout=60
)

if resp.status_code == 404:
    print("The endpoint is still deploying.")
else:
    print(resp.json())
    for i, score in enumerate(resp.json()["predictions"], start=1):
        is_good = score &amp;gt;= 0.5          # quality flag
        print(f"Row {i}: {score:.3f}  âžœ  Good quality? {is_good}")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="js2_13-1756951569054.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/308837i0BC6D9EA17EBEA20/image-size/large?v=v2&amp;amp;px=999" role="button" title="js2_13-1756951569054.png" alt="js2_13-1756951569054.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;You can use this API endpoint to perform inference from your own applications â€“ as is done with blog post : â€œ&lt;STRONG&gt;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-create-inferences-for-application-integration-with-sap-build/ba-p/14186662" target="_blank"&gt;Part 6 â€“ Create inferences for application integration with SAP Build&amp;nbsp;&lt;/A&gt;&lt;/STRONG&gt;â€ in the series.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId--448252214"&gt;Conclusion&lt;/H2&gt;&lt;P&gt;Weâ€™ve seen in this notebook, if you have followed along, the typical pattern of training a machine learning model.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;It always starts with understanding the data available. Visualising the data with histograms and box plots as shown here can be a great help. Use tools like ChatGPT to assist in the best ways to flesh out information about your data&lt;/LI&gt;&lt;LI&gt;It can often be helpful to create a quick baseline model just to see that we can do better than random luck with the training data&lt;/LI&gt;&lt;LI&gt;Use a hyperparameter optimisation tool to help search for the ideal parameters to tune the best model. Be very careful with the split of training, validation and test data and ensure that there can never be any overlap. Research how to do this if using time-series data&lt;/LI&gt;&lt;LI&gt;Use MLFlow to log training experiments and their generated models. Assign tags to highlight specific or â€œbestâ€ models.&lt;/LI&gt;&lt;LI&gt;Look at Batch Inference or Model Serving.&lt;UL&gt;&lt;LI&gt;The former (batch) being ideal if you want to batch score a table of data and potentially share it back to BDC to be used in analytics models. Make use of scheduled notebooks to keep the data up to date and to train the model on new data&lt;/LI&gt;&lt;LI&gt;Use Model Serving to expose an endpoint for real-time applications to make predictions.&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-databricks-in-sap-business-data-cloud-a-typical-machine-learning/ba-p/14206612"/>
    <published>2025-09-04T04:16:17.227000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/hello-python-my-first-script-in-sap-bas-connecting-to-hana-cloud/ba-p/14228993</id>
    <title>Hello Python: My First Script in SAP BAS Connecting to HANA Cloud</title>
    <updated>2025-09-26T13:05:26.454000+02:00</updated>
    <author>
      <name>Sharathmg</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/174516</uri>
    </author>
    <content>&lt;P&gt;Credit:&amp;nbsp;&lt;a href="https://community.sap.com/t5/user/viewprofilepage/user-id/183"&gt;@Vitaliy-R&lt;/a&gt;&amp;nbsp;Your startup blogs kindled my interest to explore working with Python in SAP ecosystem.&amp;nbsp;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/using-python-in-sap-business-application-studio-my-notes/ba-p/14155516" target="_self"&gt;Python in BAS&lt;/A&gt;&amp;nbsp;and&amp;nbsp;&lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/using-jupyter-in-sap-business-application-studio-my-notes/ba-p/14167294" target="_self"&gt;Jupyter in BAS&lt;/A&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;When I first started exploring SAP Business Application Studio (BAS), I was curious about how Python could fit into the SAP landscape. Iâ€™ve mostly associated BAS with HANA artefacts(SQLScript, hdbcalculationview, hdbreptask etc.) and CAP artefacts, so writing a Python script inside BAS felt like venturing into new territory. My goal was simple: write a basic script and connect it to SAP HANA Cloud. What I discovered along the way is that Python not only works smoothly in BAS but also makes it easy to interact with HANA Cloud, opening up opportunities for data exploration, automation, and integration in a way that feels both modern and approachable.&lt;/P&gt;&lt;P&gt;Before jumping into the Python script, I had to get my environment ready in SAP Business Application Studio (BAS). Hereâ€™s what I set up:&lt;/P&gt;&lt;P&gt;A BAS dev space with a full-stack cloud application space since it supports multiple runtimes, including Python. I had a space with HANA Native Application type. Since the Python tools extension&amp;nbsp;is not added by default, I edited the space to select the Python tools in the additional extension options.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="HANA Dev Space Python extension" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/320334iFEC4E0932EFEAC15/image-size/large?v=v2&amp;amp;px=999" role="button" title="HANA_DevSpace_Setting.png" alt="HANA Dev Space Python extension" /&gt;&lt;span class="lia-inline-image-caption" onclick="event.preventDefault();"&gt;HANA Dev Space Python extension&lt;/span&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;Note: For initial steps to check the Python version, Jupyter notebook and set ups refer to the blogs listed at the start.&amp;nbsp;&lt;/P&gt;&lt;P&gt;Use Case: I attempted to achieve the following:&amp;nbsp;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Establish a connection to HANA Cloud&lt;/LI&gt;&lt;LI&gt;Execute an SQL query on a table/view&amp;nbsp;&lt;/LI&gt;&lt;LI&gt;Display the results&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;In the BAS, I created a project from Template: SAP HANA Database Project&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Project Template.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/320351iEAE035C8FCA7C5B5/image-size/large?v=v2&amp;amp;px=999" role="button" title="Project Template.png" alt="Project Template.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Next step: Create a notebook file.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="notebook file.png" style="width: 339px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/320356i8F1BB8DEF9D0E888/image-size/medium?v=v2&amp;amp;px=400" role="button" title="notebook file.png" alt="notebook file.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;My guide to connect to HANA Cloud:&amp;nbsp;&lt;A href="https://help.sap.com/docs/SAP_HANA_CLIENT/f1b440ded6144a54ada97ff95dac7adf/d12c86af7cb442d1b9f8520e2aba7758.html" target="_self" rel="noopener noreferrer"&gt;Connect to HANA Cloud&lt;/A&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;When I first tried importing hdbcli into my Jupyter Notebook within BAS, I ran into the same ModuleNotFoundError. Even though I had already installed hdbcli In the terminal, the notebook kernel wasnâ€™t recognizing it. On some search and prompting with GPT( &lt;span class="lia-unicode-emoji" title=":beaming_face_with_smiling_eyes:"&gt;ðŸ˜&lt;/span&gt;), I understood that it's a common issue because Jupyter can run in a different Python environment than the terminal. The fix was simple: I ran&lt;/P&gt;&lt;PRE&gt;import sys
!{sys.executable} -m pip install hdbcli&lt;/PRE&gt;&lt;P&gt;directly in a notebook cell. This ensures that the HANA client is installed in the same environment as the notebook kernel. After this step, I could successfully import dbapi and connect to HANA Cloud without any errors. It was a small but important lesson about Python environments in BAS, especially when using Jupyter.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="hdbcli Module Not found.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/320378i62AF858DA44FE00C/image-size/large?v=v2&amp;amp;px=999" role="button" title="hdbcli Module Not found.png" alt="hdbcli Module Not found.png" /&gt;&lt;/span&gt;With the hdbcli package installed and working in my Jupyter Notebook, I was ready to write my first Python script to connect to SAP HANA Cloud.&lt;/P&gt;&lt;P&gt;In the next cell, I imported hdbcli in this notebook.&amp;nbsp;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import hdbcli
print(hdbcli.__file__)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="import hdbcli.png" style="width: 854px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/320388iF426637D8D8CCB0F/image-size/large?v=v2&amp;amp;px=999" role="button" title="import hdbcli.png" alt="import hdbcli.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;The next step was to&amp;nbsp;gain access to the dbapi interface, which allows you to establish connections, execute SQL queries, and fetch results from your HANA Cloud instance. This simple import is the gateway to working with HANA directly from Python.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from hdbcli import dbapi&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;The next step is to establish a connection to your HANA Cloud instance. This requires specifying the host, port, username, and password.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="hana cloud connection.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/320408i84F10DA5613166DC/image-size/large?v=v2&amp;amp;px=999" role="button" title="hana cloud connection.png" alt="hana cloud connection.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;After connecting, you can create a cursor object to execute SQL statements. An SQL statement, preferably a Select Query to test the retrieval of data from HANA Cloud. In my case, I used a Select with count on the number of records in a view. Once the variables were ready, execute the connection cursor object.&lt;/P&gt;&lt;P&gt;Note: in the SQL variable, use single quotes and a semicolon at the end of the query. (beginner tip&amp;nbsp;&lt;span class="lia-unicode-emoji" title=":slightly_smiling_face:"&gt;ðŸ™‚&lt;/span&gt; )&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="Execution Cursor.png" style="width: 799px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/320427iB0929785AAAB7257/image-size/large?v=v2&amp;amp;px=999" role="button" title="Execution Cursor.png" alt="Execution Cursor.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Now is the time to test the data retrieval from the script and compare it with the Database Explorer.&lt;/P&gt;&lt;P&gt;Drum roll....&lt;span class="lia-unicode-emoji" title=":drum:"&gt;ðŸ¥&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-left" image-alt="Data in DB explorer.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/320447i3D6BB255F8FDBF13/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Data in DB explorer.png" alt="Data in DB explorer.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-right" image-alt="Data in Script.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/320448iDA977EF3358B8FF8/image-size/medium?v=v2&amp;amp;px=400" role="button" title="Data in Script.png" alt="Data in Script.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Hurray&amp;nbsp;&lt;span class="lia-unicode-emoji" title=":party_popper:"&gt;ðŸŽ‰&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Completing my first Python script in SAP Business Application Studio and connecting it to HANA Cloud was an exciting milestone. From the initial curiosity to the small hurdles like installing hdbcli in the notebook and finally seeing my script return results, every step felt like a mini victory.&lt;/P&gt;&lt;P&gt;That simple output from HANA Cloud made all the effort worthwhile and gave me a real sense of accomplishment.&lt;/P&gt;&lt;P&gt;This experience has sparked my curiosity to explore more complex queries, data analysis, and automation using Python in SAP.&lt;/P&gt;&lt;P&gt;I hope my journey inspires others to take that first step and discover how fun and powerful working with Python and HANA Cloud can be.&lt;/P&gt;&lt;P&gt;Chao.&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/hello-python-my-first-script-in-sap-bas-connecting-to-hana-cloud/ba-p/14228993"/>
    <published>2025-09-26T13:05:26.454000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/financial-management-blog-posts-by-sap/sap-cpq-2511-scripting-for-custom-quote-actions-amp-quote-item-custom/ba-p/14253497</id>
    <title>SAP CPQ 2511 - Scripting for Custom Quote Actions &amp; Quote Item Custom Fields Access Control</title>
    <updated>2025-10-26T14:38:16.450000+01:00</updated>
    <author>
      <name>Yogananda</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/75</uri>
    </author>
    <content>&lt;P&gt;This blog introduces the scripting enhancements in SAP CPQ 2511, tested and verified in my pre-release version. The scripting examples provided here are optimized for implementation and serve as a valuable resource for CPQ developers, functional consultants, and integration specialists seeking for practical guidance.&lt;BR /&gt;&lt;BR /&gt;Special thanks to Nikola &amp;amp; Pavithran for upgrading to 2511 for pre-release testing&lt;/P&gt;&lt;P&gt;Follow for more updates from&amp;nbsp; : &lt;A href="https://profile.sap.com/u/Yogananda" target="_self" rel="noopener noreferrer"&gt;Yogananda Muthaiah&lt;/A&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="2025-10-26_14-05-29.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/332560i4533DD2CD067FDDD/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="2025-10-26_14-05-29.png" alt="2025-10-26_14-05-29.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId-1892778276"&gt;Dynamic Access Control for Quote Item Custom Fields&lt;/H3&gt;&lt;P&gt;Access level permissions on Quote Item Custom Fields can now be dynamically set through scripting in &lt;SPAN class=""&gt;Quote&lt;/SPAN&gt; 2.0. This feature allows users to control the editability, read-only status, and visibility of fields, enhancing customization and security in managing quotes.&lt;/P&gt;&lt;P class=""&gt;&lt;FONT color="#FF00FF"&gt;Here's an example script:&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;from Scripting.Quote import QuoteFieldAccessLevel

for item in context.PagedItems:
    QuoteFieldAccessContext.SetAccessLevel(item, 'TargetPrice', QuoteFieldAccessLevel.Hidden)
    QuoteFieldAccessContext.SetAccessLevel(item, 'TargetPrice', QuoteFieldAccessLevel.Readonly)
    QuoteFieldAccessContext.SetAccessLevel(item, 'TargetPrice', QuoteFieldAccessLevel.Editable)


eachItem = context.Quote.GetItemByItemNumber(1)
QuoteFieldAccessContext.SetAccessLevel(eachItem, 'TargetPrice', QuoteFieldAccessLevel.ReadOnly)  

QuoteFieldAccessContext.SetColumnAccessLevel('TargetPrice', QuoteFieldAccessLevel.ReadOnly)


QuoteFieldAccessContext.SetAccessLevelForProductType(39, 'TargetPrice', QuoteFieldAccessLevel.ReadOnly)

QuoteFieldAccessContext.SetAccessLevelForSection('ProductType', 'TargetPrice', QuoteFieldAccessLevel.Editable)

QuoteFieldAccessContext.SetAccessLevelForCartTotal&lt;/code&gt;&lt;/pre&gt;&lt;DIV&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="2025-10-26_14-10-35.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/332558i583D3AF65C315A6D/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="2025-10-26_14-10-35.png" alt="2025-10-26_14-10-35.png" /&gt;&lt;/span&gt;&lt;/DIV&gt;&lt;DIV&gt;&amp;nbsp;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/DIV&gt;&lt;H3 id="toc-hId-1696264771"&gt;Executing Custom Quote Actions via Scripting&lt;/H3&gt;&lt;P class=""&gt;As of the 2511 release, you can execute custom quote actions in &lt;SPAN class=""&gt;Quote&lt;/SPAN&gt; 2.0 within the Workflow context through scripting.&lt;/P&gt;&lt;P class=""&gt;The &lt;SPAN class=""&gt;SAP CPQ&lt;/SPAN&gt; system checks all the permissions and conditions defined in the Workflow for these actions to ensure that only available custom quote actions are executed. For each available action, the system performs pre-actions and post-actions and sends notifications. The list of available custom quote actions is also returned in scripting.&lt;/P&gt;&lt;P class=""&gt;&lt;FONT color="#FF00FF"&gt;Here's an example script:&lt;/FONT&gt;&lt;/P&gt;&lt;DIV class=""&gt;&amp;nbsp;&lt;/DIV&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;customQuoteactionAvailable = WorkflowExecutor.IsActionAvailableForQuote(3102, context.Quote.Id)
WorkflowExecutor.ExecuteActionOnQuote(3102, context.Quote.Id)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H3 id="toc-hId-1499751266"&gt;Enhanced ExtenalItemID Editing via API and Scripting&lt;/H3&gt;&lt;P class=""&gt;As of 2511 release, extenalItemID can be edited, updated and accessed through API and scripting. This enhancement enables more flexible and efficient data management and integration scenarios.&lt;/P&gt;&lt;P class=""&gt;&lt;SPAN&gt;to enable ExternalItemId to be changed from the Scripting&lt;/SPAN&gt;&lt;/P&gt;&lt;P class=""&gt;&lt;FONT color="#FF00FF"&gt;Here's an example script:&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;Items = context.Quote.GetAllItems()
Items[0].ExternalItemId = 'Testing from Yoga to update ExternalItemId '

Trace.Write(Items[0].ExternalItemId)&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-1303237761"&gt;Improved &lt;FONT color="#FF6600"&gt;RestClient &lt;/FONT&gt;Methods for Decompressed Responses&lt;/H3&gt;&lt;P&gt;&lt;SPAN class=""&gt;RestClients&lt;/SPAN&gt; previously returned unreadable strings for compressed responses. To address this issue and support the decompression of the response body for compressed responses using Gzip or Deflate compression methods, a new &lt;SPAN class=""&gt;decompressResponse&lt;/SPAN&gt; parameter was added. &lt;FONT color="#FF6600"&gt;By default, it is set to False,&lt;/FONT&gt; maintaining current functionality.&lt;/P&gt;&lt;P&gt;&lt;FONT color="#0000FF"&gt;Setting it to True&lt;/FONT&gt; automatically decompresses responses into readable JSON, allowing users to handle compressed responses without changing existing calls unless needed.&lt;/P&gt;&lt;P&gt;&lt;FONT color="#FF00FF"&gt;Here's an example script:&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;authorization_token = RestClient.GetBasicAuthenticationHeader("YMUTHAIAH", "XXXXXXXXXXXXXXX")
headers = { "Authorization": authorization_token}
token_url = "https://XXXXXXXXX.de1.demo.crm.cloud.sap/sap/c4c/api/v1/iam-service/token"
token = RestClient.Get(token_url,headers )
 
aa = token.value.access_token
 
url = "https://XXXXXXXXX.de1.demo.crm.cloud.sap/sap/c4c/api/v1/document-service/documents/"
headers1 = { "Authorization": "Bearer " + str(aa),
           'Accept': 'application/json; charset=UTF-8'}
 
json_body = {
    'isSelected':'false',
    'isDisplayDocument':'true',
    'fileName': 'CPQ-Document.xlsx',
    'category': 'DOCUMENT',
    'type': '10001'
}
 
newdata=JsonHelper.Deserialize(JsonHelper.Serialize(json_body))
response = RestClient.Post(url, newdata, headers1, True)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H3 id="toc-hId-1106724256"&gt;Script Fix for &lt;SPAN class=""&gt;Quote&lt;/SPAN&gt; 2.0 Alternatives&lt;/H3&gt;&lt;P class=""&gt;The IronPython script issue in &lt;SPAN class=""&gt;Quote&lt;/SPAN&gt; 2.0, where newly added alternative items were incorrectly flagged and excluded from calculations, has been resolved.&lt;/P&gt;&lt;P class=""&gt;This fix ensures accurate total and product type calculations, making it essential for users who rely on scripting to manage quote alternatives efficiently.&lt;/P&gt;&lt;P class=""&gt;&lt;FONT color="#FF00FF"&gt;Here's an example script:&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;quote = QuoteHelper.Get("00021088")
item = quote.GetItemByItemNumber(1)
altProduct = ProductHelper.CreateProduct('00021088',item.QuoteItem)
alternativeItem = quote.AddItem(altProduct,1).AsMainItem
alternativeItem.ChangeItemTypeToAlternative(item.Id)  &lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/financial-management-blog-posts-by-sap/sap-cpq-2511-scripting-for-custom-quote-actions-amp-quote-item-custom/ba-p/14253497"/>
    <published>2025-10-26T14:38:16.450000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/from-sap-datasphere-to-a-local-llm-llama-3-1-hands-on-tutorial/ba-p/14253357</id>
    <title>From SAP Datasphere to a Local LLM (Llama 3.1)  â€” Hands-On Tutorial</title>
    <updated>2025-10-29T12:05:30.405000+01:00</updated>
    <author>
      <name>SethiR</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1792324</uri>
    </author>
    <content>&lt;P&gt;&lt;FONT size="6"&gt;&lt;BR /&gt;&lt;/FONT&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Introduction&amp;nbsp;&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;This post documents a small, reproducible pattern for bringing SAP Datasphere data to a local large language model (LLM) for lightweight analysis. The goal is simple: keep modeling and governance in Datasphere, pull a view into a Jupyter notebook with pandas, and let a local LLM produce machine-readable JSON that you can filter, join, or visualize. The prototype runs on a CPU-only laptop so anyone can follow along without special hardware.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;What this is&lt;/STRONG&gt;:&lt;BR /&gt;-&amp;nbsp;A step-by-step walkthrough that uses hdbcli to query a Datasphere view and Transformers to run Meta Llama 3.1 locally.&lt;BR /&gt;-&amp;nbsp;A row-by-row prompt pattern that returns compact JSON (total, average, flags) you can plug back into your DataFrame. A row-by-row prompt pattern that returns compact JSON (total, average, flags) you can plug back into your DataFrame.&lt;BR /&gt;-&amp;nbsp;A row-by-row prompt pattern that returns compact JSON (total, average, flags) you can plug back into your DataFrame.&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;What this is not:&lt;/STRONG&gt;&lt;BR /&gt;-&amp;nbsp;A benchmarking or performance guide. CPU runs are slow but convenient for learning.&lt;BR /&gt;-&amp;nbsp;A recommendation to hard-code credentials. The POC mirrors the original notebook for clarity; use environment variables or a vault in real projects.A recommendation to hard-code credentials. The POC mirrors the original notebook for clarity; use environment variables or a vault in real projects.&lt;BR /&gt;-&amp;nbsp;A pattern for sending sensitive data to external endpoints. Use test or masked data if you move past local inference. A pattern for sending sensitive data to external endpoints. Use test or masked data if you move past local inference.&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;What you will build:&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;A compact flow: SAP Datasphere View -&amp;gt; Python/Jupyter (hdbcli + pandas) -&amp;gt; row-level prompt -&amp;gt; Local LLM (Transformers) -&amp;gt; JSON back to DataFrame. The same prompts can be pointed to managed inference later for production.&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;H2 id="toc-hId-1763694472"&gt;Prerequisites&lt;/H2&gt;&lt;P&gt;1.&amp;nbsp;SAP Datasphere space with permission to create a Database User and a SQL View&lt;BR /&gt;2.&amp;nbsp;Python 3.10+ with Jupyter&lt;BR /&gt;3.&amp;nbsp;Libraries: pandas, hdbcli, transformers, torch, accelerate, ipython&lt;BR /&gt;4.&amp;nbsp;Hugging Face account + access token (accept access for the Llama 3.1 model)&lt;BR /&gt;&lt;BR /&gt;Security note: The POC code below uses inline credentials to mirror the original run. In real work, put secrets in environment variables or a vault and keep TLS validation enabled.&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Part A â€” SAP Datasphere&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;A1. Enable database access for the space&lt;/P&gt;&lt;P&gt;1.&amp;nbsp;Open SAP Datasphere -&amp;gt; Spaces -&amp;gt; select your space.&lt;BR /&gt;2.&amp;nbsp;Go to Database Access and confirm SQL access is enabled for the space&amp;nbsp;&lt;/P&gt;&lt;P&gt;A2. Create a Database User&lt;/P&gt;&lt;P&gt;1.&amp;nbsp;Database Access -&amp;gt; Database Users -&amp;gt; Create.Database Access -&amp;gt; Database Users -&amp;gt; Create.&lt;BR /&gt;2.&amp;nbsp;Grant only read privileges/necessary privileges to the schema/view you will query.&lt;BR /&gt;3.&amp;nbsp;Copy the SQL Endpoint (host) and port 443 for Python connectivity. Make sure you copy password and host details and store it in a safe place.&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;A3. Create a demo view with a few rows&lt;/P&gt;&lt;P&gt;Create a SQL View (or graphical view). Here we have taken&amp;nbsp; "ACN_DWC"."DemoView_SETHIR_PY" with columns:&lt;BR /&gt;Stud_ID, Stud_Fname, Stud_Lname, Stud_DOB, Maths, Physics, Chemistry, Total, Stud_Addr, Stud_Faname for the demo.&lt;BR /&gt;Make sure the view is exposed for Consumption.&lt;BR /&gt;Optional seed SQL you can adapt:&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-sql"&gt;&lt;code&gt;SELECT * FROM (
  SELECT 101 AS Stud_ID, 'Arjun'     AS Stud_Fname, 'Singh'    AS Stud_Lname, DATE'2012-04-06' AS Stud_DOB,
         80 AS Maths, 75 AS Physics, 80 AS Chemistry, 235 AS Total, 'Chandigarh' AS Stud_Addr, 'Sukhdev' AS Stud_Faname
  UNION ALL
  SELECT 102, 'Harpreet', 'Kaur',  DATE'2010-09-08', 85, 78, 85, 248, 'Ludhiana',  'Gurinder'
  UNION ALL
  SELECT 103, 'Gursimran','Gill',  DATE'2011-07-09', 95, 85, 90, 270, 'Amritsar',  'Balwinder'
  UNION ALL
  SELECT 104, 'Manpreet', 'Sidhu', DATE'2014-01-01', 90, 85, 95, 270, 'Patiala',   'Harjit'
  UNION ALL
  SELECT 105, 'Jasleen',  'Dhillon',DATE'2012-03-02', 89, 90, 75, 254, 'Jalandhar', 'Paramjit'
) AS t;
SELECT * FROM (
  SELECT 101 AS Stud_ID, 'Arjun'     AS Stud_Fname, 'Singh'    AS Stud_Lname, DATE'2012-04-06' AS Stud_DOB,
         80 AS Maths, 75 AS Physics, 80 AS Chemistry, 235 AS Total, 'Chandigarh' AS Stud_Addr, 'Sukhdev' AS Stud_Faname
  UNION ALL
  SELECT 102, 'Harpreet', 'Kaur',  DATE'2010-09-08', 85, 78, 85, 248, 'Ludhiana',  'Gurinder'
  UNION ALL
  SELECT 103, 'Gursimran','Gill',  DATE'2011-07-09', 95, 85, 90, 270, 'Amritsar',  'Balwinder'
  UNION ALL
  SELECT 104, 'Manpreet', 'Sidhu', DATE'2014-01-01', 90, 85, 95, 270, 'Patiala',   'Harjit'
  UNION ALL
  SELECT 105, 'Jasleen',  'Dhillon',DATE'2012-03-02', 89, 90, 75, 254, 'Jalandhar', 'Paramjit'
) AS t;&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;SAP Datasphere DB user :&amp;nbsp;&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="SAP Datasphere DB user screen" style="width: 521px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/332993i05D97A6906259C90/image-dimensions/521x754/is-moderation-mode/true?v=v2" width="521" height="754" role="button" title="Screenshot 2025-10-27 170111.png" alt="SAP Datasphere DB user screen" /&gt;&lt;span class="lia-inline-image-caption" onclick="event.preventDefault();"&gt;SAP Datasphere DB user screen&lt;/span&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Demo View :&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="SethiR_0-1761564892069.png" style="width: 709px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/332994i6C5415EEA2B13F92/image-dimensions/709x443/is-moderation-mode/true?v=v2" width="709" height="443" role="button" title="SethiR_0-1761564892069.png" alt="SethiR_0-1761564892069.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Part B â€” Local environment (quick path)&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="3"&gt;1. Create a project folder and venvCreate a project folder and venv&lt;BR /&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-bash"&gt;&lt;code&gt;mkdir datasphere-local-llm &amp;amp;&amp;amp; cd datasphere-local-llm
python -m venv .venv
# Windows: .venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;2.&amp;nbsp;Install requirements&amp;nbsp;&lt;/P&gt;&lt;pre class="lia-code-sample language-bash"&gt;&lt;code&gt;pip install hdbcli
pip install sqlalchemy
pip install sqlalchemy-hana
pip install pandas
pip install hdbcli
pip install transformers torch accelerate
pip install ipython&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;3.&amp;nbsp;Launch Jupyter&lt;/P&gt;&lt;pre class="lia-code-sample language-bash"&gt;&lt;code&gt;jupyter notebook&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Part C â€” Original POC notebook&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="3"&gt;1. Open a new notebook and write the code.&lt;BR /&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# --- Imports and setup (original)
import pandas as pd
from hdbcli import dbapi
import warnings
from transformers import pipeline
from IPython.display import display
warnings.filterwarnings('ignore')

# --- Inline credentials (POC-style; replace with environment variables for real use)
db_user = 'ACN_DWC#SETHIR_DB'
db_password = 'secret'
db_host = 'secret'
db_port = 443
db_schema = 'ACN_DWC'


connection = dbapi.connect(
    address = db_host,
    port = db_port,
    user = db_user,
    password = db_password,
    encrypt = True,
    sslValidCertificate = False   # POC-only convenience; prefer True in real use
)
print("Connected to SAP Datasphere - confirmation")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;2. If you see the output - "Connected to SAP Datasphere - confirmation" -- this implies you are on right track.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# --- Query the view
view_name = 'DemoView_SETHIR_PY'
sql_query = f'SELECT * FROM "{db_schema}"."{view_name}"'
print(f"Executing query: {sql_query}")

cursor = connection.cursor()
cursor.execute(sql_query)
rows = cursor.fetchall()
columns = [desc[0] for desc in cursor.description]

df = pd.DataFrame(rows, columns=columns)
display(df.head())&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;3. At this point, you should see the outcome of the view that you have. The last statement is used for displaying the data in the form of pandas dataFrame.&lt;BR /&gt;&lt;BR /&gt;4. Now we need to prepare our data , so that the LLM can process it. This can be done by converting our data in the form of string format.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# ==============================================================================
#Prepare Data and Interact with OpenLLM
# ==============================================================================

# Convert DataFrame to a string
data_as_string = df.to_string(index=False)
print("\n--- Data Prepared for LLM ---")
print("The DataFrame has been converted to the following string format:")
print(data_as_string[:300] + "\n...")  # snippet preview&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;5. Prepare data pipeline :&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# --- LLM imports
import os
import torch
import json
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline

# --- LLM Configuration
MODEL_ID = "meta-llama/Meta-Llama-3.1-8B-Instruct"

def load_llama_pipeline():
    """
    Loads the quantized Llama 3 model, tokenizer, and the text-generation pipeline.
    """
    print("\n--- Loading Llama 3 Model ---")
    hf_token = "secret"  # replace with your HF token; accept model access first

    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=hf_token)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        token=hf_token,
        quantization_config=quantization_config,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )

    print("Model loaded. Creating text generation pipeline...")
    return pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
    )&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;6. Build the prompt for the LLM model :&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# --- Prompt builder 
def build_student_analysis_prompt(tokenizer, student_data: pd.Series) -&amp;gt; str:
    """
    Builds a structured prompt for Llama 3 to analyze student marks.
    """
    input_text = (
        f"Student {student_data['Stud_Fname']} {student_data['Stud_Lname']} "
        f"(ID: {student_data['Stud_ID']}) scored {student_data['Maths']} in Maths, "
        f"{student_data['Physics']} in Physics, and {student_data['Chemistry']} in Chemistry."
    )

    messages = [
        {
            "role": "system",
            "content": (
                "You analyze one student's marks and return ONLY a valid JSON object. "
                "Output must start with '{' and end with '}'. No commentary, no markdown, no backticks.\n\n"
                "Schema (keys and types MUST match exactly):\n"
                "{\n"
                '  "total_marks": &amp;lt;int&amp;gt;,\n'
                '  "average_percentage": &amp;lt;float&amp;gt;,\n'
                '  "is_top_performer": &amp;lt;boolean&amp;gt;\n'
                "}\n\n"
                "Rules:\n"
                "1) total_marks = Maths + Physics + Chemistry (each out of 100).\n"
                "2) average_percentage = total_marks / 3.\n"
                "3) Round average_percentage to TWO decimals.\n"
                "4) is_top_performer = true if average_percentage &amp;gt; 80.0; else false.\n"
                "5) Use lowercase true/false for booleans.\n"
                "6) Do not include extra keys. Do not include trailing commas.\n"
                "7) Return JSON only."
            )
        },
        {
            "role": "user",
            "content": input_text
        }
    ]
    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;7. Do the analysis :&amp;nbsp;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# --- Row-by-row analysis
def analyze_student_data(df, text_pipeline):
    """
    Analyzes the student DataFrame row by row using the LLM pipeline.
    """
    print("\n--- Starting Student Performance Analysis with Llama 3 ---")
    results = []
    for index, row in df.iterrows():
        print(f"\nAnalyzing student ID: {row['Stud_ID']}...")

        prompt = build_student_analysis_prompt(text_pipeline.tokenizer, row)

        raw_output = text_pipeline(
            prompt,
            max_new_tokens=128,
            do_sample=False,  # deterministic
            temperature=None,
            top_p=None,
        )[0]['generated_text']

        json_response_str = raw_output[len(prompt):].strip()
        try:
            analysis_result = json.loads(json_response_str)
            results.append(analysis_result)
            print("Analysis successful.")
        except json.JSONDecodeError:
            print(f"  &amp;gt; Failed to decode JSON from model output.")
            print(f"  &amp;gt; Raw model output: {json_response_str}")
            results.append({"error": "Invalid JSON output", "raw_output": json_response_str})

    print("\n--- Analysis Complete ---")
    return pd.DataFrame(results)

# --- Main
if __name__ == "__main__":
    if df is not None and not df.empty:
        try:
            llm_pipeline = load_llama_pipeline()
            analysis_df = analyze_student_data(df, llm_pipeline)

            if analysis_df is not None:
                final_df = pd.concat([df.reset_index(drop=True), analysis_df.reset_index(drop=True)], axis=1)

                print("\n--- Full Data with LLM Analysis ---")
                display(final_df)

                print("\n--- Top Performing Students (Average &amp;gt; 80%) ---")
                top_performers = final_df[final_df['is_top_performer'] == True]

                if not top_performers.empty:
                    display(top_performers[['Stud_ID', 'Stud_Fname', 'Stud_Lname', 'total_marks', 'average_percentage']])
                else:
                    print("No students found with an average greater than 80%.")

        except Exception as e:
            print(f"\nAn unexpected error occurred during the analysis process: {e}")
    else:
        print("\nDataFrame is empty. Cannot proceed with analysis.")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;8.&amp;nbsp;Example output (simulated for the final cell)&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-markup"&gt;&lt;code&gt;Full Data with LLM Analysis (excerpt)

Stud_ID  Stud_Fname  Stud_Lname  Maths  Physics  Chemistry  total_marks  average_percentage  is_top_performer
101      Arjun       Singh       80     75       80         235          78.33               false
102      Harpreet    Kaur        85     78       85         248          82.67               true
103      Gursimran   Gill        95     85       90         270          90.00               true
104      Manpreet    Sidhu       90     85       95         270          90.00               true
105      Jasleen     Dhillon     89     90       75         254          84.67               true

Top Performing Students (Average &amp;gt; 80%)

Stud_ID  Stud_Fname  Stud_Lname  total_marks  average_percentage
102      Harpreet    Kaur        248          82.67
103      Gursimran   Gill        270          90.00
104      Manpreet    Sidhu       270          90.00
105      Jasleen     Dhillon     254          84.67&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;FONT size="5"&gt;Part D â€”&amp;nbsp;Production path (same pattern, managed inference)&lt;/FONT&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="3"&gt;When performance or scale matters, swap the local pipeline for a hosted endpoint and keep the Datasphere extraction and prompts identical. When performance or scale matters, swap the local pipeline for a hosted endpoint and keep the Datasphere extraction and prompts identical.&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="3"&gt;1.&amp;nbsp;Databricks Model Serving / Foundation Model Endpoints (REST).&lt;BR /&gt;2.&amp;nbsp;Hugging Face Inference Endpoints (private endpoints; REST with your HF token)&lt;BR /&gt;3.&amp;nbsp;SAP AI Core (containerized model hosting; REST)SAP AI Core (containerized model hosting; REST)&lt;BR /&gt;&lt;BR /&gt;Minimal REST skeleton --&lt;BR /&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import requests, os, json
endpoint = os.getenv("INFERENCE_URL")
token = os.getenv("INFERENCE_TOKEN")
r = requests.post(endpoint, headers={"Authorization": f"Bearer {token}"},
                  json={"inputs": "your_prompt_here", "parameters": {"max_new_tokens": 128, "temperature": 0.0}})
print(r.json())&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&lt;FONT size="3"&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;FONT size="5"&gt;Conclusion ---&lt;BR /&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;This walkthrough demonstrated how to extract data from SAP Datasphere, process it in pandas, and apply a local LLM for row-level analysis that returns clean, machine-readable JSON. The pattern is intentionally small and portable: you can keep iterating locally to refine prompts and outputs, then swap the model call for a managed endpoint (Databricks, Hugging Face, or SAP AI Core) when performance, cost control, or governance call for it. From here, natural next steps include batching larger datasets, persisting results back to a database table, wiring the outputs to SAP Analytics Cloud dashboards, and adding guardrails around data privacy and prompt consistency.&lt;/P&gt;&lt;P&gt;I would be excited to hear how you adapt this to your solutions. Feel free to reach out to me or comment.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/from-sap-datasphere-to-a-local-llm-llama-3-1-hands-on-tutorial/ba-p/14253357"/>
    <published>2025-10-29T12:05:30.405000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/sap-datasphere-export-data-of-analyticalmodel-via-odata-url-amp-oauth/ba-p/14256993</id>
    <title>SAP Datasphere : Export Data of AnalyticalModel via Odata URL &amp; Oauth Client of type Technical User</title>
    <updated>2025-11-05T07:58:30.289000+01:00</updated>
    <author>
      <name>vikasparmar88</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1528256</uri>
    </author>
    <content>&lt;P&gt;In this blog, I have explained how to export data from an analytical model into CSV file securely using an OData URL and a technical user OAuth client.&lt;/P&gt;&lt;P&gt;Step - 1) Create an Oauth Client with Purpose as Technical User and select required roles. get client ID and secret and save it.&amp;nbsp;&lt;A href="https://help.sap.com/docs/SAP_DATASPHERE/9f804b8efa8043539289f42f372c4862/88b13468fc3c4ebd972bcb8faa6cafbf.html" target="_self" rel="noopener noreferrer"&gt;How to Guide&lt;/A&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Oauth.png" style="width: 347px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334244iFF68CE80547515B7/image-dimensions/347x368/is-moderation-mode/true?v=v2" width="347" height="368" role="button" title="Oauth.png" alt="Oauth.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Step - 2) Create Odata_Tech_User_OauthClient.py file with below code&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;# --- IMPORTS ---
import requests              # For making HTTP requests to token and OData endpoints
import pandas as pd          # For handling tabular data from the OData response
import os                    # For file path resolution and environment variable access
from dotenv import load_dotenv  # For loading credentials from a .env file

# --- LOAD ENV VARIABLES ---
load_dotenv()                # Load environment variables from .env file into the runtime

# --- CONFIG: Read credentials and endpoints from environment ---
odata_url = os.getenv("ODATA_URL")             # OData service endpoint
token_url = os.getenv("TOKEN_URL")             # OAuth token endpoint
client_id = os.getenv("CLIENT_ID")             # OAuth client ID
client_secret = os.getenv("CLIENT_SECRET")     # OAuth client secret

# --- GET TOKEN: Request access token using client credentials ---
token_payload = {
    "grant_type": "client_credentials",        # OAuth flow type
    "client_id": client_id,                    # Injected from .env
    "client_secret": client_secret             # Injected from .env
}
token_resp = requests.post(token_url, data=token_payload)  # POST request to token endpoint
token_resp.raise_for_status()                 # Raise error if token request fails
access_token = token_resp.json()["access_token"]  # Extract access token from response

# --- CALL ODATA SERVICE: Fetch data using bearer token ---
headers = {"Authorization": f"Bearer {access_token}"}  # Auth header with token
response = requests.get(odata_url, headers=headers)    # GET request to OData endpoint
response.raise_for_status()                            # Raise error if data fetch fails

# --- PARSE RESULTS: Convert JSON payload to DataFrame ---
data = response.json()["value"]         # Extract 'value' list from OData response
df = pd.DataFrame(data)                 # Convert list of records to pandas DataFrame

# --- DISPLAY OR EXPORT: Show preview and optionally save to CSV ---
print()
print("ðŸ” Displaying top rows for quick inspection:")
print()
print(df.head())                        # Display top rows for quick inspection
print()

# --- Extract view name from OData URL ---
view_name = odata_url.rstrip("/").split("/")[-1]  # Gets 'AM_EXPORT' from the URL

# --- Construct filename ---
filename = f"{view_name}.csv"

# --- Display and save ---
df.to_csv(filename, index=False)
print(f"ðŸ“ Exported data Saved to: {os.path.abspath(filename)}")&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Step - 3) Create .env file with all values of variables.&lt;/P&gt;&lt;P&gt;ODATA_URL : Copy the Odata link from analytical model&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="odata.png" style="width: 630px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334255i61C52D8A5777AD69/image-dimensions/630x162/is-moderation-mode/true?v=v2" width="630" height="162" role="button" title="odata.png" alt="odata.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="odata.png" style="width: 517px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334257iF036F0A98768B92D/image-dimensions/517x297/is-moderation-mode/true?v=v2" width="517" height="297" role="button" title="odata.png" alt="odata.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;TOKEN_URL : get it from Datasphere -&amp;gt; System -&amp;gt; App Integration page&lt;/P&gt;&lt;P&gt;CLIENT ID &amp;amp; Secret : Get it from Step-1&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Variables.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334246iBF8AA617FEDF18B0/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Variables.png" alt="Variables.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Step-&amp;nbsp; 4) Create .bat file with blow code&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt; off

chcp 65001 &amp;gt;nul

REM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REM ðŸš€ ODATA TECH USER OAUTH CLIENT EXECUTION SCRIPT
REM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


echo.
echo ==============================================
echo ðŸ”„ Starting OData export process...
echo ==============================================

REM --- Navigate to script directory ---
cd /d "%~dp0"

REM --- Run the Python script ---
echo ðŸ Running Python script: Odata_Tech_User_OauthClient.py
python Odata_Tech_User_OauthClient.py

REM --- Completion message ---
echo.
echo âœ… Script execution completed.
echo ==============================================

REM --- Keep window open ---
pause&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Keep all files in same directory&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Folder.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334250i25167CB60103AFB0/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Folder.png" alt="Folder.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;!--  StartFragment   --&gt;&lt;/P&gt;&lt;P&gt;Once everything is set up, just double-click the .bat&amp;nbsp;file to run the process. It will execute the Python script and, once finished, generate a .csv file name exactly same name as the analytical model name. As part of the execution, the first five rows of data will also be displayed on screen for quick preview&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="output.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334261i1BE9F41A80C0F984/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="output.png" alt="output.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Folder.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/334248iA75EC24BCB3F6067/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Folder.png" alt="Folder.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Thanks&lt;/P&gt;&lt;P&gt;Vikas Parmar&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/sap-datasphere-export-data-of-analyticalmodel-via-odata-url-amp-oauth/ba-p/14256993"/>
    <published>2025-11-05T07:58:30.289000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/sap-learning-blog-posts/want-to-explore-developing-ai-workflows-with-the-python-machine-learning/ba-p/14263178</id>
    <title>Want to explore developing AI workflows with the Python Machine Learning Client for SAP HANA?</title>
    <updated>2025-11-07T16:40:45.744000+01:00</updated>
    <author>
      <name>Margit_Wagner</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/491</uri>
    </author>
    <content>&lt;P data-unlink="true"&gt;&lt;FONT size="3"&gt;&lt;SPAN&gt;I&amp;nbsp;recommend to access our&amp;nbsp;&lt;/SPAN&gt;&lt;/FONT&gt;&lt;A title="Developing AI workflows with the Python Machine Learning Client for SAP HANA" href="https://learning.sap.com/learning-journeys/developing-ai-workflows-with-the-python-machine-learning-client-for-sap-hana" target="_blank" rel="noopener noreferrer"&gt;Developing AI workflows with the Python Machine Learning Client for SAP HANA&lt;/A&gt;&amp;nbsp; learning journey.&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;Leaning Objectives&lt;/STRONG&gt;&lt;BR /&gt;By the end of this learning journey, learners will be able to:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Know how to install and configure the Python Machine Learning Client (hana-ml) and connect securely to SAP HANA Cloud.&lt;/LI&gt;&lt;LI&gt;Apply HANA DataFrames to access, prepare, and explore SAP HANA data for machine learning tasks.&lt;/LI&gt;&lt;LI&gt;Think critically to build, train, and deploy machine learning models using PAL functions for regression, classification, and time-series forecasting.&lt;/LI&gt;&lt;LI&gt;Impact business outcomes by using real-world datasets (e.g., housing prices, employee churn) to complete end-to-end ML workflows.&lt;/LI&gt;&lt;LI&gt;Evaluate models using appropriate metrics to assess performance, robustness, and business relevance.&lt;/LI&gt;&lt;/UL&gt;&lt;P data-unlink="true"&gt;&lt;STRONG&gt;Goals&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Get started with SAP: Building the basics&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;Develop your expertise: Skill deepening learning&lt;/LI&gt;&lt;LI&gt;Excel in your expertise: Advanced specialization learning&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Prerequisites&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV&gt;&lt;P&gt;Basic python expertise&lt;/P&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Please post you question related&amp;nbsp;to the digital learning Journey in the&amp;nbsp;&lt;/STRONG&gt;&lt;A href="https://groups.community.sap.com/t5/sap-learning-q-a/qa-p/learningqanda-board" target="_blank" rel="noopener noreferrer"&gt;&lt;STRONG&gt;Q&amp;amp;A area&lt;/STRONG&gt;&lt;/A&gt;&lt;STRONG&gt;.&amp;nbsp;&lt;/STRONG&gt;&lt;/P&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;P&gt;Our SAP Learning Experts will get back to you as soon as possible!&amp;nbsp;&lt;BR /&gt;We are here to support you.&lt;/P&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;BR /&gt;I appreciate your feedback and we will make sure to continue sharing interesting topics.&lt;BR /&gt;&lt;BR /&gt;Kind regards&lt;BR /&gt;Margit&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;</content>
    <link href="https://community.sap.com/t5/sap-learning-blog-posts/want-to-explore-developing-ai-workflows-with-the-python-machine-learning/ba-p/14263178"/>
    <published>2025-11-07T16:40:45.744000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/how-to-reduce-prompt-token-costs-using-toon-save-money/ba-p/14267265</id>
    <title>How to Reduce Prompt Token Costs Using Toon = Save Money</title>
    <updated>2025-11-12T17:34:51.069000+01:00</updated>
    <author>
      <name>Yogananda</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/75</uri>
    </author>
    <content>&lt;P&gt;As you already might know prompt tokens are the backbone of communication with large language models (LLMs). However, as usage scales, token costs can quickly become a significant expense. If youâ€™re using &lt;STRONG&gt;Toon&lt;/STRONG&gt;â€”a tool designed for optimizing prompt workflowsâ€”you can dramatically cut down on these costs without sacrificing performance.&lt;/P&gt;&lt;H3 id="toc-hId-1893818944"&gt;&lt;STRONG&gt;Why Token Costs Matter&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;Every interaction with an LLM consumes tokens. These tokens represent:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Input tokens&lt;/STRONG&gt;: The text you send to the model.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Output tokens&lt;/STRONG&gt;: The text generated by the model.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;The more tokens you use, the higher your bill. For businesses running thousands of prompts daily, even small inefficiencies can lead to big costs.&lt;/P&gt;&lt;P&gt;&lt;FONT color="#FF00FF"&gt;&lt;STRONG&gt;Example:&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Original prompt: &lt;EM&gt;â€œPlease summarize the following text in a clear and concise manner, highlighting the key points and provide a RACI Matrix for S/4 HANAâ€&lt;/EM&gt;&lt;/LI&gt;&lt;LI&gt;Compressed prompt: &lt;EM&gt;â€œSummarize key points.â€&lt;/EM&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;H3 id="toc-hId-1697305439"&gt;&lt;FONT color="#800080"&gt;&lt;STRONG&gt;What is TOON?&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/H3&gt;&lt;P&gt;TOON is a &lt;STRONG&gt;compact, human-readable serialization format&lt;/STRONG&gt; designed for passing structured data to LLMs with &lt;STRONG&gt;significantly reduced token usage&lt;/STRONG&gt;. It acts as a &lt;STRONG&gt;lossless, drop-in representation of JSON&lt;/STRONG&gt;, optimized for token efficiency.&lt;/P&gt;&lt;H3 id="toc-hId-1500791934"&gt;&lt;FONT color="#800080"&gt;&lt;STRONG&gt;Why Use TOON?&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/H3&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Token-efficient&lt;/STRONG&gt;: Saves &lt;STRONG&gt;30â€“60% tokens&lt;/STRONG&gt; compared to formatted JSON for large uniform arrays.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;LLM-friendly&lt;/STRONG&gt;: Explicit lengths and fields improve parsing and validation.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Minimal syntax&lt;/STRONG&gt;: Removes redundant punctuation (braces, quotes).&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Tabular arrays&lt;/STRONG&gt;: Declare keys once, stream data as rows.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Optional key folding&lt;/STRONG&gt;: Collapses nested chains into dotted paths for fewer tokens.&lt;/LI&gt;&lt;/UL&gt;&lt;H3 id="toc-hId-1304278429"&gt;&lt;STRONG&gt;Benchmarks&lt;/STRONG&gt;&lt;/H3&gt;&lt;UL&gt;&lt;LI&gt;TOON uses &lt;STRONG&gt;39.6% fewer tokens&lt;/STRONG&gt; than JSON while improving retrieval accuracy (73.9% vs 69.7%).&lt;/LI&gt;&lt;LI&gt;For uniform tabular data, TOON is slightly larger than CSV (+6%) but far smaller than JSON (-58%).&lt;/LI&gt;&lt;/UL&gt;&lt;H3 id="toc-hId-1107764924"&gt;&lt;STRONG&gt;When NOT to Use TOON&lt;/STRONG&gt;&lt;/H3&gt;&lt;UL&gt;&lt;LI&gt;Deeply nested or non-uniform structures â†’ JSON may be better.&lt;/LI&gt;&lt;LI&gt;Pure tabular data â†’ CSV is smaller.&lt;/LI&gt;&lt;LI&gt;Latency-critical apps â†’ Benchmark first; compact JSON might be faster.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;How to Use TOON&lt;/STRONG&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-abap"&gt;&lt;code&gt;NPM Lib
npm install -format/toon

Python
https://github.com/xaviviro/python-toon&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;&amp;nbsp;&lt;A href="https://github.com/toon-format/toon" target="_blank" rel="noopener nofollow noreferrer"&gt;https://github.com/toon-format/toon&lt;/A&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;A href="https://github.com/toon-format/spec" target="_blank" rel="noopener nofollow noreferrer"&gt;https://github.com/toon-format/spec&lt;/A&gt;&amp;nbsp;&lt;/P&gt;&lt;H3 id="toc-hId-911251419"&gt;&lt;STRONG&gt;Why TOON Matters while your working for different SAP Product LoBs&lt;/STRONG&gt;&lt;/H3&gt;&lt;OL&gt;&lt;LI&gt;&lt;P&gt;&lt;STRONG&gt;Token Cost Reduction&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;SAP S/4 HANA systems handle large datasets (e.g., thousands of line items in a purchase order).&lt;/LI&gt;&lt;LI&gt;JSON representation of these datasets is expensive in token terms.&lt;/LI&gt;&lt;LI&gt;TOON compresses this data by &lt;STRONG&gt;30â€“60%&lt;/STRONG&gt;, reducing LLM API costs significantly.&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;LI&gt;&lt;P&gt;&lt;STRONG&gt;Performance Gains&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Smaller prompts mean &lt;STRONG&gt;lower latency&lt;/STRONG&gt; and &lt;STRONG&gt;faster response times&lt;/STRONG&gt;.&lt;/LI&gt;&lt;LI&gt;This is critical for real-time SAP applications like &lt;STRONG&gt;Joule for procurement&lt;/STRONG&gt; or &lt;STRONG&gt;HR assistants&lt;/STRONG&gt;.&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;LI&gt;&lt;P&gt;&lt;STRONG&gt;Improved Accuracy&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;TOONâ€™s explicit structure (e.g., tabular arrays with declared keys) helps LLMs parse data better.&lt;/LI&gt;&lt;LI&gt;This reduces hallucinations in SAP workflows like &lt;STRONG&gt;financial reconciliation&lt;/STRONG&gt; or &lt;STRONG&gt;compliance checks&lt;/STRONG&gt;.&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&lt;STRONG&gt;High Level Flow&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Yogananda_0-1762963441825.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/339677iCF14D8DA461534BD/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="Yogananda_0-1762963441825.png" alt="Yogananda_0-1762963441825.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H2 id="toc-hId-585655195"&gt;&lt;STRONG&gt;Integration Approach&lt;/STRONG&gt;&lt;/H2&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Middleware Layer&lt;/STRONG&gt;: Convert SAP OData/JSON responses to TOON before sending to LLM.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;SAP BTP Extension&lt;/STRONG&gt;: Implement TOON conversion in &lt;STRONG&gt;CAP,&amp;nbsp;Cloud Foundry apps&lt;/STRONG&gt; or &lt;STRONG&gt;Kyma runtime or SAP Databricks, AI Core Models you have selected&lt;/STRONG&gt;.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Prompt Wrapping&lt;/STRONG&gt;: Always wrap TOON in fenced code blocks for LLM clarity&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="G5qMVMlacAAosFZ.png" style="width: 800px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/340395iDB351B41F1A46C83/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="G5qMVMlacAAosFZ.png" alt="G5qMVMlacAAosFZ.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H2 id="toc-hId-389141690"&gt;&lt;STRONG&gt;Best Practices&lt;/STRONG&gt;&lt;/H2&gt;&lt;UL&gt;&lt;LI&gt;Use &lt;STRONG&gt;tab-delimited rows&lt;/STRONG&gt; for extra token savings.&lt;/LI&gt;&lt;LI&gt;Benchmark TOON vs JSON for your specific SAP dataset.&lt;/LI&gt;&lt;LI&gt;Cache static context to avoid repeated token costs.&lt;/LI&gt;&lt;/UL&gt;&lt;H2 id="toc-hId-192628185"&gt;&lt;STRONG&gt;Conclusion&lt;/STRONG&gt;&lt;/H2&gt;&lt;P&gt;TOON offers a simple yet powerful way to reduce LLM costs in your SAP landscape environments. By compressing structured data without losing meaning, SAP teams can achieve:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Up to 60% cost savings&lt;/STRONG&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Faster response times&lt;/STRONG&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Improved AI accuracy&lt;/STRONG&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;As SAP continues its AI journey, adopting TOON can make intelligent automation more scalable and cost-effective.&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/how-to-reduce-prompt-token-costs-using-toon-save-money/ba-p/14267265"/>
    <published>2025-11-12T17:34:51.069000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/sap-rpt-1-context-model-vs-training-classical-models-the-models-battle/ba-p/14268507</id>
    <title>SAP RPT-1 Context Model vs. Training Classical Models: The Models Battle (Python Hands-on)</title>
    <updated>2025-11-20T07:50:27.670000+01:00</updated>
    <author>
      <name>nicolasestevan</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1198632</uri>
    </author>
    <content>&lt;H2 id="toc-hId-1764768715"&gt;&lt;span class="lia-unicode-emoji" title=":collision:"&gt;ðŸ’¥&lt;/span&gt;The Models Battle&lt;/H2&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_5-1763206328497.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341535i2A2C9A98D24BF43B/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_5-1763206328497.png" alt="nicolasestevan_5-1763206328497.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Predictive modeling is becoming a built-in capability across SAP, improving how teams handle forecasting, pricing, and planning. &lt;STRONG&gt;Many SAP professionals, however, arenâ€™t machine-learning specialists&lt;/STRONG&gt;, and traditional models often demand extensive setup, tuning, and repeated training, which slows down new ideas.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;SAP RPT-1&lt;/STRONG&gt; offers a simpler path. Itâ€™s a pretrained model from SAP, also available in an OSS version, that lets developers and consultants produce predictions with far less technical effort, no deep ML background required.&lt;/P&gt;&lt;P&gt;I've explored SAP RPT-1 hands-on, comparing it with traditional regressors using Python and a real public vehicles price dataset.&amp;nbsp;&lt;/P&gt;&lt;BLOCKQUOTE&gt;&lt;P&gt;&lt;STRONG&gt;Goal:&lt;/STRONG&gt; To see (as a non Data Scientist) how &lt;STRONG&gt;SAP RPT-1&lt;/STRONG&gt; behaves in practice, what advantages and limits it shows, and when it could make sense in a predictive scenario.&lt;/P&gt;&lt;/BLOCKQUOTE&gt;&lt;P&gt;Usually for real-world scenario, the right approach would be consume the SAP RPT-1 though the available and simplified API, but for studies proposal and fair comparision over othe traditional ML models, the &lt;STRONG&gt;OSS&lt;/STRONG&gt; fits perfectly for it:&lt;/P&gt;&lt;HR /&gt;&lt;H2 id="toc-hId-1568255210"&gt;&lt;span class="lia-unicode-emoji" title=":thinking_face:"&gt;ðŸ¤”&lt;/span&gt;&amp;nbsp;SAP RPT-1 vs Traditional Machine Learning - Core Differences&lt;/H2&gt;&lt;P&gt;Before diving into the code, letâ€™s quickly revisit how&lt;STRONG&gt; traditional ML&lt;/STRONG&gt; models work:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Training-based models like Random Forest, LightGBM, and Linear Regression learn patterns directly from data.&amp;nbsp;&lt;/LI&gt;&lt;LI&gt;They require hundreds or thousands of examples to tune their internal parameters.&lt;/LI&gt;&lt;LI&gt;Their performance depends heavily on data quantity and quality.&lt;/LI&gt;&lt;LI&gt;The more relevant examples they see, the smarter they get.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;On the other hand, &lt;STRONG&gt;SAP RPT-1 f&lt;/STRONG&gt;ollows a different philosophy. Itâ€™s part of the RPT (Representational Predictive Transformer) family, pretrained on a wide variety of business and contextual data. This means:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;You donâ€™t "train" it in the traditional sense. Instead, it uses context embeddings to predict outcomes.&lt;/LI&gt;&lt;LI&gt;It can be used immediately, even with smaller datasets.&lt;/LI&gt;&lt;LI&gt;The OSS version allows developers to experiment directly in Python.&lt;/LI&gt;&lt;LI&gt;No special SAP backend required.&lt;/LI&gt;&lt;/UL&gt;&lt;BLOCKQUOTE&gt;&lt;P&gt;&lt;STRONG&gt;Outcome:&lt;/STRONG&gt; Traditional ML models learn from high amount of data. SAP RPT-1 already knows how to deal with small context amount of data.&lt;/P&gt;&lt;/BLOCKQUOTE&gt;&lt;HR /&gt;&lt;H2 id="toc-hId-1371741705"&gt;&lt;span class="lia-unicode-emoji" title=":desktop_computer:"&gt;ðŸ–¥&lt;/span&gt;&amp;nbsp;The Experiment - Setup &amp;amp; Dataset&amp;nbsp;&lt;/H2&gt;&lt;div class="lia-spoiler-container"&gt;&lt;a class="lia-spoiler-link" href="#" rel="nofollow noopener noreferrer"&gt;Spoiler&lt;/a&gt;&lt;noscript&gt; (Highlight to read)&lt;/noscript&gt;&lt;div class="lia-spoiler-border"&gt;&lt;div class="lia-spoiler-content"&gt;Don't worry on "playing puzzles" copying + pasting code below. The full version is available for download at end!&lt;/div&gt;&lt;noscript&gt;&lt;div class="lia-spoiler-noscript-container"&gt;&lt;div class="lia-spoiler-noscript-content"&gt;Don't worry on "playing puzzles" copying + pasting code below. The full version is available for download at end!&lt;/div&gt;&lt;/div&gt;&lt;/noscript&gt;&lt;/div&gt;&lt;/div&gt;&lt;P&gt;To make this comparison tangible, I built a simple yet realistic Python experiment to predict vehicle selling prices using a public dataset containing car attributes like make, model, year, transmission, and mileage.&lt;/P&gt;&lt;P&gt;Why vehicle pricing? Because itâ€™s an intuitive example where both traditional machine learning and pretrained AI models can be applie, and it helps visualize how prediction quality evolves as the sample size grows.&lt;/P&gt;&lt;P&gt;This entire analysis runs on a local Python environment&amp;nbsp;with the following stack:&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;import os
import gc
import warnings
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LinearRegression
from sap_rpt_oss import SAP_RPT_OSS_Regressor
import lightgbm as lgb&lt;/code&gt;&lt;/pre&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;pandas&lt;/STRONG&gt; and &lt;STRONG&gt;numpy&lt;/STRONG&gt; for data manipulation&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;scikit-learn&lt;/STRONG&gt; for classical ML regressors (R&lt;STRONG&gt;andom Forest, Linear Regression&lt;/STRONG&gt;)&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;LightGBM&lt;/STRONG&gt; for gradient &lt;STRONG&gt;boosting&lt;/STRONG&gt; comparison&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;sap_rpt_oss&lt;/STRONG&gt; â€” the open-source Python version of &lt;STRONG&gt;SAPâ€™s RPT-1 model&lt;/STRONG&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;matplotlib&lt;/STRONG&gt; for all &lt;STRONG&gt;visualizations&lt;/STRONG&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;BLOCKQUOTE&gt;&lt;P&gt;&lt;STRONG&gt;SAP RPT-1 OSS &lt;/STRONG&gt;can be downloaded installed following official Hugging Face:&amp;nbsp;&lt;A title="https://huggingface.co/SAP/sap-rpt-1-oss?library=sap-rpt-1-oss" href="https://huggingface.co/SAP/sap-rpt-1-oss?library=sap-rpt-1-oss" target="_blank" rel="noopener nofollow noreferrer"&gt;https://huggingface.co/SAP/sap-rpt-1-oss?library=sap-rpt-1-oss&lt;/A&gt;&amp;nbsp;. Python can be installed with executable download on Windows, or via &lt;STRONG&gt;Home Brew&lt;/STRONG&gt; for Mac and &lt;STRONG&gt;apt&lt;/STRONG&gt; commands for Linux. Libraries dependencies can be downloaded with &lt;STRONG&gt;pip&lt;/STRONG&gt; commands. Googling it may not be a road blocker.&lt;/P&gt;&lt;/BLOCKQUOTE&gt;&lt;P&gt;We use a sample&amp;nbsp;vehicle sales dataset. The complete file is about to 88Mb but for such experiment a restricted sample of 20k as it's more than enough to prove our the concept, still it's faster and consuming less computing resources.&lt;/P&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;TABLE border="1" width="498px"&gt;&lt;TBODY&gt;&lt;TR&gt;&lt;TD&gt;&lt;STRONG&gt;Feature&lt;/STRONG&gt;&lt;/TD&gt;&lt;TD&gt;&lt;STRONG&gt;Description&lt;/STRONG&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;year&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Vehicle model year&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;make&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Brand (e.g., Toyota, Ford, BMW)&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;model&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Specific model name&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;body&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Type (SUV, Sedan, etc.)&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;transmission&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Gear type&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;odometer&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Vehicle mileage&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;color&lt;/CODE&gt;, &lt;CODE&gt;interior&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;Visual attributes&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="248.57px" height="30px"&gt;&lt;CODE&gt;sellingprice&lt;/CODE&gt;&lt;/TD&gt;&lt;TD width="248.43px" height="30px"&gt;The target variable to predict&lt;/TD&gt;&lt;/TR&gt;&lt;/TBODY&gt;&lt;/TABLE&gt;&lt;P&gt;&lt;STRONG&gt;&lt;span class="lia-unicode-emoji" title=":bar_chart:"&gt;ðŸ“Š&lt;/span&gt;&amp;nbsp;Dataset Download:&lt;/STRONG&gt;&amp;nbsp;&lt;A title="https://www.kaggle.com/datasets/syedanwarafridi/vehicle-sales-data?resource=download" href="https://www.kaggle.com/datasets/syedanwarafridi/vehicle-sales-data?resource=download" target="_blank" rel="noopener nofollow noreferrer"&gt;https://www.kaggle.com/datasets/syedanwarafridi/vehicle-sales-data?resource=download&lt;/A&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;The dataset is loaded and preprocessed in a few simple steps:&lt;/P&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;df = pd.read_csv("car_prices.csv").sample(n=20000, random_state=42)

# Fill missing values for categorical columns
fill_defaults = {
    'make': 'Other', 'model': 'Other', 'color': 'Other',
    'interior': 'Unknown', 'body': 'Unknown', 'transmission': 'Unknown'
}
for col, val in fill_defaults.items():
    df[col] = df[col].fillna(val)

X = df[["year", "make", "model", "body", "transmission", "odometer", "color", "interior"]]
y = df["sellingprice"]&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;At this point, the stage is set:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;The data is clean.&lt;/LI&gt;&lt;LI&gt;The environment is ready.&lt;/LI&gt;&lt;LI&gt;All models, traditionals and SAP RPT-1, are ready to be tested under identical conditions.&lt;/LI&gt;&lt;/UL&gt;&lt;HR /&gt;&lt;H2 id="toc-hId-1175228200"&gt;&lt;span class="lia-unicode-emoji" title=":robot_face:"&gt;ðŸ¤–&lt;/span&gt;&amp;nbsp;Training the Models - Three different ones&lt;/H2&gt;&lt;P&gt;With the dataset ready, the &lt;STRONG&gt;next step&lt;/STRONG&gt; is to run each model under the same conditions: &lt;STRONG&gt;same features, same target, same train/test split and same random seed&lt;/STRONG&gt;. This ensures the comparison is fair and repeatable.&lt;/P&gt;&lt;P&gt;We evaluate prediction performance using &lt;STRONG&gt;RÂ² (coefficient of determination)&lt;/STRONG&gt;, which indicates how much of the price variation the model can explain (1.0 = perfect prediction).&lt;/P&gt;&lt;HR /&gt;&lt;H3 id="toc-hId-1107797414"&gt;Training Model #1 - Random Forest&lt;/H3&gt;&lt;P&gt;Random Forest is often the first model used in tabular ML. It works by creating &lt;STRONG&gt;many decision trees&lt;/STRONG&gt; and averaging their predictions. Before training, categorical variables need to be &lt;STRONG&gt;label-encoded&lt;/STRONG&gt; into numbers, a common requirement for classical ML models:&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;def train_random_forest(X, y):
    X = X.copy()
    cat_cols = ["make", "model", "body", "transmission", "color", "interior"]
    le = LabelEncoder()

    for col in cat_cols:
        X[col] = le.fit_transform(X[col].astype(str).fillna("Unknown"))

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=default_test_size, random_state=42
    )

    model = RandomForestRegressor(
        n_estimators=150, max_depth=20, random_state=42, n_jobs=-1
    )

    try:
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        r2 = r2_score(y_test, preds)
    except Exception as e:
        preds, r2 = np.zeros_like(y_test), 0

    return [preds, r2, y_test]&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-911283909"&gt;Up to 50 rows:&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_3-1763206176248.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341502i82216AA724092E03/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_3-1763206176248.png" alt="nicolasestevan_3-1763206176248.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId-714770404"&gt;Up to 7067 rows:&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_8-1763206511155.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341538iF2A25E0C0EBE0612/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_8-1763206511155.png" alt="nicolasestevan_8-1763206511155.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId-518256899"&gt;Live view&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="RandomForest_20251115_092355.gif" style="width: 960px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341551i3A2C874AFAF47388/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="RandomForest_20251115_092355.gif" alt="RandomForest_20251115_092355.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;HR /&gt;&lt;H3 id="toc-hId-321743394"&gt;Training Model #2 - LightGBM&lt;/H3&gt;&lt;P&gt;LightGBM is one of the most powerful models for tabular data. Unlike Random Forest (many independent trees), LightGBM builds trees &lt;STRONG&gt;sequentially&lt;/STRONG&gt;, each correcting the errors of the previous one. It supports categorical features natively, which simplifies preprocessing.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;def train_lightgbm(X, y):
    X = X.copy()
    cat_cols = ["make", "model", "body", "transmission", "color", "interior"]
    for col in cat_cols:
        X[col] = X[col].astype(str).fillna("Unknown").astype("category")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=default_test_size, random_state=42
    )

    model = lgb.LGBMRegressor(
        n_estimators=500, learning_rate=0.05, num_leaves=31,
        subsample=0.8, colsample_bytree=0.8, random_state=42
    )

    try:
        model.fit(X_train, y_train, categorical_feature=cat_cols)
        preds = model.predict(X_test)
        r2 = r2_score(y_test, preds)
    except Exception:
        preds, r2 = np.zeros_like(y_test), 0

    return [preds, r2, y_test]&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId-125229889"&gt;Up to 50 rows:&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_2-1763205951324.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341474i1AAB214E2D01C2B2/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_2-1763205951324.png" alt="nicolasestevan_2-1763205951324.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId--146514985"&gt;Up to 7067 rows:&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_7-1763206474860.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341537i0ACD453B96C87ADF/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_7-1763206474860.png" alt="nicolasestevan_7-1763206474860.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId--343028490"&gt;Live view&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="LightGBM_20251115_092355.gif" style="width: 960px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341552i30BC4DE94C4988F6/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="LightGBM_20251115_092355.gif" alt="LightGBM_20251115_092355.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;HR /&gt;&lt;H3 id="toc-hId--539541995"&gt;Training Model #3 - Linear Regression&lt;/H3&gt;&lt;P&gt;Not fancy and even not complex, Linear Regression provides a baseline that shows:&amp;nbsp;&lt;SPAN&gt;â€œIf the relationship between attributes and price is roughly linear, how well can a simple model perform?â€&lt;/SPAN&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;def train_linear_model(X, y):
    X = X.copy()
    cat_cols = ["make", "model", "body", "transmission", "color", "interior"]
    for col in cat_cols:
        X[col] = LabelEncoder().fit_transform(X[col].astype(str).fillna("Unknown"))

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=default_test_size, random_state=42
    )

    model = LinearRegression()
    X_train = X_train.fillna(X_train.mean(numeric_only=True))
    X_test = X_test.fillna(X_test.mean(numeric_only=True))

    try:
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        r2 = r2_score(y_test, preds)
    except Exception:
        preds, r2 = np.zeros_like(y_test), 0

    return [preds, r2, y_test]&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId--736055500"&gt;&lt;STRONG&gt;Up to 50 rows:&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_1-1763205857765.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341472i81AFB2D0BE770F90/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_1-1763205857765.png" alt="nicolasestevan_1-1763205857765.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId--932569005"&gt;&lt;STRONG&gt;Up to 7067 rows:&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_6-1763206428099.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341536iC708165AEAE11D46/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_6-1763206428099.png" alt="nicolasestevan_6-1763206428099.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId--1129082510"&gt;Live view&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="LinearModel_20251115_092355.gif" style="width: 960px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341553i0849B4C842A417EE/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="LinearModel_20251115_092355.gif" alt="LinearModel_20251115_092355.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H2 id="toc-hId--1032193008"&gt;&lt;span class="lia-unicode-emoji" title=":chequered_flag:"&gt;ðŸ&lt;/span&gt;&amp;nbsp;&lt;SPAN&gt;SAP RPT-1 OSS: Context Model&lt;/SPAN&gt;&lt;/H2&gt;&lt;P&gt;This is where things get interesting. SAP RPT-1 does &lt;STRONG&gt;not&lt;/STRONG&gt; rely on learning patterns from the dataset. Instead, it uses a pretrained transformer architecture to infer relationships directly through &lt;STRONG&gt;context embeddings&lt;/STRONG&gt;. Lean and simple, "for non-Data Science PhD":&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;def train_sap_rpt1(X, y):
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=default_test_size, random_state=42
    )

    model = SAP_RPT_OSS_Regressor(max_context_size=8192, bagging=8)
    model.fit(X_train, y_train)

    preds = model.predict(X_test)
    r2 = r2_score(y_test, preds)

    return [preds, r2, y_test]&lt;/code&gt;&lt;/pre&gt;&lt;H3 id="toc-hId--1522109520"&gt;&lt;STRONG&gt;Up to 50 rows:&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_0-1763205729558.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341471i4AC7007DCA5A0F76/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_0-1763205729558.png" alt="nicolasestevan_0-1763205729558.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId--1718623025"&gt;&lt;STRONG&gt;Up to 2055 rows:&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="nicolasestevan_4-1763206228416.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341505i9ADE9D2D2B38C363/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="nicolasestevan_4-1763206228416.png" alt="nicolasestevan_4-1763206228416.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;H3 id="toc-hId--1915136530"&gt;Live view&lt;/H3&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="SAP_RPT1_20251115_092355.gif" style="width: 960px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/341566i0BE0E0D666836951/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="SAP_RPT1_20251115_092355.gif" alt="SAP_RPT1_20251115_092355.gif" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;HR /&gt;&lt;H2 id="toc-hId--1650063337"&gt;&lt;STRONG&gt;&lt;span class="lia-unicode-emoji" title=":magnifying_glass_tilted_right:"&gt;ðŸ”Ž&lt;/span&gt;&amp;nbsp;Running Experiments at Multiple Sample Sizes&lt;/STRONG&gt;&lt;/H2&gt;&lt;P&gt;This section breaks down how the iterative experiment loop works, why the SAP RPT-1 OSS model has a max-context limit, and how performance changes as we scale up the dataset. By running the same models across several sample sizes, we can see where traditional ML shines, where RPT-1 stays competitive, and how both behave as the data grows.&lt;/P&gt;&lt;pre class="lia-code-sample language-python"&gt;&lt;code&gt;sample_sizes = np.linspace(50, len(X), 200, dtype=int)
results, max_r2_rpt1, max_sample_rpt1 = [], 0, 0

for n in sample_sizes:
    idx = np.random.choice(len(X), n, replace=False)
    X_sample, y_sample = X.iloc[idx], y.iloc[idx]


    # SAP RPT-1 OSS (limited sample size)
    if n &amp;lt;= rpt1_limit:
        rpt_res = train_sap_rpt1(X_sample, y_sample)
        fn = plot_predictions(rpt_res[2], rpt_res[0], rpt_res[1], "SAP_RPT1", n)
        video_frames["SAP_RPT1"].append(fn)
        r2_rpt1 = rpt_res[1]
        max_r2_rpt1 = max(max_r2_rpt1, r2_rpt1)
    else:
        r2_rpt1 = max_r2_rpt1
        if max_sample_rpt1 == 0:
            max_sample_rpt1 = n

    # Train and plot models
    rf_res = train_random_forest(X_sample, y_sample)
    fn = plot_predictions(rf_res[2], rf_res[0], rf_res[1], "RandomForest", n)
    video_frames["RandomForest"].append(fn)

    lgb_res = train_lightgbm(X_sample, y_sample)
    fn = plot_predictions(lgb_res[2], lgb_res[0], lgb_res[1], "LightGBM", n)
    video_frames["LightGBM"].append(fn)

    lin_res = train_linear_model(X_sample, y_sample)
    fn = plot_predictions(lin_res[2], lin_res[0], lin_res[1], "LinearModel", n)
    video_frames["LinearModel"].append(fn)

    results.append((n, rf_res[1], r2_rpt1, lgb_res[1], lin_res[1]))

    # Early stop if traditional model reaches SAP RPT-1
    if rf_res[1] &amp;gt;= max_r2_rpt1 or lgb_res[1] &amp;gt;= max_r2_rpt1 or lin_res[1] &amp;gt;= max_r2_rpt1:
        break
    gc.collect()&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;This loop compares SAP RPT-1 OSS with traditional ML models as sample sizes increase. Each iteration randomly selects a subset of the data and trains all models on the same slice for a fair comparison. SAP RPT-1 can only run up to its max-context limit, so once the sample size exceeds that threshold, it stops retraining and simply carries forward its best RÂ². The traditional models continue training at every step. The loop ends early when any traditional model matches or surpasses RPT-1â€™s best score, making the experiment efficient while showing how performance evolves as data grows.&lt;/P&gt;&lt;HR /&gt;&lt;H2 id="toc-hId--1846576842"&gt;&lt;STRONG&gt;&lt;span class="lia-unicode-emoji" title=":end_arrow:"&gt;ðŸ”š&lt;/span&gt;&amp;nbsp;Conclusion and Final Thoughts&lt;/STRONG&gt;&lt;/H2&gt;&lt;P&gt;&amp;nbsp;SAP RPT-1 OSS stands out because it performs well with small datasets, requires minimal code, and can generate useful predictions with just an API call and a bit of context. This makes it ideal for jump-starting predictive use cases early on, delivering fast business value without a full ML pipeline. Traditional models, however, still shine when projects mature, data grows, and fine-tuned control becomes important. Itâ€™s not about choosing one over the other, but understanding where each approach brings the most value.&lt;/P&gt;&lt;TABLE border="1" width="100%"&gt;&lt;TBODY&gt;&lt;TR&gt;&lt;TD&gt;&lt;STRONG&gt;&amp;nbsp;&lt;/STRONG&gt;&lt;STRONG&gt;Aspect&amp;nbsp;&lt;/STRONG&gt;&lt;/TD&gt;&lt;TD&gt;&lt;STRONG&gt;SAP RPT-1 OSS&amp;nbsp;&lt;/STRONG&gt;&lt;/TD&gt;&lt;TD&gt;&lt;STRONG&gt;Traditional ML (RF, LGBM, Linear)&lt;/STRONG&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Data Requirements&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;Low (performs well with small samples)&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Medium/High (performance scales with data&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Setup Effort&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;Minimal (API call + context)&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Higher (preprocessing, encoding, tuning)&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Training Process&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;None (pretrained context model)&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Full training pipeline required&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Speed to Insights&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;Very fast&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Moderate to slow&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Best Use Case&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;Early-stage predictive cases, quick baselines&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Mature pipelines, high control and customization&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Flexibility&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;Limited tuning / plug-and-play&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Highly customizable&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="19.011815252416756%" height="30px"&gt;Business Value&lt;/TD&gt;&lt;TD width="38.66809881847476%" height="30px"&gt;Immediate, fast, accessible&lt;/TD&gt;&lt;TD width="42.21267454350161%" height="30px"&gt;Strong when optimized and scaled&lt;/TD&gt;&lt;/TR&gt;&lt;/TBODY&gt;&lt;/TABLE&gt;&lt;P&gt;This experiment highlights a simple truth: &lt;STRONG&gt;SAP RPT-1 isnâ€™t here to replace traditional ML, it jump-starts it.&amp;nbsp;&lt;/STRONG&gt;With a pretrained, context-driven approach, RPT-1 delivers fast, reliable insights with very little data and almost no setup. Traditional models still excel in mature, data-rich scenarios, but RPT-1 shines as a rapid accelerator and early-value generator inside SAP landscapes.&lt;/P&gt;&lt;HR /&gt;&lt;H3 id="toc-hId-1958473942"&gt;&lt;STRONG&gt;&lt;span class="lia-unicode-emoji" title=":speech_balloon:"&gt;ðŸ’¬&lt;/span&gt;Open for Exchange&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;If you're testing RPT-1, exploring predictive cases, or want the full code, feel free to reach out.&lt;BR /&gt;&lt;STRONG&gt;Happy to connect, compare experiences, and push this topic forward together.&lt;/STRONG&gt;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/sap-rpt-1-context-model-vs-training-classical-models-the-models-battle/ba-p/14268507"/>
    <published>2025-11-20T07:50:27.670000+01:00</published>
  </entry>
</feed>
