<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>https://raw.githubusercontent.com/ajmaradiaga/feeds/main/scmt/topics/Big-Data-blog-posts.xml</id>
  <title>SAP Community - Big Data</title>
  <updated>2025-10-11T11:10:20.804527+00:00</updated>
  <link href="https://community.sap.com/t5/c-khhcw49343/Big Data/pd-p/139269250608756787992873" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <subtitle>Big Data blog posts in SAP Community</subtitle>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/unified-analytics-with-sap-datasphere-databricks-lakehouse-platform-data/ba-p/13555907</id>
    <title>Unified Analytics with SAP Datasphere &amp; Databricks Lakehouse Platform- Data Federation Scenarios</title>
    <updated>2023-03-10T15:56:11+01:00</updated>
    <author>
      <name>Vivek-RR</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/143545</uri>
    </author>
    <content>&lt;H1 id="toc-hId-833060718"&gt;Introduction&lt;/H1&gt;&lt;BR /&gt;
With the announcements in the&lt;A href="https://news.sap.com/2023/03/sap-datasphere-power-of-business-data/" target="_blank" rel="noopener noreferrer"&gt; SAP Data Unleashed&lt;/A&gt;, SAP introduced the successor of SAP Data Warehouse Cloud, &lt;STRONG&gt;&lt;B&gt;SAP Datasphere&lt;/B&gt;&lt;/STRONG&gt; – a powerful Business Technology Platform data service that addresses the Data to value strategy of every Enterprise organization to deliver seamless access to business data. &amp;nbsp;Our Datasphere has been enriched with new features thereby delivering a unified service for data integration, cataloging, semantic modeling, data warehousing, and virtualizing workloads across SAP and non-SAP data.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic1-6.png" height="241" width="564" /&gt;&lt;/P&gt;&lt;BR /&gt;
I am sure there will be lots of blogs that will be published soon discussing the latest offerings, roadmaps, and features. However, this blog focuses on the latest announcement related to open data partners and I am going to start by focusing on Databricks.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic2-5.png" height="224" width="524" /&gt;&lt;/P&gt;&lt;BR /&gt;
With the rise of the Lakehouse platform that combines both Data Warehouses &amp;amp; Data Lakes, there has been a trend with SAP customers exploring Unified Analytics Platforms or say unified environments that address different perspectives of data management, governance, development, and finally deployments of analytic workloads based on diverse data sources and formats. Previously, there is a need to replicate the data completely out of SAP environments for customers to adopt the lakehouse platform. But the current partnership with Databricks will focus to simplify and integrate the hybrid landscapes efficiently.&lt;BR /&gt;
&lt;BR /&gt;
There was an article posted by &lt;A href="https://www.theregister.com/2023/03/08/sap_datasphere/" target="_blank" rel="nofollow noopener noreferrer"&gt;The Register&lt;/A&gt; regarding the SAP Datasphere&amp;nbsp; and it exactly resonated with the SAP messaging&lt;BR /&gt;
&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/PressRelease.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;BR /&gt;
&lt;STRONG&gt;&lt;B&gt;Please note that what I am about to discuss further is the data federation scenarios between SAP Datasphere and Databricks that works as of today&lt;/B&gt;&lt;/STRONG&gt;. &lt;STRONG&gt;&lt;B&gt;There will be additional ways of integrating with Databricks in the &lt;A href="https://news.sap.com/2023/03/sap-datasphere-business-data-fabric/" target="_blank" rel="noopener noreferrer"&gt;future&lt;/A&gt;.&lt;/B&gt;&lt;/STRONG&gt;&lt;BR /&gt;
&lt;H2 id="toc-hId-765629932"&gt;&lt;STRONG&gt;&lt;B&gt;Brief Introduction to the Lakehouse Platform&lt;/B&gt;&lt;/STRONG&gt;&lt;/H2&gt;&lt;BR /&gt;
In simple terms, a lakehouse is a Data Management architecture that enables users to perform diverse workloads such as BI, SQL Analytics, Data Science &amp;amp; Machine Learning on a unified platform. And by utilizing a combined data management platform such as lakehouse has the following benefits&lt;BR /&gt;
&lt;OL&gt;&lt;BR /&gt;
 	&lt;LI&gt;Enables Direct Data Access across SAP and Non-SAP sources&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Simplifies data governance&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Removes Data Redundancy&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Direct Connectivity to BI Tools&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Simplified security with a single source of truth&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Support for open source &amp;amp; commercial tooling&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Operating on multi-cloud environments and many more.&lt;/LI&gt;&lt;BR /&gt;
&lt;/OL&gt;&lt;BR /&gt;
And Databricks is one such Lakehouse platform that takes a unified approach by integrating disparate workloads to execute data engineering, Analytics, Data Science &amp;amp; Machine Learning use cases. And as mentioned on their &lt;A href="https://www.databricks.com/product/data-lakehouse" target="_blank" rel="nofollow noopener noreferrer"&gt;site&lt;/A&gt;, the platform is simple, open &amp;amp; multi-cloud. We will discuss the Lakehouse platform features and capabilities in future blogs but as mentioned before we are going to focus on a data federation scenario to access data from Databricks SQL into SAP Datasphere.&lt;BR /&gt;
&lt;BR /&gt;
&lt;STRONG&gt;Consider a scenario where the data from a non-SAP source is continuously ingested into cloud object storage say AWS S3. Note that Databricks has an &lt;A href="https://docs.databricks.com/ingestion/auto-loader/index.html" target="_blank" rel="nofollow noopener noreferrer"&gt;autoloader&lt;/A&gt; feature to efficiently process data files from different cloud storages as they arrive and ingest them into Lakehouse seamlessly. Then we utilize the &lt;A href="https://docs.databricks.com/workflows/delta-live-tables/index.html" target="_blank" rel="nofollow noopener noreferrer"&gt;delta live table&lt;/A&gt; framework for building data pipelines and storing the transformed data in&amp;nbsp;&lt;A href="https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdocs.databricks.com%2Fdelta%2Findex.html&amp;amp;data=05%7C01%7Cv.rr%40sap.com%7C2118d74bda7a4a6c5b4e08db21fbef44%7C42f7676cf455423c82f6dc2d99791af7%7C0%7C0%7C638141138200467635%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;amp;sdata=dgoglym6W6gc8yAilgaVRZdIAWGjTjNs0d3j4mXmi0w%3D&amp;amp;reserved=0" target="_blank" rel="nofollow noopener noreferrer"&gt;Delta format&lt;/A&gt; on&amp;nbsp;cloud storage, which can subsequently be accessed by Databricks SQL(DB SQL).&lt;/STRONG&gt;&lt;BR /&gt;
&lt;BR /&gt;
As referred to in the integration scenario below, SAP Datasphere will connect to Databricks SQL with the existing data federation capabilities and users can blend the data with SAP sources for reporting/BI workloads based on SAP Analytics Cloud(SAC).&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Dbricks_mar13.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;SPAN style="font-size: 1rem"&gt;Assuming you process the incoming data and persist as tables in Databricks SQL, you will then perform the following steps to establish connectivity to SAP Datasphere&lt;/SPAN&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;H2 id="toc-hId-569116427"&gt;&lt;STRONG&gt;Data Federation with Databricks SQL&lt;/STRONG&gt;&lt;/H2&gt;&lt;BR /&gt;
&lt;H3 id="toc-hId-501685641"&gt;&lt;SPAN style="font-size: 16.38px"&gt;Prerequisites&lt;/SPAN&gt;&lt;/H3&gt;&lt;BR /&gt;
&lt;OL&gt;&lt;BR /&gt;
 	&lt;LI&gt;You have access to SAP Datasphere with authorization to create a new Data provisioning agent&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Access to Virtual Machine or On-Premise system where you install Data provisioning agent.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;You have access to Databricks Clusters as well as SQL warehouse.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;You have downloaded the &lt;A href="https://www.databricks.com/spark/jdbc-drivers-download" target="_blank" rel="nofollow noopener noreferrer"&gt;JDBC driver&lt;/A&gt; from Databricks website.&lt;/LI&gt;&lt;BR /&gt;
&lt;/OL&gt;&lt;BR /&gt;
&lt;H2 id="toc-hId-176089417"&gt;&lt;STRONG&gt;Databricks Cluster &amp;amp; SQL Access&lt;/STRONG&gt;&lt;/H2&gt;&lt;BR /&gt;
As mentioned in the previous section, I assume you are already working on Databricks topics. If you are interested in getting access , you can &lt;A href="https://www.databricks.com/try-databricks#account" target="_blank" rel="nofollow noopener noreferrer"&gt;sign up&lt;/A&gt; for the free trial for 14 days. Or you can sign up from the hyperscaler marketplace such as AWS marketplace for the same.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic5-4.png" height="239" width="562" /&gt;&lt;/P&gt;&lt;BR /&gt;
Once you login to your account, you will notice the Unified environment for different workspaces&lt;BR /&gt;
&lt;BR /&gt;
Data Science and Engineering&lt;BR /&gt;
&lt;BR /&gt;
Machine Learning&lt;BR /&gt;
&lt;BR /&gt;
SQL&lt;BR /&gt;
&lt;BR /&gt;
Navigate to workspace “Data Science and Engineering” and select the compute which you have been using for data transformation. Just to note that when we build Delta Live tables pipelines, it uses its own compute to run pipelines. We will discuss that in the later blogs.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic6-7.png" height="217" width="570" /&gt;&lt;/P&gt;&lt;BR /&gt;
Select the all-purpose compute to get additional information.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic7-3.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
Navigate to the “Advanced options” to get JDBC connection details&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic8-3.png" height="302" width="544" /&gt;&lt;/P&gt;&lt;BR /&gt;
You will find the connection details here. We need to tweak it a bit prior to connecting with generic JDBC connectors.&amp;nbsp; Also, we will be connecting using personal access tokens and not user credentials.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic9-3.png" height="303" width="555" /&gt;&lt;/P&gt;&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;BR /&gt;
To generate the personal access token, you can generate it from the user settings and save the token for later use.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic10-2.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
Here is the modified JDBC URL that we will be using in SAP Datasphere connection. We need to add &lt;STRONG&gt;“IgnoreTransactions=1”&lt;/STRONG&gt; to ignore transaction related operations. In your case, you will be just copying the URL from advanced options and add the parameter IgnoreTransactions as shown below&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;jdbc:databricks://dbc-xxxxxxxxxxx.cloud.databricks.com:443/default;IgnoreTransactions=1;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/XXXXXXXXXXXXXXXXX268/XX-XX-56fird2a;AuthMech=3;UID=token;PWD=&amp;lt;Personal Access Token&amp;gt;&lt;BR /&gt;
&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
We can now navigate to Databricks SQL to see if we have the necessary schemas and tables to access. The tables were created under the schema “dltcheck”.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic11-1.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;H2 id="toc-hId--20424088"&gt;&lt;STRONG&gt;Data Provisioning Agent(DP Agent) Installation Instructions&lt;/STRONG&gt;&lt;/H2&gt;&lt;BR /&gt;
I am not going the discuss the DP agent installation in detail as there are numerous articles posted on the same topic. And will be addressing the specific changes that need to be done for Databricks access.&lt;BR /&gt;
&lt;BR /&gt;
In order to establish secure connectivity between SAP Datasphere and Databricks SQL, the Data Provisioning Agent(DP agent) has to be installed on a virtual machine and configured. For DP agent installation, you can refer to the following &lt;A href="https://help.sap.com/docs/SAP_DATASPHERE/9f804b8efa8043539289f42f372c4862/8f6185069a51404ebf23c684fee8cf39.html" target="_blank" rel="noopener noreferrer"&gt;document&lt;/A&gt;. To connect the DP agent to the SAP HANA Cloud tenant of SAP Datasphere, please follow the steps mentioned in this &lt;A href="https://help.sap.com/docs/SAP_DATASPHERE/9f804b8efa8043539289f42f372c4862/e87952d7c656477cb5558e5c2f44ae9c.html" target="_blank" rel="noopener noreferrer"&gt;document&lt;/A&gt;. Assuming all the steps were completed, the status of the DP agent will be displayed as “Connected”. In my case, the DP agent is DBRICKSNEW.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic12-1.png" height="354" width="536" /&gt;&lt;/P&gt;&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;H2 id="toc-hId--216937593"&gt;&lt;STRONG&gt;&lt;B&gt;Configuration Adjustments on the Virtual Machine&lt;/B&gt;&lt;/STRONG&gt;&lt;/H2&gt;&lt;BR /&gt;
Navigate to the folder &amp;lt;dpagent&amp;gt;/camel/lib and copy the jar file that is downloaded from &lt;A href="https://www.databricks.com/spark/jdbc-drivers-download" target="_blank" rel="nofollow noopener noreferrer"&gt;Databricks site&lt;/A&gt;. Also, extract the jar files in the same location.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic13-2.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
And navigate to &amp;lt;dpagent&amp;gt;/camel folder and adjust the properties of configfile-jdbc.properties&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic15.png" height="105" width="586" /&gt;&lt;/P&gt;&lt;BR /&gt;
Change the delimident value to BACKTICK.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic16.png" height="84" width="345" /&gt;&lt;/P&gt;&lt;BR /&gt;
Save and restart the DP agent. Login to SAP Datasphere to check the status of DP agent. It should display the status as "Connected". Edit the DPagent and select the Agent Adapter “CameJDBCAdapter”&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic17.png" height="279" width="251" /&gt;&lt;/P&gt;&lt;BR /&gt;
Now we should be able to connect to Databricks SQL from SAP Datasphere.&lt;BR /&gt;
&lt;H2 id="toc-hId--413451098"&gt;Establishing Connection from SAP Datasphere&lt;/H2&gt;&lt;BR /&gt;
Navigate to your space and create a new connection for the “Generic JDBC” connector&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic18.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
Provide any Business Name and the following details&lt;BR /&gt;
&lt;PRE class="language-abap"&gt;&lt;CODE&gt;Class : com.databricks.client.jdbc.Driver&lt;BR /&gt;
&lt;BR /&gt;
JDBC URL :  jdbc:databricks://&amp;lt;your databricksaccount&amp;gt;. cloud.databricks.com&amp;gt;:443/default;IgnoreTransactions=1;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/XXXXXXXXXXXXXXXXX268/XX-XX-XXXXXX;AuthMech=3;UID=token;PWD=&amp;lt;Personal Access Token&amp;gt; &lt;BR /&gt;
&lt;BR /&gt;
&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;STRONG&gt;Please copy the JDBC URL as such from Databricks Cluster advanced options&lt;/STRONG&gt; and just &lt;STRONG&gt;add&lt;/STRONG&gt; the parameter “&lt;STRONG&gt;IgnoreTransactions=1&lt;/STRONG&gt;”. Setting this property to 1 ignores any transaction-related operations and returns successfully.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic20.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
Provide your Databricks user account credentials or token credentials with user as token and select the Data provisioning agent that you just activated.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic21.png" height="302" width="503" /&gt;&lt;/P&gt;&lt;BR /&gt;
With the connection details and configurations done properly, validation should be successful.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic22.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;BR /&gt;
To make sure we have access to data, let’s use the Data Builder to build an analytical model that could be consumed in SAP Analytics Cloud. Navigate to Data Builder and create a new Graphical View&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic23.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
Navigate to Sources-&amp;gt;Connections-&amp;gt;Databricks("Your Business Name For Generic JDBC Connection")-&amp;gt;"Databricks Schema". You will find the list of tables under the schema "dltcheck"&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic24.png" height="326" width="563" /&gt;&lt;/P&gt;&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;BR /&gt;
Here is the Databricks SQL Access for the same schema "dltcheck"&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic25.png" height="209" width="558" /&gt;&lt;/P&gt;&lt;BR /&gt;
Select the data with which you wanted to build the Analytical model. In my case it is bucketraw1,&amp;nbsp; added some calculated columns, aggregated the necessary data, and exposed the relevant columns as the Analytical model “Confirmed_cases”. And the data preview of the model shows the corresponding records too.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic26.png" height="257" width="507" /&gt;&lt;/P&gt;&lt;BR /&gt;
This model could be consumed in SAP Analytics Cloud for reporting.&lt;BR /&gt;
&lt;H2 id="toc-hId--609964603"&gt;Troubleshooting Errors&lt;/H2&gt;&lt;BR /&gt;
1. If you do not see the connection validated, then there are two options to troubleshoot. Either we can use the generic log files from DP agent to identify the issue or add the log parameters in JDBC URL and collect those specific errors. If you face validation issues, then add these parameters in the jdbc url.&lt;BR /&gt;
&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;PRE class="language-abap"&gt;&lt;CODE&gt;jdbc:databricks://&amp;lt;youraccount&amp;gt;.cloud.databricks.com:443/default;LogLevel=5;LogPath= &amp;lt;foldername&amp;gt;;IgnoreTransactions=1;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/&amp;lt;SQLPath&amp;gt;;AuthMech=3;UID=token;PWD=&amp;lt;token&amp;gt;&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
Log Level – 5&amp;nbsp;&amp;nbsp; which means enable logging on the DEBUG level, which logs detailed information that is&amp;nbsp; useful for debugging the connector&lt;BR /&gt;
&lt;BR /&gt;
Logpath – This will be available in the following path /usr/sap/&amp;lt;dpagentfolder&amp;gt;/&lt;BR /&gt;
&lt;BR /&gt;
The log path can be found on the linux VM and it will generate the specific log files&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic27.png" height="414" width="493" /&gt;&lt;/P&gt;&lt;BR /&gt;
2. If you see some errors related to mismatched input, then please adjust the camel jdbc properties as mentioned in DPAgent Installation Instructions. The “DELIMIDENT” value should be set&amp;nbsp; to “BACKTICK”.&lt;BR /&gt;
&lt;PRE class="language-abap"&gt;&lt;CODE&gt;2022-12-19 16:31:57,552 [ERROR] [f6053d30-7fcd-436f-bed8-bf4d1358847788523] DefaultErrorHandler | CamelLogger.log [] - Failed delivery for (MessageId: 2567788B8CF359D-0000000000000000 on ExchangeId: 2567788B8CF359D-0000000000000000). Exhausted after delivery attempt: 1 caught: java.sql.SQLException: [Databricks][DatabricksJDBCDriver](500051) ERROR processing query/statement. Error Code: 0, SQL state: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: mismatched input '"FUNCTION_ALIAS_PREFIX_0"' expecting {&amp;lt;EOF&amp;gt;, ';'}(line 1, pos 33)&lt;BR /&gt;
&lt;BR /&gt;
                                == SQL ==&lt;BR /&gt;
&lt;BR /&gt;
                                SELECT SUM("confirmed_cases") AS "FUNCTION_ALIAS_PREFIX_0", SUM("deaths") AS "FUNCTION_ALIAS_PREFIX_1", "covidsummarygcs$VT"."county_name" FROM "default"."covidsummarygcs" "covidsummarygcs$VT" GROUP BY "covidsummarygcs$VT"."county_name"&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;BR /&gt;
3. If you see the communication link failure, then the IgnoreTransactions parameter has not been set in your JDBC URL.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/Pic28.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
As mentioned before, the data federation is enabled through JDBC connector as of now . But things will change with additional connections in the future. Hopefully this helped you in understanding the Data Federation capabilities with Databricks. Please do share your feedback. In case of connection issues, feel free to comment, and will try to help you out.&lt;BR /&gt;
&lt;BR /&gt;
&amp;nbsp;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/unified-analytics-with-sap-datasphere-databricks-lakehouse-platform-data/ba-p/13555907"/>
    <published>2023-03-10T15:56:11+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/model-compression-without-compromising-predictive-accuracy-in-sap-hana-pal/ba-p/13564339</id>
    <title>Model Compression without Compromising Predictive Accuracy in SAP HANA PAL</title>
    <updated>2023-03-30T11:29:26+02:00</updated>
    <author>
      <name>xinchen</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/712820</uri>
    </author>
    <content>&lt;H1 id="toc-hId-833948777"&gt;1. Introduction&lt;/H1&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;The recent success of applying state-of-the-art AI algorithms on tasks with modern big data has raised concerns on their efficiency. For instance, ensemble methods like random forest typically require numerous sub-learners to achieve favorable predictive performance, which result in an increasing demand for model storage space &lt;SPAN&gt;as the dataset size increases.&lt;/SPAN&gt;&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&lt;SPAN&gt;Such tree-based models may reach GB level, making them difficult to deploy on resource-constrained devices or proving costly in the cloud. Furthermore, a larger model usually leads to higher inference time and energy consumption, which are unacceptable in many real-world applications.&lt;/SPAN&gt;&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&lt;SPAN&gt;To address this issue, several efficient methods have been proposed. These approaches aim to make machine learning model inference faster (time-efficient), use fewer computational resources (computationally efficient), require less memory (memory-efficient), and take less disk space (storage-efficient). Among these methods, model compression is one of the most popular. It reduces the size of large models without compromising predictive accuracy.&lt;/SPAN&gt;&lt;BR /&gt;&lt;BR /&gt;In SAP HANA &lt;STRONG&gt;Predictive Analysis Library&lt;/STRONG&gt; (PAL), we have also introduced lossy model compression techniques in several popular methods including Support Vector Machine (&lt;STRONG&gt;SVM&lt;/STRONG&gt;), Random Decision Trees (&lt;STRONG&gt;RDT&lt;/STRONG&gt;) and Hybrid Gradient Boosting Tree (&lt;STRONG&gt;HGBT&lt;/STRONG&gt;). &lt;SPAN&gt;These techniques minimize the loss of predictive accuracy and do not cause any delay in inference.&amp;nbsp;&lt;/SPAN&gt;In the following sections, we will dive deep into the model compression methods applied in SAP HANA PAL. At the end of the blog post, some useful terms and links are listed.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="toc-hId-637435272"&gt;2. Algorithms&lt;/H1&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&lt;STRONG&gt;Tree-based Algorithms – RDT/ HGBT&lt;/STRONG&gt;&lt;BR /&gt;Regards to these tree-based algorithm whose trees are large and independent and identically distributed random entities that contain many additional characteristics and parameters given the training data. &lt;SPAN&gt;Therefore, the ensemble model is abundant in redundancy which allows us to infer their probability structure and construct an efficient encoding scheme.&lt;/SPAN&gt;&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;The compression methodology we applied in RDT/ HGBT focus on the following attributes:&lt;/P&gt;&lt;UL class="lia-align-justify" style="text-align : justify;"&gt;&lt;LI&gt;&lt;STRONG&gt;Tree structure&lt;/STRONG&gt;: Each of the trees’ structure is represent with a Zaks sequence and then we apply a simple LZ-based encoder after all sequences are concatenated.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;The split of the nodes&lt;/STRONG&gt;&amp;nbsp;(Variable Name, Split Value): Usually, each node is defined by a variable name and a corresponding selected split value. As a result of the recursive construction of the tree, the method assumes that the probabilistic model for a node only depends on its depth and parents. Then, based on probabilistic modeling, models (Variable Name Models and Split Value Models) are clustered via Bregman divergence. Then, the data which corresponds to a model could be compressed by Huffman code according to center probability.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;The values of the leaves&lt;/STRONG&gt;: Also, the method uses a simplified model in which the distribution of the fits in a leaf relies on its depth and parent’s variable name. In the case of classification problems, the usage of entropy coding is suitable as the fits are categorical. However, in the regression problems, quantization techniques like Lloyd-max are required to take over continuous set of values. &amp;nbsp;Such quantization results generally lead to very regularized distortion which could be handled well by setting the distortion level to achieve favorable compression rate.&lt;/LI&gt;&lt;/UL&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;Moreover, this lossless compression scheme allows make predictions from the compressed model. The results of compression rate of RDT and HGBT with various number of trees are shown in the following two figures. The dataset (768 rows, 9 columns) used in the example is from Kaggle and the original data comes from National Institute of Diabetes and Digestive and Kidney Diseases.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&lt;IMG src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/RDT-rate.png" border="0" /&gt;&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&lt;IMG src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/03/hgbt_rate.png" border="0" /&gt;&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&lt;STRONG&gt;SVM&lt;/STRONG&gt;&lt;BR /&gt;&lt;BR /&gt;The aim of SVM is to find a set of support vectors to distinguish the classes and maximize the margin. Sometimes, the number of support vectors is very huge. In particular, categorical data need to be mapped to be continuous through one-hot encoding technique, which required more space for model storage in JSON/PMML format.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;Hence, we applied Lloyd-Max quantization and Huffman coding to compress support vectors into a string in SVM. The scheme also enables predictions from the compressed format. In addition, in one-hot encoding, each categorical value is represented only by a value which can significantly reduce the use of memory.&lt;BR /&gt;&amp;nbsp;&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="toc-hId-440921767"&gt;3. Terms&lt;/H1&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&lt;STRONG&gt;Compression Rate&lt;/STRONG&gt;&lt;BR /&gt;Compression Rate = Compressed Size / Uncompressed Size&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;Quantization&lt;/STRONG&gt;&lt;BR /&gt;Quantization is the process of mapping continuous infinite values to a smaller set of discrete finite values. One popular quantizer is Lloyd-Max algorithm which designs non-uniform quantizers optimized according to the prevailing pdf of the input data.&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;Entropy Coding&lt;/STRONG&gt;&lt;BR /&gt;Entropy is the smallest number of bits needed to represent a symbol.&amp;nbsp; Hence, entropy coding techniques are lossless coding methods which could approach such entropy limit. One common form is Huffman coding which uses a discrete number of bits for each symbol. Another is arithmetic coding, which outputs a bit sequence representing a point inside an interval. The interval is built recursively by the probabilities of the encoded symbols.&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;Tree-based Model Compression&lt;/STRONG&gt;&lt;BR /&gt;There are many research directions of model compression of tree ensembles. One popular line focus on pruning techniques such as removing redundant components (features and trees), selecting optimal rule subsets, and choosing an optimal subset of trees. However, such pruning schemes are lossy and no guarantees on the compression rate. Another line is to train an artificial neural network to mimic the functionality of tree ensembles which is faster but both lossy and irreversible.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="toc-hId-244408262"&gt;4. Summary&lt;/H1&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;In this blog post, we introduce the model compression methodology used in RDT, HGBT and SVM in SAP HANA PAL which could offer lossless model compression without compromising predictive performance. Hope you enjoyed reading this blog post!&lt;/P&gt;&lt;H3 id="toc-hId-306060195"&gt;&amp;nbsp;&lt;/H3&gt;&lt;H3 id="toc-hId-109546690"&gt;Other Useful Links:&lt;/H3&gt;&lt;OL&gt;&lt;LI&gt;&lt;A href="https://pypi.org/project/hana-ml/" target="_blank" rel="noopener nofollow noreferrer"&gt;hana-ml&lt;/A&gt; on Pypi.&lt;/LI&gt;&lt;LI&gt;Python Machine Learning Client for SAP HANA (hana-ml)&amp;nbsp;&lt;A href="https://help.sap.com/doc/cd94b08fe2e041c2ba778374572ddba9/2024_1_QRC/en-US/hana_ml.html#" target="_self" rel="noopener noreferrer"&gt;Documentation&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;R Machine Learning Client for SAP HANA (hana.ml.r) &lt;A href="https://help.sap.com/doc/b64d3cac2f0b42be9ca9fc662715f36b/2024_1_QRC/en-US/index.html" target="_self" rel="noopener noreferrer"&gt;Documentation&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;SAP HANA Predictive Analysis Library (PAL)&amp;nbsp;&lt;A href="https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/sap-hana-cloud-sap-hana-database-predictive-analysis-library-pal-sap-hana-cloud-sap-hana-database-predictive-analysis-library-pal-c9eeed7" target="_self" rel="noopener noreferrer"&gt;Documentation&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;Other blog posts on HANA Machine Learning:&lt;/LI&gt;&lt;/OL&gt;&lt;UL class="lia-list-style-type-circle"&gt;&lt;LI&gt;&lt;A href="https://community.sap.com/t5/technology-blogs-by-sap/fairness-in-machine-learning-a-new-feature-in-sap-hana-cloud-pal/ba-p/13580185" target="_self"&gt;Fairness in Machine Learning - A New Feature in SAP HANA Cloud PAL&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://blogs.sap.com/2020/12/18/identification-of-seasonality-in-time-series-with-python-machine-learning-client-for-sap-hana/" target="_blank" rel="noopener noreferrer"&gt;Identification of Seasonality in Time Series &lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://blogs.sap.com/2020/12/11/outlier-detection-using-statistical-tests-in-python-machine-learning-client-for-sap-hana/" target="_blank" rel="noopener noreferrer"&gt;Outlier Detection using Statistical Tests&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://blogs.sap.com/2020/12/16/outlier-detection-by-clustering/" target="_blank" rel="noopener noreferrer"&gt;Outlier Detection by Clustering&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://blogs.sap.com/2020/12/21/anomaly-detection-in-time-series-using-seasonal-decomposition-in-python-machine-learning-client-for-sap-hana/" target="_blank" rel="noopener noreferrer"&gt;Anomaly Detection in Time-Series using Seasonal Decomposition&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://blogs.sap.com/2020/12/29/outlier-detection-with-one-class-classification-using-python-machine-learning-client-for-sap-hana/" target="_blank" rel="noopener noreferrer"&gt;Outlier Detection with One-class Classification&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://blogs.sap.com/2020/12/31/learning-from-labeled-anomalies-for-efficient-anomaly-detection-using-python-machine-learning-client-for-sap-hana/" target="_blank" rel="noopener noreferrer"&gt;Learning from Labeled Anomalies for Efficient Anomaly Detection&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://blogs.sap.com/2020/12/17/import-multiple-excel-files-into-a-single-sap-hana-table/" target="_blank" rel="noopener noreferrer"&gt;Import multiple excel files into a single SAP HANA table&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://blogs.sap.com/2020/12/16/copd-study-explanation-and-interpretability-with-python-machine-learning-client-for-sap-hana/" target="_blank" rel="noopener noreferrer"&gt;COPD study, explanation and interpretability&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://blogs.sap.com/wp-admin/post.php?post=1262813&amp;amp;action=edit" target="_blank" rel="noopener noreferrer"&gt;Model Storage with Python Machine Learning Client for SAP HANA&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://blogs.sap.com/2023/03/29/ways-to-accelerate-the-training-process-of-gbdt-models-in-hgbt/?source=email-global-notification-bp-new-in-tag-followed" target="_blank" rel="noopener noreferrer"&gt;Ways to Accelerate the Training Process of GBDT Models in HGBT&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;&lt;A href="https://community.sap.com/t5/technology-blogs-by-sap/global-explanation-capabilities-in-sap-hana-machine-learning/ba-p/13620594" target="_self"&gt;Global Explanation Capabilities in SAP HANA Machine Learning&lt;/A&gt;&amp;nbsp;&amp;nbsp;&lt;/LI&gt;&lt;/UL&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/model-compression-without-compromising-predictive-accuracy-in-sap-hana-pal/ba-p/13564339"/>
    <published>2023-03-30T11:29:26+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/automatic-outlier-detection-for-time-series-in-sap-hana/ba-p/13559856</id>
    <title>Automatic Outlier Detection for Time Series in SAP HANA</title>
    <updated>2023-06-29T11:06:04+02:00</updated>
    <author>
      <name>yangzhi</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/861208</uri>
    </author>
    <content>In time series, an outlier is a data point that is different from the general behavior of remaining data points. In Predictive Analysis Library (PAL) of SAP HANA, we have automatic outlier detection for time series. You can find more details in &lt;A href="https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/outlier-detection?version=2023_2_QRC" target="_blank" rel="noopener noreferrer"&gt;Outlier Detection for Time Series in PAL&lt;/A&gt;.&lt;BR /&gt;
&lt;BR /&gt;
In PAL, the outlier detection procedure is divided into two steps. In step 1, we get the residual from the original series. In step 2, we detect the outliers from the residual. &lt;STRONG&gt;In step 1, we have an automatic method.&lt;/STRONG&gt;&lt;BR /&gt;
&lt;BR /&gt;
In this blog post, you will learn:&lt;BR /&gt;
&lt;UL&gt;&lt;BR /&gt;
 	&lt;LI&gt;introduction of outlier in time series and automatic detection method in PAL&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;some use cases of automatic outlier detection&lt;/LI&gt;&lt;BR /&gt;
&lt;/UL&gt;&lt;BR /&gt;
To make it easy to test the performance of automatic outlier detection, we use Z1 score method (the default method) in step 2 and set the parameter THRESHOLD = 3 (the default value). This parameter value is based on 3 sigma rule.&lt;BR /&gt;
&lt;BR /&gt;
To make it easy to read and show the results, we call the PAL procedure in Jupyter Notebook. For calling the PAL procedure and plotting the results, we need some functions. We put the functions in the Appendix.&lt;BR /&gt;
&lt;H1 id="toc-hId-833179075"&gt;Introduction&lt;/H1&gt;&lt;BR /&gt;
In time series, outliers can have many causes, such as data entry error, experimental error, sampling error and natural outlier. Outliers have a huge impact on the result of data analysis, such as the seasonality test. Outlier detection is an important data preprocessing for time series analysis.&lt;BR /&gt;
&lt;BR /&gt;
In this algorithm, the outlier detection procedure is divided into two steps. In step 1, we get the residual from the original series. In step 2, we detect the outliers from the residual. We focus on the automatic method of step 1 in this blog.&lt;BR /&gt;
&lt;BR /&gt;
In step 1, we have an automatic method. For the automatic method, we combine seasonal decomposition, linear regression, median filter and super smoother. The processes in the automatic method is shown in the picture below.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/auto_procedure.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;P class="image_caption" style="text-align: center;font-style: italic"&gt;Processes of Automatic Method&lt;/P&gt;&lt;BR /&gt;
In the output of this algorithm, we have a result table and a statistic table. In the result table, the residual, outlier score and outlier label are included. In the statistic table, some information of the time series and outlier detection method is included. For automatic method, the final smoothing method in step 1 is shown in the statistic table.&lt;BR /&gt;
&lt;H1 id="toc-hId-636665570"&gt;Test Cases&lt;/H1&gt;&lt;BR /&gt;
To call the PAL procedure with python, we need to import some python packages.&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;import numpy as np&lt;BR /&gt;
from datetime import datetime&lt;BR /&gt;
from pandas import read_table&lt;BR /&gt;
import matplotlib.pyplot as plt&lt;BR /&gt;
import pandas as pd&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;H2 id="toc-hId-569234784"&gt;case 1: smooth data without seasonality&lt;/H2&gt;&lt;BR /&gt;
&lt;H3 id="smooth-data-without-seasonality" id="toc-hId-501803998"&gt;data&lt;/H3&gt;&lt;BR /&gt;
The data is from &lt;A href="https://github.com/ocefpaf/python4oceanographers/blob/master/content/downloads/notebooks/data/spikey_v.dat" target="_blank" rel="nofollow noopener noreferrer"&gt;spikey_v.dat&lt;/A&gt;.&amp;nbsp; It is smooth, but without seasonality. The plot of the data is shown with the code below.&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;str_path = 'your data path'&lt;BR /&gt;
cols = ['j', 'u', 'v', 'temp', 'sal', 'y', 'mn', 'd', 'h', 'mi']&lt;BR /&gt;
&lt;BR /&gt;
df = read_table(str_path+'spikey_v.dat' , delim_whitespace=True, names=cols)&lt;BR /&gt;
&lt;BR /&gt;
df.index = [datetime(*x) for x in zip(df['y'], df['mn'], df['d'], df['h'], df['mi'])]&lt;BR /&gt;
df = df.drop(['y', 'mn', 'd', 'h', 'mi'], axis=1)&lt;BR /&gt;
data = np.full(len(df), np.nan)&lt;BR /&gt;
for i in range(len(df)):&lt;BR /&gt;
    data[i] = df['u'][i]&lt;BR /&gt;
plt.plot(data)&lt;BR /&gt;
plt.grid()&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/spikey_v_data-1.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;H3 id="automatic-outlier-detection" id="toc-hId-305290493"&gt;automatic outlier detection&lt;/H3&gt;&lt;BR /&gt;
We call the procedure _SYS_AFL.PAL_OUTLIER_DETECTION_FOR_TIME_SERIES to detect outliers automatically. The python code and results are as follows.&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;Outlier_parameters = {&lt;BR /&gt;
    "AUTO": 1,&lt;BR /&gt;
}&lt;BR /&gt;
df = pd.concat([pd.DataFrame({"ID":list(range(len(data)))}),pd.DataFrame(data)],axis=1)&lt;BR /&gt;
dfOutlierResults, dfStats, dfMetri = OutlierDetectionForTimeSeries(df,Outlier_parameters,cc)&lt;BR /&gt;
dfOutlierResults&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/spikey_v_output_table.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;dfStats&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/spikey_v_statistic_table-1.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
The results are plotted as below.&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;outlier_result_plot(dfOutlierResults)&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/spikey_v_output_plots-1.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;outlier_plot(dfOutlierResults)&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/spikey_v_outlier-1.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;From the statistic table, we can see that the final smoothing method is median filter, as the time series is quite smooth. From the above results, we find that PAL miss detecting an outlier. This is because there are two big outliers here and the standard deviation becomes large. From the plots of residual and outlier score, we can find four outliers very clearly. We can adjust the threshold or choose other methods in step 2 to find all outliers.&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;H2 id="toc-hId--20305731"&gt;case 2: non-smooth data without seasonality&lt;/H2&gt;&lt;BR /&gt;
&lt;H3 id="toc-hId--87736517"&gt;data&lt;/H3&gt;&lt;BR /&gt;
The data is from R package "fpp2". It is the gold daily price data from January 1st 1985 to March 31th 1989. The data is neither smooth nor seasonal. There are some missing values. The missing values are imputed by linear interpolation. The plot of the data is shown with the code below.&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;str_path = 'your data path'&lt;BR /&gt;
df = pd.read_csv(str_path+'daily_csv_no_missing_value.csv')&lt;BR /&gt;
num = len(df)&lt;BR /&gt;
data = np.full(num,np.nan)&lt;BR /&gt;
for i in range(num):&lt;BR /&gt;
    data[i] = df['Price'][i]&lt;BR /&gt;
plt.plot(data)&lt;BR /&gt;
plt.grid()&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/gold_price_data.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;H3 id="automatic-outlier-detection" id="toc-hId--284250022"&gt;automatic outlier detection&lt;/H3&gt;&lt;BR /&gt;
We call the procedure _SYS_AFL.PAL_OUTLIER_DETECTION_FOR_TIME_SERIES to detect outliers automatically. The python code and results are as follows.&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;Outlier_parameters = {&lt;BR /&gt;
    "AUTO": 1,&lt;BR /&gt;
}&lt;BR /&gt;
df = pd.concat([pd.DataFrame({"ID":list(range(len(data)))}),pd.DataFrame(data)],axis=1)&lt;BR /&gt;
dfOutlierResults, dfStats, dfMetri = OutlierDetectionForTimeSeries(df,Outlier_parameters,cc)&lt;BR /&gt;
dfOutlierResults&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/gold_price_output_table.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;dfStats&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/gold_price_statistic_table.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
The results are plotted as below.&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;outlier_result_plot(dfOutlierResults)&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/gold_price_output_plots.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;outlier_plot(dfOutlierResults)&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/gold_price_outlier.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
From the statistic table, we can see that the final smoothing method is super smoother, as the time series is not so smooth. From the above results, we find that PAL detect the outlier at t = 769 successfully and also consider some other points as outliers. From the plots of residual and outlier score, we can find that the outlier at t = 769 is very obvious. We can adjust the threshold or choose other methods in step 2 to only detect the outlier at t = 769.&lt;BR /&gt;
&lt;H2 id="toc-hId--609846246"&gt;case 3: smooth data with seasonality&lt;/H2&gt;&lt;BR /&gt;
&lt;H3 id="toc-hId--677277032"&gt;data&lt;/H3&gt;&lt;BR /&gt;
The data is synthetic. It is seasonal with period = 40. There are four obvious outliers in the time series. The time series is as below.&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;import math&lt;BR /&gt;
random_seed = 3&lt;BR /&gt;
np.random.seed(random_seed)&lt;BR /&gt;
cols = 200 # length of time series&lt;BR /&gt;
cycle = 40&lt;BR /&gt;
outlier_idx = [30, 45, 73, 126, 159, 173]&lt;BR /&gt;
timestamp = np.full(cols,np.nan,dtype = int)&lt;BR /&gt;
for i in range(cols):&lt;BR /&gt;
    timestamp[i] = i&lt;BR /&gt;
seasonal = np.full(cols,np.nan,dtype = float)&lt;BR /&gt;
for i in range(cols):&lt;BR /&gt;
    seasonal[i] = math.sin(2*math.pi/cycle*timestamp[i])&lt;BR /&gt;
const = np.full(cols,2,dtype = float)&lt;BR /&gt;
noise = np.full(cols,0,dtype = float)&lt;BR /&gt;
for i in range(cols):&lt;BR /&gt;
    noise[i] = 0.2*(np.random.rand()-0.5)&lt;BR /&gt;
trend = 0.01*timestamp&lt;BR /&gt;
outlier = np.full(cols,0,dtype = float)&lt;BR /&gt;
for i in outlier_idx:&lt;BR /&gt;
    outlier[i] = 4*(np.random.rand()-0.5)&lt;BR /&gt;
data = seasonal + const + noise + trend + outlier&lt;BR /&gt;
plt.plot(data)&lt;BR /&gt;
plt.grid()&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/seasonal2_data.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;H3 id="automatic-outlier-detection" id="toc-hId--949021906"&gt;automatic outlier detection&lt;/H3&gt;&lt;BR /&gt;
We call the procedure _SYS_AFL.PAL_OUTLIER_DETECTION_FOR_TIME_SERIES to detect outliers automatically. The python code and results are as follows.&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;Outlier_parameters = {&lt;BR /&gt;
    "AUTO": 1,&lt;BR /&gt;
}&lt;BR /&gt;
df = pd.concat([pd.DataFrame({"ID":list(range(len(data)))}),pd.DataFrame(data)],axis=1)&lt;BR /&gt;
dfOutlierResults, dfStats, dfMetri = OutlierDetectionForTimeSeries(df,Outlier_parameters,cc)&lt;BR /&gt;
dfOutlierResults&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/seasonal2_output_table.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;dfStats&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/seasonal2_statistic_table.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
The results are plotted as below.&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;outlier_result_plot(dfOutlierResults)&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/seasonal2_output_plots.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;outlier_plot(dfOutlierResults)&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/seasonal2_outlier.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
From the statistic table, we can see that the final smoothing method is median filter and seasonal decomposition, followed by super smoother, as the time series is quite smooth and seasonal. From the above results, we can see that the four obvious outliers are detected by PAL.&lt;BR /&gt;
&lt;H2 id="toc-hId--852132404"&gt;case 4: non-smooth data with seasonality&lt;/H2&gt;&lt;BR /&gt;
&lt;H3 id="toc-hId--1342048916"&gt;data&lt;/H3&gt;&lt;BR /&gt;
The data is monthly ice cream data. The period is 12. You can find the data in &lt;A href="https://github.com/ritvikmath/Time-Series-Analysis/blob/master/ice_cream_interest.csv" target="_blank" rel="nofollow noopener noreferrer"&gt;ice_cream_interest.csv&lt;/A&gt;. There are two obvious outliers in the time series. The plot of the time series is as below.&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;str_path = 'your data path'&lt;BR /&gt;
df = pd.read_csv(str_path+'ice_cream_interest.csv')&lt;BR /&gt;
data = np.full(len(df),np.nan,dtype = float)&lt;BR /&gt;
for i in range(len(df)):&lt;BR /&gt;
    data[i] = df['interest'][i]&lt;BR /&gt;
plt.plot(data)&lt;BR /&gt;
plt.grid()&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/ice_cream_data.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;H3 id="automatic-outlier-detection" id="toc-hId--1538562421"&gt;automatic outlier detection&lt;/H3&gt;&lt;BR /&gt;
We call the procedure _SYS_AFL.PAL_OUTLIER_DETECTION_FOR_TIME_SERIES to detect outliers automatically. The python code and results are as follows.&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;Outlier_parameters = {&lt;BR /&gt;
    "AUTO": 1,&lt;BR /&gt;
}&lt;BR /&gt;
df = pd.concat([pd.DataFrame({"ID":list(range(len(data)))}),pd.DataFrame(data)],axis=1)&lt;BR /&gt;
dfOutlierResults, dfStats, dfMetri = OutlierDetectionForTimeSeries(df,Outlier_parameters,cc)&lt;BR /&gt;
dfOutlierResults&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/ice_cream_output_table.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;dfStats&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/ice_cream_statistic_table.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
The results are plotted as below.&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;outlier_result_plot(dfOutlierResults)&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/ice_cream_output_plots.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;outlier_plot(dfOutlierResults)&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/06/ice_cream_outlier.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
From the statistic table, we can see that the final smoothing method is seasonal decomposition, followed by super smoother, as the time series is seasonal, but not so smooth. From the above results, we can see that the two obvious outliers are detected by PAL.&lt;BR /&gt;
&lt;H1 id="toc-hId--1148269912"&gt;Conclusions&lt;/H1&gt;&lt;BR /&gt;
In this blog post, we describe what is outlier in time series and provide an automatic outlier detection method for time series in PAL. We also provide some examples to show how to call the automatic outlier detection procedure and show the detection results. From the above results, we can see that the automatic method can detect outliers in different time series. Hope you enjoy reading this blog!&lt;BR /&gt;
&lt;BR /&gt;
The method will also be included in hana-ml. If you want to learn more about the automatic outlier detection method for time series in SAP HANA Predictive Analysis Library (PAL) and hana-ml, please refer to the following links:&lt;BR /&gt;
&lt;BR /&gt;
&lt;A href="https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/outlier-detection?version=2023_2_QRC" target="_blank" rel="noopener noreferrer"&gt;Outlier Detection for Time Series in PAL&lt;/A&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;A href="https://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.07/en-US/pal/algorithms/hana_ml.algorithms.pal.tsa.outlier_detection.OutlierDetectionTS.html#hana_ml.algorithms.pal.tsa.outlier_detection.OutlierDetectionTS" target="_blank" rel="noopener noreferrer"&gt;Outlier Detection for Time Series in hana-ml (automatic method will be included after 2023 Q3)&lt;/A&gt;&lt;BR /&gt;
&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;H3 id="toc-hId--1931589431"&gt;Other Useful Links:&lt;/H3&gt;&lt;BR /&gt;
&lt;A href="https://blogs.sap.com/2020/12/11/outlier-detection-using-statistical-tests-in-python-machine-learning-client-for-sap-hana/" target="_blank" rel="noopener noreferrer"&gt;Outlier Detection using Statistical Tests in Python Machine Learning Client for SAP HANA&lt;/A&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;A href="https://blogs.sap.com/2020/12/16/outlier-detection-by-clustering/" target="_blank" rel="noopener noreferrer"&gt;Outlier Detection by Clustering using Python Machine Learning Client for SAP HANA&lt;/A&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;A href="https://blogs.sap.com/2020/12/21/anomaly-detection-in-time-series-using-seasonal-decomposition-in-python-machine-learning-client-for-sap-hana/" target="_blank" rel="noopener noreferrer"&gt;Anomaly Detection in Time-Series using Seasonal Decomposition in Python Machine Learning Client for SAP HANA&lt;/A&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;A href="https://blogs.sap.com/2020/12/29/outlier-detection-with-one-class-classification-using-python-machine-learning-client-for-sap-hana/" target="_blank" rel="noopener noreferrer"&gt;Outlier Detection with One-class Classification using Python Machine Learning Client for SAP HANA&lt;/A&gt;&lt;BR /&gt;
&lt;H1 id="toc-hId--1541296922"&gt;&lt;/H1&gt;&lt;BR /&gt;
&lt;H1 id="toc-hId--1737810427"&gt;Appendix&lt;/H1&gt;&lt;BR /&gt;
&lt;H2 id="toc-hId-2067240357"&gt;SAP HANA Connection&lt;/H2&gt;&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;import hana_ml&lt;BR /&gt;
from hana_ml import dataframe&lt;BR /&gt;
conn = dataframe.ConnectionContext('host', 'port', 'username', 'password')&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;H2 id="toc-hId-1870726852"&gt;Functions for Table&lt;/H2&gt;&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;def createEmptyTable(table_name, proto, cc):&lt;BR /&gt;
    with cc.connection.cursor() as cur:&lt;BR /&gt;
        try:&lt;BR /&gt;
            joint = []&lt;BR /&gt;
            for key in proto:&lt;BR /&gt;
                joint.append(" ".join(['"{:s}"'.format(key), proto[key]]))&lt;BR /&gt;
            cur.execute('CREATE COLUMN TABLE %s (%s);' %&lt;BR /&gt;
                        (table_name, ",".join(joint)))&lt;BR /&gt;
        except:&lt;BR /&gt;
            print(&lt;BR /&gt;
                f"\"CREATE TABLE {table_name}\" was unsuccessful. Maybe the table has existed.")&lt;BR /&gt;
&lt;BR /&gt;
def dropTable(table_name, cc):&lt;BR /&gt;
    with cc.connection.cursor() as cur:&lt;BR /&gt;
        try:&lt;BR /&gt;
            cur.execute(f"DROP TABLE {table_name}")&lt;BR /&gt;
        except:&lt;BR /&gt;
            print(f"\"DROP TABLE {table_name}\" was unsuccessful. Maybe the table does not exist yet.")&lt;BR /&gt;
&lt;BR /&gt;
&lt;BR /&gt;
def createTableFromDataFrame(df, table_name, cc):&lt;BR /&gt;
    dropTable(table_name, cc)&lt;BR /&gt;
    dt_ml = dataframe.create_dataframe_from_pandas(cc, df, table_name=table_name, table_structure={"MODEL_CONTENT":"NCLOB"})&lt;BR /&gt;
    # dt_ml = dataframe.create_dataframe_from_pandas(cc, df, table_name=table_name, table_structure={"COL1":"CLOB"})&lt;BR /&gt;
    return dt_ml&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;H2 id="toc-hId-1842397038"&gt;Function of Calling the PAL Procedure of Outlier Detection for Time Series&lt;/H2&gt;&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;def OutlierDetectionForTimeSeries(df, parameters, cc,&lt;BR /&gt;
            data_table='ZPD_PAL_DATA_TBL',&lt;BR /&gt;
            parameter_table='ZPD_PAL_PARAMETERS_TBL',&lt;BR /&gt;
            result_table='ZPD_PAL_RESULT_TBL',&lt;BR /&gt;
            stats_table='ZPD_PAL_STATS_TBL',&lt;BR /&gt;
            metri_table='ZPD_PAL_METRI_TBL'):&lt;BR /&gt;
&lt;BR /&gt;
    # Input table&lt;BR /&gt;
    createTableFromDataFrame(df, data_table, cc)&lt;BR /&gt;
&lt;BR /&gt;
    # Result table&lt;BR /&gt;
    dropTable(result_table, cc)&lt;BR /&gt;
    createEmptyTable(result_table, {&lt;BR /&gt;
                     "TIMESTAMP": "INTEGER","RAW_DATA":"DOUBLE","RESIDUAL":"DOUBLE","OUTLIER_SCORE":"DOUBLE","IS_OUTLIER":"INTEGER"}, cc)&lt;BR /&gt;
&lt;BR /&gt;
    # Metri table&lt;BR /&gt;
    dropTable(metri_table, cc)&lt;BR /&gt;
    createEmptyTable(metri_table, {&lt;BR /&gt;
                     "STAT_NAME": "NVARCHAR(1000)","VALUE":"DOUBLE"}, cc)&lt;BR /&gt;
&lt;BR /&gt;
    # Stats table&lt;BR /&gt;
    dropTable(stats_table, cc)&lt;BR /&gt;
    createEmptyTable(stats_table, {&lt;BR /&gt;
                     "STAT_NAME": "NVARCHAR(1000)", "STAT_VALUE": "NVARCHAR(1000)"}, cc)&lt;BR /&gt;
&lt;BR /&gt;
    # Parameter table&lt;BR /&gt;
    dropTable(parameter_table, cc)&lt;BR /&gt;
    createEmptyTable(parameter_table, {"PARAM_NAME": "nvarchar(256)", "INT_VALUE": "integer",&lt;BR /&gt;
                     "DOUBLE_VALUE": "double", "STRING_VALUE": "nvarchar(1000)"}, cc)&lt;BR /&gt;
&lt;BR /&gt;
    if parameters:&lt;BR /&gt;
        with cc.connection.cursor() as cur:&lt;BR /&gt;
            for parName, parValue in parameters.items():&lt;BR /&gt;
&lt;BR /&gt;
                if isinstance(parValue, str):&lt;BR /&gt;
                    parValue = f"'{parValue}'"&lt;BR /&gt;
                    parametersSQL = f"{parValue if isinstance(parValue,int) else 'NULL'}, {parValue if isinstance(parValue,float) else 'NULL'}, { parValue if isinstance(parValue,str) else 'NULL'}"&lt;BR /&gt;
                    cur.execute(&lt;BR /&gt;
                    f"INSERT INTO {parameter_table} VALUES ('{parName}', {parametersSQL});")&lt;BR /&gt;
&lt;BR /&gt;
                elif isinstance(parValue,list):&lt;BR /&gt;
                    for x in parValue:&lt;BR /&gt;
                        if isinstance(x, str):&lt;BR /&gt;
                            x = f"'{x}'"&lt;BR /&gt;
                        parametersSQL = f"{x if isinstance(x,int) else 'NULL'}, {x if isinstance(x,float) else 'NULL'}, { x if isinstance(x,str) else 'NULL'}"&lt;BR /&gt;
                        cur.execute(&lt;BR /&gt;
                        f"INSERT INTO {parameter_table} VALUES ('{parName}', {parametersSQL});")&lt;BR /&gt;
                else:&lt;BR /&gt;
                    parametersSQL = f"{parValue if isinstance(parValue,int) else 'NULL'}, {parValue if isinstance(parValue,float) else 'NULL'}, { parValue if isinstance(parValue,str) else 'NULL'}"&lt;BR /&gt;
                    cur.execute(&lt;BR /&gt;
                    f"INSERT INTO {parameter_table} VALUES ('{parName}', {parametersSQL});")&lt;BR /&gt;
                    &lt;BR /&gt;
    else:&lt;BR /&gt;
        print("No parameters given using default values.")&lt;BR /&gt;
&lt;BR /&gt;
    sql_str = f"\&lt;BR /&gt;
        do begin \&lt;BR /&gt;
            lt_data = select * from {data_table}; \&lt;BR /&gt;
            lt_control = select * from {parameter_table};\&lt;BR /&gt;
            CALL _SYS_AFL.PAL_OUTLIER_DETECTION_FOR_TIME_SERIES(:lt_data, :lt_control, lt_res, lt_stats, lt_metri); \&lt;BR /&gt;
            INSERT INTO {result_table} SELECT * FROM :lt_res; \&lt;BR /&gt;
            INSERT INTO {stats_table} SELECT * FROM :lt_stats;\&lt;BR /&gt;
            INSERT INTO {metri_table} SELECT * FROM :lt_metri; \&lt;BR /&gt;
        end;"&lt;BR /&gt;
&lt;BR /&gt;
    with cc.connection.cursor() as cur:&lt;BR /&gt;
        cur.execute(sql_str)&lt;BR /&gt;
&lt;BR /&gt;
    return cc.table(result_table).collect(), cc.table(stats_table).collect(), cc.table(metri_table).collect()&lt;BR /&gt;
&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;H2 id="toc-hId-1645883533"&gt;Functions of Plotting Results&lt;/H2&gt;&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;def outlier_result_plot(dResults):&lt;BR /&gt;
    dResults.sort_values(by = list(dResults)[0], inplace = True, ascending = True)&lt;BR /&gt;
    raw_data = np.array(dResults['RAW_DATA'])&lt;BR /&gt;
    residual = np.array(dResults['RESIDUAL'])&lt;BR /&gt;
    outlier_score = np.array(dResults['OUTLIER_SCORE'])&lt;BR /&gt;
    is_outlier = np.array(dResults['IS_OUTLIER'])&lt;BR /&gt;
    plt.figure(figsize = (24,4.5))&lt;BR /&gt;
    plt.subplot(1,4,1)&lt;BR /&gt;
    plt.plot(raw_data)&lt;BR /&gt;
    plt.grid()&lt;BR /&gt;
    plt.title('RAW_DATA')&lt;BR /&gt;
    plt.subplot(1,4,2)&lt;BR /&gt;
    plt.plot(residual)&lt;BR /&gt;
    plt.grid()&lt;BR /&gt;
    plt.title('RESIDUAL')&lt;BR /&gt;
    plt.subplot(1,4,3)&lt;BR /&gt;
    plt.plot(outlier_score)&lt;BR /&gt;
    plt.grid()&lt;BR /&gt;
    plt.title('OUTLIER_SCORE')&lt;BR /&gt;
    plt.subplot(1,4,4)&lt;BR /&gt;
    plt.plot(is_outlier)&lt;BR /&gt;
    plt.grid()&lt;BR /&gt;
    plt.title('IS_OUTLIER')&lt;/CODE&gt;&lt;/PRE&gt;&lt;BR /&gt;
&lt;PRE class="language-python"&gt;&lt;CODE&gt;def outlier_plot(dResults):&lt;BR /&gt;
    dResults.sort_values(by = list(dResults)[0], inplace = True, ascending = True)&lt;BR /&gt;
    raw_data = np.array(dResults['RAW_DATA'])&lt;BR /&gt;
    is_outlier = np.array(dResults['IS_OUTLIER'])&lt;BR /&gt;
    outlier_idx = np.array([],dtype = int)&lt;BR /&gt;
    for i in range(len(is_outlier)):&lt;BR /&gt;
        if is_outlier[i] == 1:&lt;BR /&gt;
            outlier_idx = np.append(outlier_idx,i)&lt;BR /&gt;
    plt.plot(raw_data)&lt;BR /&gt;
    plt.scatter(outlier_idx,raw_data[outlier_idx],color = 'red')&lt;BR /&gt;
    plt.grid()&lt;BR /&gt;
    plt.title("series and outlier")&lt;/CODE&gt;&lt;/PRE&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/automatic-outlier-detection-for-time-series-in-sap-hana/ba-p/13559856"/>
    <published>2023-06-29T11:06:04+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/5-steps-to-a-business-data-fabric/ba-p/13580358</id>
    <title>5 Steps to a Business Data Fabric</title>
    <updated>2023-10-17T19:31:56+02:00</updated>
    <author>
      <name>SavannahVoll</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/13466</uri>
    </author>
    <content>&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/10/291513_GettyImages-1146500457_small.jpg" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;SPAN data-contrast="auto"&gt;Managing and leveraging data can be a daunting task. Businesses grapple with complex datasets from many different and unconnected sources, including operations, finance, marketing, customer success, and more. Plus, a lot of organizations are geographically dispersed and have complicated use cases or specific needs, like storing data across cloud, hybrid, multi-cloud, and on-premises devices.&lt;/SPAN&gt;&lt;SPAN data-ccp-props="{&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;SPAN data-contrast="auto"&gt; A &lt;/SPAN&gt;&lt;A href="https://blogs.sap.com/2023/09/15/what-is-a-business-data-fabric/" target="_blank" rel="noopener noreferrer"&gt;&lt;SPAN data-contrast="none"&gt;business data fabric&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN data-contrast="auto"&gt; offers a solution.&lt;/SPAN&gt;&lt;SPAN data-ccp-props="{&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;SPAN data-contrast="auto"&gt;This data management architecture provides an integrated, semantically-rich data layer over underlying data landscapes to deliver scalable access to data without duplication. &lt;/SPAN&gt;&lt;SPAN data-contrast="none"&gt;In other data platforms, when data is extracted from core systems, much of its original context is lost. A business data fabric preserves this context, helping ensure the data remains meaningful and relevant for decision-making, regardless of its origin.&lt;/SPAN&gt; &lt;SPAN data-ccp-props="{&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;SPAN data-contrast="auto"&gt;This approach &lt;/SPAN&gt;&lt;SPAN data-contrast="auto"&gt;offers a number of &lt;/SPAN&gt;&lt;A href="https://blogs.sap.com/2023/09/18/3-benefits-of-a-business-data-fabric/" target="_blank" rel="noopener noreferrer"&gt;&lt;SPAN data-contrast="none"&gt;benefits&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN data-contrast="auto"&gt; including enhanced data accessibility, improved data governance, and accelerated insights.&lt;/SPAN&gt;&lt;SPAN data-ccp-props="{&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;SPAN data-contrast="auto"&gt;But how do you get started?&lt;/SPAN&gt;&lt;SPAN data-ccp-props="{&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;BR /&gt;
&lt;H2 id="toc-hId-964759435"&gt;&lt;STRONG&gt;Implementation framework&amp;nbsp;&lt;/STRONG&gt;&lt;/H2&gt;&lt;BR /&gt;
&lt;SPAN data-contrast="auto"&gt;Let’s look at a high-level framework for implementing a business data fabric architecture in your organization.&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN data-ccp-props="{&amp;quot;134233117&amp;quot;:false,&amp;quot;134233118&amp;quot;:false,&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559685&amp;quot;:0,&amp;quot;335559737&amp;quot;:0,&amp;quot;335559738&amp;quot;:0,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;BR /&gt;
&lt;H3 id="toc-hId-897328649"&gt;&lt;STRONG&gt;1. Data Ingestion&amp;nbsp;&lt;/STRONG&gt;&lt;/H3&gt;&lt;BR /&gt;
&lt;SPAN data-contrast="auto"&gt;The first step is to ensure that all your data, whether it’s structured or unstructured, can be easily ingested into the system. A business data fabric, with its open data ecosystem, allows for simple data ingestion, regardless of the source or the format of the data.&lt;/SPAN&gt;&lt;SPAN data-ccp-props="{&amp;quot;134233117&amp;quot;:false,&amp;quot;134233118&amp;quot;:false,&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559685&amp;quot;:0,&amp;quot;335559737&amp;quot;:0,&amp;quot;335559738&amp;quot;:0,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;BR /&gt;
&lt;H3 id="toc-hId-700815144"&gt;&lt;STRONG&gt;2. Data Integration&amp;nbsp;&lt;/STRONG&gt;&lt;/H3&gt;&lt;BR /&gt;
&lt;SPAN data-contrast="auto"&gt;Data from various sources must be integrated and transformed into a unified format easily consumed by data users. The interoperability of a business data fabric enables data from different sources to be combined and connected rather than being moved around.&lt;/SPAN&gt;&lt;SPAN data-ccp-props="{&amp;quot;134233117&amp;quot;:false,&amp;quot;134233118&amp;quot;:false,&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559685&amp;quot;:0,&amp;quot;335559737&amp;quot;:0,&amp;quot;335559738&amp;quot;:0,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;BR /&gt;
&lt;H3 id="toc-hId-504301639"&gt;&lt;STRONG&gt;3. Data Governance&amp;nbsp;&lt;/STRONG&gt;&lt;/H3&gt;&lt;BR /&gt;
&lt;SPAN data-contrast="auto"&gt;With the growing c&lt;/SPAN&gt;&lt;SPAN data-contrast="auto"&gt;omplexity and volume of data, governance becomes an increasingly important topic. This includes ensuring data quality, privacy, and compliance with various regulations. A business data fabric ensures effective governance by maintaining metadata, lineage, and control measures.&lt;/SPAN&gt;&lt;SPAN data-ccp-props="{&amp;quot;134233117&amp;quot;:false,&amp;quot;134233118&amp;quot;:false,&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559685&amp;quot;:0,&amp;quot;335559737&amp;quot;:0,&amp;quot;335559738&amp;quot;:0,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;BR /&gt;
&lt;H3 id="toc-hId-307788134"&gt;&lt;STRONG&gt;4. Data Cataloging&amp;nbsp;&lt;/STRONG&gt;&lt;/H3&gt;&lt;BR /&gt;
&lt;SPAN data-contrast="auto"&gt;This involves creating an inventory of data assets and their metadata. The catalog serves as a single source of truth for users to find, understand, and trust the data they need. It’s a critical component of the business data fabric that allows data consumers to understand the business semantics.&lt;/SPAN&gt;&lt;SPAN data-ccp-props="{&amp;quot;134233117&amp;quot;:false,&amp;quot;134233118&amp;quot;:false,&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559685&amp;quot;:0,&amp;quot;335559737&amp;quot;:0,&amp;quot;335559738&amp;quot;:0,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;BR /&gt;
&lt;H3 id="toc-hId-111274629"&gt;&lt;STRONG&gt;5. Data Consumption&amp;nbsp;&lt;/STRONG&gt;&lt;/H3&gt;&lt;BR /&gt;
&lt;SPAN data-contrast="auto"&gt;This is about delivering the right data, in the right format, at the right time, to the right people. The business data fabric supports data federation, which enables unified and consistent access to data across diverse sources, reducing redundancy. It ensures data is presented in business-friendly terms and contexts, making it simple for data consumers to interpret and use the data for their specific use cases.&lt;/SPAN&gt;&lt;SPAN data-ccp-props="{&amp;quot;134233117&amp;quot;:false,&amp;quot;134233118&amp;quot;:false,&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559685&amp;quot;:0,&amp;quot;335559737&amp;quot;:0,&amp;quot;335559738&amp;quot;:0,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;SPAN data-contrast="auto"&gt;This is where SAP provides the foundation for a business data fabric: SAP Datasphere.&lt;/SPAN&gt;&lt;SPAN data-ccp-props="{&amp;quot;134233117&amp;quot;:false,&amp;quot;134233118&amp;quot;:false,&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559685&amp;quot;:0,&amp;quot;335559737&amp;quot;:0,&amp;quot;335559738&amp;quot;:0,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;BR /&gt;
&lt;H2 id="toc-hId--214321595"&gt;&lt;STRONG&gt;Transform your organization with SAP Datasphere&amp;nbsp;&amp;nbsp;&lt;/STRONG&gt;&lt;/H2&gt;&lt;BR /&gt;
&lt;A href="https://www.sap.com/canada/products/technology-platform/datasphere.html" target="_blank" rel="noopener noreferrer"&gt;&lt;SPAN data-contrast="none"&gt;SAP Datasphere&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN data-contrast="auto"&gt; is a comprehensive data service that empowers users&amp;nbsp;to provide seamless and scalable access to mission-critical business data.&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN data-ccp-props="{&amp;quot;134233117&amp;quot;:false,&amp;quot;134233118&amp;quot;:false,&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559685&amp;quot;:0,&amp;quot;335559737&amp;quot;:0,&amp;quot;335559738&amp;quot;:0,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;SPAN data-contrast="auto"&gt;It makes it easy for organizations to deliver meaningful data to every data consumer with business context and logic intact. &lt;/SPAN&gt;&lt;SPAN data-contrast="none"&gt;As organizations need accurate data that is quickly available and described with business-friendly terms, this approach enables data professionals to permeate the clarity that business semantics provide throughout every use case.&lt;/SPAN&gt;&lt;SPAN data-ccp-props="{&amp;quot;134233117&amp;quot;:false,&amp;quot;134233118&amp;quot;:false,&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559685&amp;quot;:0,&amp;quot;335559737&amp;quot;:0,&amp;quot;335559738&amp;quot;:0,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;SPAN data-contrast="none"&gt;In a major moment for our industry and customers, SAP is partnering with other open data partners —&amp;nbsp;&lt;/SPAN&gt;&lt;A href="https://www.databricks.com/" target="_blank" rel="nofollow noopener noreferrer"&gt;&lt;SPAN data-contrast="none"&gt;Databricks&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN data-contrast="none"&gt;,&amp;nbsp;&lt;/SPAN&gt;&lt;A href="https://collibra.com/sap-partnership" target="_blank" rel="nofollow noopener noreferrer"&gt;&lt;SPAN data-contrast="none"&gt;Collibra&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN data-contrast="none"&gt;,&amp;nbsp;&lt;/SPAN&gt;&lt;A href="https://www.confluent.io/" target="_blank" rel="nofollow noopener noreferrer"&gt;&lt;SPAN data-contrast="none"&gt;Confluent&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN data-contrast="none"&gt;, &lt;/SPAN&gt;&lt;A href="https://www.datarobot.com/blog/datarobot-and-sap-partner-to-deliver-joint-enterprise-ai-solution/" target="_blank" rel="nofollow noopener noreferrer"&gt;&lt;SPAN data-contrast="none"&gt;DataRobot,&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN data-contrast="none"&gt; and &lt;/SPAN&gt;&lt;A href="https://discover.sap.com/google/en-us/index.html" target="_blank" rel="noopener noreferrer"&gt;&lt;SPAN data-contrast="none"&gt;Google Cloud&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN data-contrast="none"&gt; — to radically simplify customers’ data landscapes. By closely integrating their data and AI platforms with SAP Datasphere, organizations can access their mission-critical business data across any cloud infrastructure.&lt;/SPAN&gt;&lt;SPAN data-ccp-props="{&amp;quot;134233117&amp;quot;:false,&amp;quot;134233118&amp;quot;:false,&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559685&amp;quot;:0,&amp;quot;335559737&amp;quot;:0,&amp;quot;335559738&amp;quot;:0,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;SPAN data-contrast="none"&gt;SAP Datasphere, and its open data ecosystem, is the technology foundation that enables a &lt;/SPAN&gt;&lt;A href="https://news.sap.com/2023/03/sap-datasphere-power-of-business-data/" target="_blank" rel="noopener noreferrer"&gt;&lt;SPAN data-contrast="none"&gt;business data fabric&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN data-contrast="none"&gt;. &lt;/SPAN&gt;&lt;SPAN data-ccp-props="{&amp;quot;134233117&amp;quot;:false,&amp;quot;134233118&amp;quot;:false,&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559685&amp;quot;:0,&amp;quot;335559737&amp;quot;:0,&amp;quot;335559738&amp;quot;:0,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;BR /&gt;
&lt;H2 id="toc-hId--410835100"&gt;&lt;STRONG&gt;Learn more&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/STRONG&gt;&lt;/H2&gt;&lt;BR /&gt;
&lt;SPAN data-contrast="auto"&gt;Read the new &lt;/SPAN&gt;&lt;A href="https://www.sap.com/documents/2023/10/4675c6a3-927e-0010-bca6-c68f7e60039b.html" target="_blank" rel="noopener noreferrer"&gt;&lt;SPAN data-contrast="none"&gt;e-book&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN data-contrast="auto"&gt; to learn more about the practical applications of a business data fabric, including: &lt;/SPAN&gt;&lt;SPAN data-ccp-props="{&amp;quot;134233117&amp;quot;:false,&amp;quot;134233118&amp;quot;:false,&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559685&amp;quot;:0,&amp;quot;335559737&amp;quot;:0,&amp;quot;335559738&amp;quot;:0,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;BR /&gt;
&lt;UL&gt;&lt;BR /&gt;
 	&lt;LI data-leveltext="" data-font="Symbol" data-listid="1" data-list-defn-props="{&amp;quot;335552541&amp;quot;:1,&amp;quot;335559684&amp;quot;:-2,&amp;quot;335559685&amp;quot;:720,&amp;quot;335559991&amp;quot;:360,&amp;quot;469769226&amp;quot;:&amp;quot;Symbol&amp;quot;,&amp;quot;469769242&amp;quot;:[8226],&amp;quot;469777803&amp;quot;:&amp;quot;left&amp;quot;,&amp;quot;469777804&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;469777815&amp;quot;:&amp;quot;hybridMultilevel&amp;quot;}" data-aria-posinset="1" data-aria-level="1"&gt;&lt;SPAN data-contrast="auto"&gt;Why you need a business data fabric&lt;/SPAN&gt;&lt;SPAN data-ccp-props="{&amp;quot;134233117&amp;quot;:false,&amp;quot;134233118&amp;quot;:false,&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559737&amp;quot;:0,&amp;quot;335559738&amp;quot;:0,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI data-leveltext="" data-font="Symbol" data-listid="1" data-list-defn-props="{&amp;quot;335552541&amp;quot;:1,&amp;quot;335559684&amp;quot;:-2,&amp;quot;335559685&amp;quot;:720,&amp;quot;335559991&amp;quot;:360,&amp;quot;469769226&amp;quot;:&amp;quot;Symbol&amp;quot;,&amp;quot;469769242&amp;quot;:[8226],&amp;quot;469777803&amp;quot;:&amp;quot;left&amp;quot;,&amp;quot;469777804&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;469777815&amp;quot;:&amp;quot;hybridMultilevel&amp;quot;}" data-aria-posinset="2" data-aria-level="1"&gt;&lt;SPAN data-contrast="auto"&gt;How to implement a business data fabric&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN data-ccp-props="{&amp;quot;134233117&amp;quot;:false,&amp;quot;134233118&amp;quot;:false,&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559737&amp;quot;:0,&amp;quot;335559738&amp;quot;:0,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI data-leveltext="" data-font="Symbol" data-listid="1" data-list-defn-props="{&amp;quot;335552541&amp;quot;:1,&amp;quot;335559684&amp;quot;:-2,&amp;quot;335559685&amp;quot;:720,&amp;quot;335559991&amp;quot;:360,&amp;quot;469769226&amp;quot;:&amp;quot;Symbol&amp;quot;,&amp;quot;469769242&amp;quot;:[8226],&amp;quot;469777803&amp;quot;:&amp;quot;left&amp;quot;,&amp;quot;469777804&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;469777815&amp;quot;:&amp;quot;hybridMultilevel&amp;quot;}" data-aria-posinset="3" data-aria-level="1"&gt;&lt;SPAN data-contrast="auto"&gt;Five business data fabric use cases&lt;/SPAN&gt;&lt;SPAN data-ccp-props="{&amp;quot;134233117&amp;quot;:false,&amp;quot;134233118&amp;quot;:false,&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559737&amp;quot;:0,&amp;quot;335559738&amp;quot;:0,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;/LI&gt;&lt;BR /&gt;
&lt;/UL&gt;&lt;BR /&gt;
&lt;SPAN data-contrast="auto"&gt;Get started with the &lt;/SPAN&gt;&lt;A href="https://www.sap.com/documents/2023/10/4675c6a3-927e-0010-bca6-c68f7e60039b.html" target="_blank" rel="noopener noreferrer"&gt;&lt;SPAN data-contrast="none"&gt;Five Steps to a Business Data Fabric Architecture&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN data-contrast="auto"&gt; e-book today.&amp;nbsp;&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN data-ccp-props="{&amp;quot;134233117&amp;quot;:false,&amp;quot;134233118&amp;quot;:false,&amp;quot;201341983&amp;quot;:0,&amp;quot;335551550&amp;quot;:1,&amp;quot;335551620&amp;quot;:1,&amp;quot;335559685&amp;quot;:0,&amp;quot;335559737&amp;quot;:0,&amp;quot;335559738&amp;quot;:0,&amp;quot;335559739&amp;quot;:160,&amp;quot;335559740&amp;quot;:259}"&gt;&amp;nbsp;&lt;/SPAN&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;A href="https://www.sap.com/documents/2023/10/4675c6a3-927e-0010-bca6-c68f7e60039b.html" target="_blank" rel="noopener noreferrer"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/10/Five-Steps-to-a-BDF_Paid_1200x627_V2.png" /&gt;&lt;/A&gt;&lt;/P&gt;&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;BR /&gt;
&amp;nbsp;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/5-steps-to-a-business-data-fabric/ba-p/13580358"/>
    <published>2023-10-17T19:31:56+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/why-business-data-is-fundamental-to-artificial-intelligence/ba-p/13572569</id>
    <title>Why Business Data is Fundamental to Artificial Intelligence</title>
    <updated>2023-10-30T23:09:56+01:00</updated>
    <author>
      <name>i032821</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/148569</uri>
    </author>
    <content>The introduction of cloud computing has enabled organisations all over the world to store vast amounts of data in a cost-effective way as they digitally transform their business operations.&amp;nbsp; Data has commonly been referred to as the 'new oil' and is where companies are looking to help increase their productivity going forward.&amp;nbsp; However, in order to harness this technology the data needs to be relevant, reliable and responsible.&amp;nbsp; As the adage goes, garbage in, garbage out.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/10/Image-01.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;BR /&gt;
AI serves as a powerful tool for extracting actionable insights from the vast amount of reliable data generated and stored within SAP systems.&amp;nbsp; Combining AI with SAP BTP, advanced data analytics and machine learning algorithms becomes possible, allowing organisations to tap into the potential of their SAP data along with the AI technologies available in the market.&lt;BR /&gt;
&lt;BR /&gt;
Five pillars support SAP BTP: App development, automation, integration, data analytics, and AI. These pillars interplay with embedded intelligent technologies like situation handling, machine learning, and analytics, all fully integrated within the SAP S/4HANA Cloud. Furthermore, side-by-side capabilities through SAP BTP offer additional intelligent industry functionalities like Intelligent Situation Automation, SAP Build Process Automation, and chatbot technology.&lt;BR /&gt;
&lt;BR /&gt;
By harnessing artificial intelligence, SAP BTP combines business data from S4 with external data, enabling the creation of increasingly precise models in real-time.&amp;nbsp; This ensures a versatile and agile platform that propels innovation while retaining a clean digital core. This demonstrates a shift from traditional systems of record to systems of intelligence.&lt;BR /&gt;
&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;BR /&gt;
&lt;STRONG&gt;AI-Powered Capabilities&amp;nbsp;&lt;/STRONG&gt;&lt;BR /&gt;
&lt;BR /&gt;
SAP has embedded AI into its products for many years, from journal reconciliations in S4 to AI-powered writing assistants aimed to streamline HR-related tasks in Success Factors.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/10/Image-02.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;BR /&gt;
These innovative functionalities are not merely theoretical but are practically applicable, ensuring HR admins, managers, and employees can operate more efficiently.&amp;nbsp; In fact, these innovations exist across all the functions in your SAP landscape such as procurement, finance, and human resources.&lt;BR /&gt;
&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;BR /&gt;
&lt;STRONG&gt;How to get started?&lt;/STRONG&gt;&lt;BR /&gt;
&lt;BR /&gt;
Automation is pivotal for managing manual and repetitive tasks, especially those involving the consolidation and manipulation of data from diverse sources like MS Excel, vendor portals, and SAP systems. High-volume processes, often exceeding 1000 steps a day—such as data migrations and approvals—and those requiring access to multiple applications, can be streamlined, ensuring seamless operation across your SAP environment.&lt;BR /&gt;
&lt;BR /&gt;
SAP has provided templates across all the business functions to accelerate these initiatives, as shown below.&lt;BR /&gt;
&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/10/Image-03.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;BR /&gt;
&lt;STRONG&gt;A More Advanced AI Use Case&lt;/STRONG&gt;&lt;BR /&gt;
&lt;BR /&gt;
The transition from a rules-based approach to an AI-empowered, data-driven model is illustrated through an example case study of an Australian customer.&amp;nbsp; A decade ago, an employee scripted manual “if:then” statements for road upgrades; a process that has now been revolutionised by AI. AI can now analyse these rules and infuse them with real-time data like weather, road usage, and vehicle types. As a part of their operations, this customer assesses road conditions using specialised trucks called profilometers, generating colossal data volumes that outpace their storage capacities. SAP BTP, however, can house this data in expansive lakes, giving AI the agility to model exponentially precise “if:then” statements.&lt;BR /&gt;
&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;BR /&gt;
The shift will allow this customer to manage large datasets from disparate sources seamlessly, scaling memory and compute capabilities to handle big data without losing granularity. Moreover, unlike fixed rules, the AI algorithms continually evolve based on data, thereby ensuring maintenance and road upgrade strategies that are timely, relevant, and efficient.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/10/Image-05-2.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
In the realm of road maintenance, AI’s practical application is manifest, where even a small percentage improvement can result in significant savings for this customer.&amp;nbsp; This financial efficacy, combined with the potential to extend the useful life of assets, underscores the tangible, impactful benefits of combining AI with SAP BTP.&lt;BR /&gt;
&lt;BR /&gt;
&lt;STRONG&gt;In Summation&lt;/STRONG&gt;&lt;BR /&gt;
&lt;BR /&gt;
Early AI integration can offer businesses a decisive advantage. SAP’s AI vision isn’t just about pioneering technology; it's about tangible, real-world applications. From simple tools deployable within days to intricate endeavours with broad impact.&lt;BR /&gt;
&lt;BR /&gt;
If you’d like to find out about the value AI can bring to businesses through automation and explore other use cases, then visit the &lt;A href="https://www.sap.com/australia/products/artificial-intelligence.html" target="_blank" rel="noopener noreferrer"&gt;&lt;STRONG&gt;SAP Business AI&lt;/STRONG&gt;&lt;/A&gt; website.</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/why-business-data-is-fundamental-to-artificial-intelligence/ba-p/13572569"/>
    <published>2023-10-30T23:09:56+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/sap-and-dxc-team-plan-to-deliver-rise-with-sap-s-4hana-cloud-in-customer/ba-p/13578185</id>
    <title>SAP and DXC team plan to deliver RISE with SAP S/4HANA Cloud, in customer data centers and co-location facilities, creating a new and powerful platform for digital transformation</title>
    <updated>2023-11-06T18:39:34+01:00</updated>
    <author>
      <name>j_zarb</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/631199</uri>
    </author>
    <content>SAP and DXC aim to deliver RISE with SAP S/4HANA Cloud, private edition, customer data center option as a turn-key service delivered by DXC.&amp;nbsp; The new service is ideally suited for Private Cloud customers and other managed services customers who wish to run SAP either from their own data center or a DXC managed data center and get the transformational benefits of RISE with SAP.&lt;BR /&gt;
&lt;BR /&gt;
In this blog, DXC reaffirms its commitment as a Partner Managed Cloud (PMC) service provider of RISE with SAP by expanding its distinct deployment capabilities already announced with DXC Hyperscaler solutions to support the customer data center option.&lt;BR /&gt;
&lt;BR /&gt;
This partnership empowers DXC and SAP to offer a comprehensive catalog of managed services and extraordinary opportunities that surpass what each entity could achieve independently, benefiting our mutual customers.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/11/DXC.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;STRONG&gt;RISE with SAP S/4HANA Cloud, private edition, customer data center option&lt;/STRONG&gt; or “CDC” represents the hybridization of SAP’s strategic RISE cloud solution, where a customer can run S/4HANA as a cloud service from their data center; while accessing SAP BTP and SAP Signavio, and all the other innovative RISE components from the public cloud – aka a mixture of internal and external cloud services.&amp;nbsp; For more information about CDC, please visit this dedicated web page: &amp;nbsp;&lt;A href="https://www.sap.com/products/erp/rise/customer-data-center.html" target="_blank" rel="noopener noreferrer"&gt;Customer data center option | RISE with SAP&lt;/A&gt;&lt;BR /&gt;
&lt;BR /&gt;
Similar to SAP’s cloud solutions, CDC provides diverse deployment options on Lenovo, HPE and Dell infrastructure.&amp;nbsp; DXC enhances this offering further by enabling delivery of SAP and non-SAP Infrastructure as a Service (IaaS), platform as a service (PaaS), and software as a service (SaaS), all backed by a top service level agreement.&lt;BR /&gt;
&lt;BR /&gt;
&lt;STRONG&gt;With DXC, SAP’s strategic RISE Cloud offering deployable in Customer Data Centers,&lt;/STRONG&gt; is specifically designed to:&lt;BR /&gt;
&lt;OL&gt;&lt;BR /&gt;
 	&lt;LI&gt;Enhance RISE with SAP by harnessing DXC’s expertise in delivering SAP and non-SAP managed services and enabling mutual customers to become more agile and alleviate the challenges often associated with digital transformation projects. This is achieved through best-of-breed solutions and a wealth of experience and skills.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Establish a secure infrastructure and platform for SAP within the customer data centers, backed, managed, designed to perform by SAP and DXC.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Empower customers who want to run in their data center (on-premise) while staying aligned with SAP’s cloud innovation agenda, including ML/AI (Machine Learning / Artificial Intelligence), LLMS (large language models), etc.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Meet the needs of industries with strict regulatory compliance requirements that may prevent running SAP in a public shared Hyperscaler such as utilities, public sector, healthcare, pharmaceuticals, aerospace &amp;amp; defense, etc.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Provide extended services to address specific data sovereignty needs of customers, governments, and industry stakeholders by keeping sensitive data within their national boundaries, governed by local laws.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Offer an innovative approach for those seeking a cloud OpEx model while benefiting from a high-performance dedicated onPrem system with minimal latency.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Benefit from a dedicated on-premise setup, without the data center environment having to be managed by the customer.&lt;/LI&gt;&lt;BR /&gt;
&lt;/OL&gt;&lt;BR /&gt;
To learn more about the DXC &amp;amp; SAP cloud solutions and the DXC Premier Services for RISE with SAP, please visit the following link: &lt;A href="https://dxc.com/us/en/offerings/applications/eas-sap/dxc-premier-services-for-rise-with-sap" target="_blank" rel="nofollow noopener noreferrer"&gt;DXC Premier Services for RISE with SAP&lt;/A&gt;&lt;BR /&gt;
&lt;BR /&gt;
All thoughts and questions are welcome, please share your comments below to contribute to this discussion.&lt;BR /&gt;
&lt;BR /&gt;
Joseph Zarb&lt;BR /&gt;
Head of RISE with SAP – Customer Data Center&lt;BR /&gt;
SAP RISE Global GTM Execution&lt;BR /&gt;
10 Hudson Yards, 51st Floor, New York NY 10001 USA&lt;BR /&gt;
&lt;BR /&gt;
j.zarb@sap.com</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-and-dxc-team-plan-to-deliver-rise-with-sap-s-4hana-cloud-in-customer/ba-p/13578185"/>
    <published>2023-11-06T18:39:34+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/how-to-connect-sap-ecc-s-4hana-on-prem-and-private-cloud-with-confluent/ba-p/13575214</id>
    <title>How to connect SAP ECC + S/4HANA on-prem and private cloud with Confluent Cloud or Confluent Platform (1)</title>
    <updated>2023-12-01T15:27:08+01:00</updated>
    <author>
      <name>FlorianFarr</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/176163</uri>
    </author>
    <content>This blog post explains how to connect SAP ECC or S/4HANA on-prem and private cloud editions with Confluent Cloud or Confluent Platform.&lt;BR /&gt;
Whether you're using SAP NetWeaver Event-enablement Add-on or ASAPIO Integration Add-on, this step-by-step guide provides all you need as a first step to enable SAP systems for communication with a Confluent broker.&lt;BR /&gt;
&lt;H2 id="toc-hId-963983780"&gt;Architecture / Connection types&lt;/H2&gt;&lt;BR /&gt;
When connecting SAP with Confluent, it is important to understand that there are two very different approaches in terms of connection architecture.&lt;BR /&gt;
&lt;H3 id="toc-hId-896552994"&gt;&lt;SPAN style="font-size: 1rem"&gt;Using a REST Proxy&lt;/SPAN&gt;&lt;/H3&gt;&lt;BR /&gt;
"Confluent REST Proxy for Kafka" or a similar product is the standard approach currently and therefore can be considered mandatory for the connectivity.&lt;BR /&gt;
Please see &lt;A href="https://github.com/confluentinc/kafka-rest" target="_blank" rel="nofollow noopener noreferrer"&gt;https://github.com/confluentinc/kafka-rest&lt;/A&gt; for details.&lt;BR /&gt;
&lt;BR /&gt;
Reason for using REST instead of AMQP is, that SAP ECC does not support streaming protocols and 3rd-party-libraries cannot be used in SAP-certified Add-ons.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/11/aia_confluent_architecture_2024.png" height="334" width="261" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;P class="image_caption" style="text-align: center;font-style: italic"&gt;SAP-to-Confluent Architecture&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;H3 id="toc-hId-700039489"&gt;&lt;SPAN style="font-size: 1rem"&gt;Direct connect/V3:&lt;/SPAN&gt;&lt;/H3&gt;&lt;BR /&gt;
A Connector for Confluent Cloud direct connect (V3 REST API) is available as pre-release in the download section for registered ASAPIO customers. This connector can only handle outbound connectivity at the time of this blog being published (November 2023). For inbound connectivity, the REST proxy approach above is still required.&lt;BR /&gt;
&lt;H3 id="toc-hId-503525984"&gt;&lt;SPAN style="font-size: 1rem"&gt;AMQP support:&lt;/SPAN&gt;&lt;/H3&gt;&lt;BR /&gt;
AMQP support is planned to be released by ASAPIO in 2024, for S/4HANA systems.&lt;BR /&gt;
&lt;H2 id="toc-hId-177929760"&gt;System prerequisites&lt;/H2&gt;&lt;BR /&gt;
Before diving into the integration process, make sure you have the following components available:&lt;BR /&gt;
&lt;H3 id="toc-hId-110498974"&gt;Software components required on your SAP system&lt;/H3&gt;&lt;BR /&gt;
&lt;UL&gt;&lt;BR /&gt;
 	&lt;LI&gt;SAP NetWeaver Event-enablement Add-on (SAP Event Mesh Edition)&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;or, alternatively, ASAPIO Integration Add-on - Framework (full version)&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;ASAPIO Connector for Confluent (using REST proxy)&lt;/LI&gt;&lt;BR /&gt;
&lt;/UL&gt;&lt;BR /&gt;
For direct connect/V3 REST API, a pre-release Connector for Confluent Cloud is available for registered ASAPIO customers.&lt;BR /&gt;
&lt;H3 id="toc-hId--86014531"&gt;Non-SAP components&lt;/H3&gt;&lt;BR /&gt;
Please make sure you have endpoint URI and authorization data at hand for:&lt;BR /&gt;
&lt;UL&gt;&lt;BR /&gt;
 	&lt;LI&gt;Confluent Components:&lt;BR /&gt;
&lt;UL&gt;&lt;BR /&gt;
 	&lt;LI&gt;Confluent REST Proxy for Kafka (&lt;A href="https://docs.confluent.io/platform/current/kafka-rest/index.html" target="_blank" rel="nofollow noopener noreferrer"&gt;More info&lt;/A&gt;)&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Confluent Cloud&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Or, alternatively Confluent Platform&lt;/LI&gt;&lt;BR /&gt;
&lt;/UL&gt;&lt;BR /&gt;
&lt;/LI&gt;&lt;BR /&gt;
&lt;/UL&gt;&lt;BR /&gt;
&lt;STRONG&gt;Licensing&lt;/STRONG&gt;&lt;BR /&gt;
&lt;BR /&gt;
All software above requires the purchase of appropriate licenses.&lt;BR /&gt;
&lt;H2 id="toc-hId--411610755"&gt;Set-up Connectivity&lt;/H2&gt;&lt;BR /&gt;
&lt;H3 id="toc-hId--479041541"&gt;1. Create RFC Destinations to Confluent REST Proxy&lt;/H3&gt;&lt;BR /&gt;
Transaction: &lt;CODE&gt;SM59&lt;/CODE&gt;&lt;BR /&gt;
&lt;BR /&gt;
Type: "G" (HTTP Connection to External Server)&lt;BR /&gt;
&lt;BR /&gt;
Target Host: Endpoint of Confluent REST Proxy&lt;BR /&gt;
&lt;BR /&gt;
Save and perform a "Connection Test" to ensure HTTP status code 200.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/11/confluent_blog1.png" height="344" width="331" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;H3 id="toc-hId--675555046"&gt;2. Set-up Authentication to REST Proxy&lt;/H3&gt;&lt;BR /&gt;
Pre-requisites: Obtain user and password for the REST proxy or exchange certificates with the SAP system.&lt;BR /&gt;
&lt;BR /&gt;
Transaction: &lt;CODE&gt;SM59&lt;/CODE&gt;&lt;BR /&gt;
&lt;BR /&gt;
Choose the correct RFC destination, go to "Logon &amp;amp; Security," and select authentication method:&lt;BR /&gt;
&lt;UL&gt;&lt;BR /&gt;
 	&lt;LI&gt;"Basic Authentication" with username and password&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;SSL certificate-based authentication&lt;/LI&gt;&lt;BR /&gt;
&lt;/UL&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/11/confluent_blog2.png" height="243" width="422" /&gt;&lt;/P&gt;&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;H3 id="toc-hId--947299920"&gt;3. Set-up Basic Settings&lt;/H3&gt;&lt;BR /&gt;
Activate BC-Sets: Use &lt;CODE&gt;SCPR20&lt;/CODE&gt; to activate BC-Sets for cloud adapter and codepages.&lt;BR /&gt;
&lt;BR /&gt;
Configure Cloud Adapter: In &lt;CODE&gt;SPRO&lt;/CODE&gt;, go to ASAPIO Cloud Integrator, Maintain Cloud Adapter, and add an entry for the Confluent connector.&lt;BR /&gt;
&lt;H3 id="toc-hId--1143813425"&gt;4. Set-up Connection Instance&lt;/H3&gt;&lt;BR /&gt;
Transaction: &lt;CODE&gt;SPRO&lt;/CODE&gt; or &lt;CODE&gt;/ASADEV/68000202&lt;/CODE&gt;&lt;BR /&gt;
&lt;BR /&gt;
Add a new entry specifying connection details, RFC destination, ISO code, and cloud type.&lt;BR /&gt;
&lt;H3 id="toc-hId--1340326930"&gt;5. Set-up Error Type Mapping&lt;/H3&gt;&lt;BR /&gt;
Create an entry mapping response codes to message types.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/11/confluent_blog5-1.png" height="243" width="492" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;H3 id="toc-hId--1536840435"&gt;6. Set-up Connection Values&lt;/H3&gt;&lt;BR /&gt;
Maintain default values for the connection to Confluent in &lt;CODE&gt;Connections -&amp;gt; Default values&lt;/CODE&gt;.&lt;BR /&gt;
&lt;TABLE style="height: 96px" width="560"&gt;&lt;BR /&gt;
&lt;TBODY&gt;&lt;BR /&gt;
&lt;TR&gt;&lt;BR /&gt;
&lt;TH style="width: 199.512px"&gt;Default Attribute&lt;/TH&gt;&lt;BR /&gt;
&lt;TH style="width: 345.688px"&gt;Default Attribute Value&lt;/TH&gt;&lt;BR /&gt;
&lt;/TR&gt;&lt;BR /&gt;
&lt;TR&gt;&lt;BR /&gt;
&lt;TD style="width: 199.512px"&gt;KAFKA_ACCEPT&lt;/TD&gt;&lt;BR /&gt;
&lt;TD style="width: 345.688px"&gt;application/vnd.kafka.v2+json&lt;/TD&gt;&lt;BR /&gt;
&lt;/TR&gt;&lt;BR /&gt;
&lt;TR&gt;&lt;BR /&gt;
&lt;TD style="width: 199.512px"&gt;KAFKA_CALL_METHOD&lt;/TD&gt;&lt;BR /&gt;
&lt;TD style="width: 345.688px"&gt;POST&lt;/TD&gt;&lt;BR /&gt;
&lt;/TR&gt;&lt;BR /&gt;
&lt;TR&gt;&lt;BR /&gt;
&lt;TD style="width: 199.512px"&gt;KAFKA_CONTENT_TYPE&lt;/TD&gt;&lt;BR /&gt;
&lt;TD style="width: 345.688px"&gt;application/vnd.kafka.json.v2+json&lt;BR /&gt;
(or application/vnd.kafka.jsonschema.v2+json)&lt;/TD&gt;&lt;BR /&gt;
&lt;/TR&gt;&lt;BR /&gt;
&lt;/TBODY&gt;&lt;BR /&gt;
&lt;/TABLE&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/11/confluent_blog6.png" height="194" width="490" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;H2 id="toc-hId--1439950933"&gt;Set-up Outbound Messaging&lt;/H2&gt;&lt;BR /&gt;
&lt;H3 id="toc-hId--1929867445"&gt;1. Create Message Type&lt;/H3&gt;&lt;BR /&gt;
Transaction: &lt;CODE&gt;WE81&lt;/CODE&gt;&lt;BR /&gt;
&lt;BR /&gt;
Add a new entry specifying a unique name and description for the integration.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/11/confluent_blog7.png" height="70" width="490" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;H3 id="toc-hId--2126380950"&gt;2. Activate Message Type&lt;/H3&gt;&lt;BR /&gt;
Transaction: &lt;CODE&gt;BD50&lt;/CODE&gt;&lt;BR /&gt;
&lt;BR /&gt;
Activate the created message type.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/11/confluent_blog8.png" height="76" width="283" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;H3 id="toc-hId-1972072841"&gt;3. Set-up additional settings in 'Header Attributes'&lt;/H3&gt;&lt;BR /&gt;
Configure the topic, fields for the key, and schema IDs for key/value schemas.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/11/confluent_blog9.png" height="152" width="529" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;H3 id="toc-hId-1775559336"&gt;4. Set up 'Business Object Event Linkage'&lt;/H3&gt;&lt;BR /&gt;
Link the configuration of the outbound object to a Business Object event.&lt;BR /&gt;
&lt;H2 id="toc-hId-1872448838"&gt;Send a "Simple Notifications" event for testing&lt;/H2&gt;&lt;BR /&gt;
&lt;H3 id="toc-hId-1550716017"&gt;1. Create Outbound Object Configuration&lt;/H3&gt;&lt;BR /&gt;
Transaction: &lt;CODE&gt;SPRO&lt;/CODE&gt; or &lt;CODE&gt;/ASADEV/68000202&lt;/CODE&gt;&lt;BR /&gt;
&lt;BR /&gt;
Select the created connection and go to Outbound Objects.&lt;BR /&gt;
&lt;BR /&gt;
Add a new entry specifying the object, extraction function module, message type, load type, and response function.&lt;BR /&gt;
&lt;H3 id="toc-hId-1354202512"&gt;2. Test Outbound Event Creation&lt;/H3&gt;&lt;BR /&gt;
In the example above, please pick any test sales order in transaction &lt;CODE&gt;/nVA02&lt;/CODE&gt; and force a change event, e.g., by changing the requested delivery date on header level.&lt;BR /&gt;
&lt;H3 id="toc-hId-1157689007"&gt;3. Check monitor transaction for actual message and payload&lt;/H3&gt;&lt;BR /&gt;
Access to monitor application&lt;BR /&gt;
User must have PFCG role &lt;CODE&gt;/ASADEV/ACI_ADMIN_ROLE&lt;/CODE&gt; to access Add-On monitor.&lt;BR /&gt;
&lt;BR /&gt;
Use transaction &lt;CODE&gt;/n/ASADEV/ACI_MONITOR&lt;/CODE&gt; to start the monitor.&lt;BR /&gt;
You will see the entry screen with a selection form on top.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/11/confluent_blog10.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/11/confluent_blog12.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;STRONG&gt;Congrats, you are now able to send data out to Confluent.&lt;/STRONG&gt;&lt;BR /&gt;
&lt;BR /&gt;
In the next blog, we will create a custom payload for the event.</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/how-to-connect-sap-ecc-s-4hana-on-prem-and-private-cloud-with-confluent/ba-p/13575214"/>
    <published>2023-12-01T15:27:08+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/event-driven-architecture-simplifying-payload-creation-with-payload/ba-p/13577526</id>
    <title>Event-driven architecture: Simplifying Payload Creation with Payload Designer</title>
    <updated>2023-12-11T08:42:28+01:00</updated>
    <author>
      <name>Benedikt_Sprung</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/661488</uri>
    </author>
    <content>&lt;H2 id="toc-hId-964046278"&gt;&lt;STRONG&gt;Overview&lt;/STRONG&gt;&lt;/H2&gt;&lt;BR /&gt;
Configuring payloads for the SAP NetWeaver Add-On for Event enablement has become remarkably straightforward, all thanks to the Payload Designer.&lt;BR /&gt;
&lt;BR /&gt;
This powerful tool enables you to effortlessly add tables, define relationships with inner joins, left outer joins, rename tables and fields, all through simple configuration.&lt;BR /&gt;
&lt;BR /&gt;
In this blog post, we will guide you through the process of configuring your payload with just a few easy steps.&lt;BR /&gt;
&lt;BR /&gt;
If you you are new to SAP Enterprise Messaging in SAP ERP systems and the Integration Add-on, you can have a look at following Blog-Posts:&lt;BR /&gt;
&lt;UL&gt;&lt;BR /&gt;
 	&lt;LI&gt;&lt;A title="Event-driven architecture – now available for SAP ECC users" href="https://blogs.sap.com/?p=1132020" target="_blank" rel="noopener noreferrer"&gt;Event-driven architecture – now available for SAP ECC users&lt;/A&gt;&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;&lt;A title="SAP Enterprise Messaging for SAP ERP: HowTo-Guide (Part 1 - Connectivity)" href="https://blogs.sap.com/?p=1185933" target="_blank" rel="noopener noreferrer"&gt;SAP Enterprise Messaging for SAP ERP: HowTo-Guide (Part 1 - Connectivity)&lt;/A&gt;&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;&lt;A title="SAP Enterprise Messaging for SAP ERP: HowTo-Guide (Part 2 - First use case)" href="https://blogs.sap.com/?p=1179612" target="_blank" rel="noopener noreferrer"&gt;SAP Enterprise Messaging for SAP ERP: HowTo-Guide (Part 2 - First use case)&lt;/A&gt;&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;&lt;A title="Data Events scenario With SAP Event Enablement Add-on for SAP S/4HANA, SAP Event Mesh and SAP Cloud Integration: Step-by-Step Guide" href="https://blogs.sap.com/2022/03/04/data-events-scenario-with-sap-event-enablement-add-on-for-sap-s-4hana-sap-event-mesh-and-sap-cloud-integration-step-by-step-guide/" target="_blank" rel="noopener noreferrer"&gt;Data Events scenario With SAP Event Enablement Add-on for SAP S/4HANA, SAP Event Mesh and SAP Cloud Integration: Step-by-Step Guide&lt;/A&gt;&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;&lt;A href="https://blogs.sap.com/2021/08/13/emit-data-events-from-sap-s-4hana-or-sap-ecc-through-sap-netweaver-add-on-for-event-enablement/" target="_blank" rel="noopener noreferrer"&gt;Emit Data Events from SAP S/4HANA or SAP ECC through SAP NetWeaver Add-On for Event Enablement&lt;/A&gt;&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;&lt;A href="https://blogs.sap.com/2023/12/01/how-to-connect-sap-ecc-s-4hana-on-prem-and-private-cloud-with-confluent-cloud-or-confluent-platform-1/" target="_blank" rel="noopener noreferrer"&gt;How to connect SAP ECC + S/4HANA on-prem and private cloud with Confluent Cloud or Confluent Platform (1)&lt;/A&gt;&lt;/LI&gt;&lt;BR /&gt;
&lt;/UL&gt;&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;H2 id="toc-hId-767532773"&gt;&lt;STRONG&gt;System Prerequisite&lt;/STRONG&gt;&lt;/H2&gt;&lt;BR /&gt;
One of the Software components needs to be availabvle on your system:&lt;BR /&gt;
&lt;UL&gt;&lt;BR /&gt;
 	&lt;LI&gt;SAP NetWeaver Event-enablement Add-on (SAP Event Mesh Edition)&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;or, alternatively, ASAPIO Integration Add-on – Framework (full version)&lt;/LI&gt;&lt;BR /&gt;
&lt;/UL&gt;&lt;BR /&gt;
&lt;STRONG&gt;Licensing&lt;/STRONG&gt;&lt;BR /&gt;
&lt;BR /&gt;
All software above requires the purchase of appropriate licenses.&lt;BR /&gt;
&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;BR /&gt;
&amp;nbsp;&lt;BR /&gt;
&lt;H2 id="toc-hId-571019268"&gt;&lt;STRONG&gt;Creating Custom Payloads with Payload Designer&lt;/STRONG&gt;&lt;/H2&gt;&lt;BR /&gt;
&lt;H3 id="toc-hId-503588482"&gt;&lt;STRONG&gt;Step 1:&lt;/STRONG&gt; &lt;STRONG&gt;Creating Custom Payloads with Payload Designer&lt;/STRONG&gt;&lt;/H3&gt;&lt;BR /&gt;
1. Navigate to transaction /n/ASADEV/DESIGN.&lt;BR /&gt;
&lt;BR /&gt;
Click the "Create Payload Designer" button on the main screen and fill in the necessary fields. This action creates the initial version of the payload and takes you to the main screen.&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/11/1-41.png" height="365" width="468" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;P class="image_caption" style="text-align: center;font-style: italic"&gt;Creating a Payload Designer version in transaction /n/ASADEV/DESIGN&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;H3 id="toc-hId-307074977"&gt;&lt;/H3&gt;&lt;BR /&gt;
2. Use the join builder to establish table joins.&lt;BR /&gt;
&lt;UL&gt;&lt;BR /&gt;
 	&lt;LI&gt;Insert new tables or custom views.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Adjust table joins through field connections.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Return to the main screen.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Note: Parent relationships in the Table section and key fields in the Field section are automatically determined based on hierarchical sorting.&lt;/LI&gt;&lt;BR /&gt;
&lt;/UL&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/10/PD__join.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;H3 id="toc-hId-110561472"&gt;&lt;/H3&gt;&lt;BR /&gt;
3. Add additional payload fields from the tables:&lt;BR /&gt;
&lt;UL&gt;&lt;BR /&gt;
 	&lt;LI&gt;Double click on the preferred table.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Select one or multiple fields.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Fields can be reordered using sequence numbers.&lt;/LI&gt;&lt;BR /&gt;
&lt;/UL&gt;&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/10/2-85.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;H3 id="toc-hId--85952033"&gt;&lt;/H3&gt;&lt;BR /&gt;
&lt;H3 id="toc-hId--282465538"&gt;&lt;STRONG&gt;Step 2: Outbound Configuration Using Payload Designer&lt;/STRONG&gt;&lt;/H3&gt;&lt;BR /&gt;
To configure outbound objects using Payload Designer, follow these steps:&lt;BR /&gt;
&lt;OL&gt;&lt;BR /&gt;
 	&lt;LI&gt;Access transaction SPRO.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Navigate to IMG &amp;gt; Cloud Integrator – Connection and Replication Object Customizing or directly to transaction: /ASADEV/68000202.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Select the created connection.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Go to the "Outbound Objects" section.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Add a new entry and specify the following:&lt;BR /&gt;
&lt;UL&gt;&lt;BR /&gt;
 	&lt;LI&gt;Object: Name of the outbound configuration.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Extraction Func. Module: /ASADEV/ACI_GEN_PDVIEW_EXTRACT.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Load Type: Incremental Load.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Trace: Activate for testing purposes.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Formatting Func.: /ASADEV/ACI_GEN_VIEW_FORM_CB.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Field Payload View Name: Payload Name.&lt;/LI&gt;&lt;BR /&gt;
 	&lt;LI&gt;Field Payload View Version: Payload Version.&lt;/LI&gt;&lt;BR /&gt;
&lt;/UL&gt;&lt;BR /&gt;
&lt;/LI&gt;&lt;BR /&gt;
&lt;/OL&gt;&lt;BR /&gt;
&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/11/3-12.png" height="396" width="629" /&gt;&lt;BR /&gt;
&lt;H3 id="toc-hId--478979043"&gt;&lt;/H3&gt;&lt;BR /&gt;
&lt;H3 id="toc-hId--675492548"&gt;&lt;STRONG&gt;Step 3: &lt;/STRONG&gt;See your Payload in the ACI_Monitor&lt;/H3&gt;&lt;BR /&gt;
Navigate to transaction /n/ASADEV/ACI_MONITOR&lt;BR /&gt;
&lt;P style="overflow: hidden;margin-bottom: 0px"&gt;&lt;IMG class="migrated-image" src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/10/ACI_Monitor.png" /&gt;&lt;/P&gt;&lt;BR /&gt;
&lt;BR /&gt;
&lt;H3 id="toc-hId--947237422"&gt;&lt;/H3&gt;&lt;BR /&gt;
&lt;H2 id="toc-hId--850347920"&gt;&lt;STRONG&gt;Conclusion&lt;/STRONG&gt;&lt;/H2&gt;&lt;BR /&gt;
Payload Designer simplifies SAP interface configuration by providing a user-friendly, code-free approach to defining payloads. With its intuitive interface and powerful features, it enables organizations to streamline their data integration processes and improve efficiency in managing payloads for event messages sent to the SAP Event Mesh.&lt;BR /&gt;
&lt;BR /&gt;
&amp;nbsp;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/event-driven-architecture-simplifying-payload-creation-with-payload/ba-p/13577526"/>
    <published>2023-12-11T08:42:28+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/sap-hana-cloud-data-lake-files-%E3%81%B8%E3%81%AE%E6%9C%80%E5%88%9D%E3%81%AE%E3%82%A2%E3%82%AF%E3%82%BB%E3%82%B9%E8%A8%AD%E5%AE%9A/ba-p/13574382</id>
    <title>SAP HANA Cloud, data lake Files への最初のアクセス設定</title>
    <updated>2023-12-19T07:37:51+01:00</updated>
    <author>
      <name>Sawa_Ito</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/7449</uri>
    </author>
    <content>&lt;P&gt;このブログは、2022 年 11 月 15 日に SAP ジャパン公式ブログに掲載されたものを SAP ジャパン公式ブログ閉鎖に伴い転載したものです。&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;HR /&gt;&lt;P&gt;&lt;BR /&gt;&lt;BR /&gt;このブログは、&lt;SPAN class=""&gt;jason.hinsperger&lt;/SPAN&gt; が執筆したブログ「&lt;A href="https://blogs.sap.com/2021/08/05/setting-up-initial-access-to-hana-cloud-data-lake-files/" target="_blank" rel="noopener noreferrer"&gt;&lt;STRONG&gt;Setting Up Initial Access to HANA Cloud data lake Files&lt;/STRONG&gt;&lt;/A&gt;（2021 年 8 月 5 日）の抄訳です。最新の情報は、&lt;A href="https://blogs.sap.com/tags/7efde293-f35d-4737-b40f-756b6a798216/" target="_blank" rel="noopener noreferrer"&gt;SAP Community の最新ブログ&lt;/A&gt;や&lt;A href="https://help.sap.com/docs/SAP_HANA_DATA_LAKE?locale=en-US" target="_blank" rel="noopener noreferrer"&gt;マニュアル&lt;/A&gt;を参照してください。&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;HR /&gt;&lt;P&gt;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;SAP HANA Cloud, data lake は、フォーマットのあらゆるタイプのデータのネイティブ形式でのストレージをサポートしています。&lt;BR /&gt;&lt;BR /&gt;マネージドファイルストレージは、外部のハイパースケーラーアカウントでストレージを設定することなく、あらゆるタイプのファイルをセキュアに格納するストレージを提供します。&lt;BR /&gt;&lt;BR /&gt;これは、高速 SQL 分析を行う目的で SAP HANA Cloud, data lake にデータを高速投入する必要がある場合や、何等かの目的でデータを extract する場合にとても便利です。&lt;BR /&gt;&lt;BR /&gt;SAP HANA Cloud, data lake Files への初回のアクセス設定は、特にデータベースのバックグラウンドを持ち、オブジェクトストレージや REST API に詳しくない場合には少し難しいプロセスかもしれません。&lt;BR /&gt;&lt;BR /&gt;以下は、私が SAP HANA Cloud, data lake files をテストするのに使用したプロセスです。&lt;BR /&gt;&lt;BR /&gt;SAP HANA Cloud, data lake files はユーザーセキュリティーやアクセスを認証経由で管理するため、ユーザーアクセスの設定には署名付きの証明書の生成が必要です。&lt;BR /&gt;&lt;BR /&gt;認証局へのアクセスがない場合には、OpenSSL を利用する以下のプロセスを使用してCAと署名付きのクライアント証明書を作成してSAP HANA Cloud, data lake files 設定を更新することができます。&lt;BR /&gt;私はこれまで何度もこれでテストしたことがあるので読者の方でも同様に行えるでしょう。&lt;BR /&gt;&lt;BR /&gt;最初に、CA バンドルを作成してアップロードする必要があります。&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;以下の OpenSSL コマンドを使用して CA を生成できます。&lt;BR /&gt;&lt;BR /&gt;openssl genrsa -out ca.key 2048&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;次に、CA の公開証明書（この場合は 200 日間有効）を作成します。共通名を最低限入力し、他のフィールドを必要に応じて入力します。&lt;BR /&gt;&lt;BR /&gt;openssl req -x509 -new -key ca.key -days 200 -out ca.crt&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;クライアント証明書の署名リクエストを作成する必要があります。共通名を最低限提供し、他のフィールドを必要に応じて入力します。&lt;BR /&gt;&lt;BR /&gt;openssl req -new -nodes -newkey rsa:2048 -out client.csr -keyout client.key&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;最後に、クライアント証明書を作成します（この場合は 100 日有効）。&lt;BR /&gt;&lt;BR /&gt;openssl x509 -days 100 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt&lt;/P&gt;&lt;BLOCKQUOTE&gt;&lt;STRONG&gt;*&lt;/STRONG&gt;&lt;STRONG&gt;&lt;EM&gt;備考&lt;/EM&gt;&lt;/STRONG&gt;&lt;EM&gt; – CA とクライアント証明書のフィールドがすべて全く同じにならないようにしてください。さもないと、自己署名証明書とみなされ、以下の証明書の認証が失敗します。&lt;/EM&gt;&lt;/BLOCKQUOTE&gt;&lt;P&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;証明書が任意の CA によって署名されたことを認証するには（SAP HANA Cloud, data lake に CA 証明書をアップロードしたときにクライアント証明書を認証するために使用できるとわかるように）&lt;BR /&gt;&lt;BR /&gt;openssl verify -CAfile ca.crt client.crt&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;次に、SAP HANA Cloud Central でインスタンスを開き、「Manage File Container」を選択し、SAP HANA Cloud, data lake files ユーザーを設定します。&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;IMG src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2021/08/HDLFiles_EditConfig.jpg" border="0" /&gt;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;設定を編集し、「Trusts」セクションの「Add」を選択します。前に生成した ca.crt をコピーまたはアップロードし、「Apply」をクリックします。すぐには「Manage File Container」スクリーンはクローズしないでください。&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;IMG src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2021/08/HDLFiles_AddTrust.jpg" border="0" /&gt;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;これで、管理されたファイルストレージにアクセスできるようにユーザーを構成できるようになりました。&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;「Authorizations」セクションをスクロールダウンして、「Add」を選択します。新しい入力欄が表示されます。&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;IMG src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2021/08/HDLFiles_AddUser-1.jpg" border="0" /&gt;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;ユーザーのロールをドロップダウンリストから選択します（デフォルトでは admin とユーザーロールがあります）。&lt;BR /&gt;&lt;BR /&gt;ここからが少し難しいところです。&lt;BR /&gt;&lt;BR /&gt;リクエストしたときに、ストレージゲートウェイ（SAP HANA Cloud, data lake files へのエントリーポイント）がどのユーザーに対して認証するのか決定できるようクライアント証明書からパターン文字列を追加する必要があります。&lt;BR /&gt;&lt;BR /&gt;パターン文字列を生成するにあたり、2 つのオプションがあります。&lt;BR /&gt;以下の OpenSSL コマンドを使用して、パターン文字列を生成することができます(アウトプットに表示される 「subject= 」　プレフィックスは省略します) 。&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;openssl x509 -in client.crt -in client.crt -nameopt RFC2253 -subject -noout&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;あるいは、スクリーンにある「generate pattern」オプションを使用することもできます。&lt;BR /&gt;これは、ダイアログボックスを開き、クライアント証明書をアップロード/貼り付けて、自動でパターンを生成します。&lt;BR /&gt;証明書は保存せず、パターン文字列だけを保存することに注意してください。&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;IMG src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2021/08/HDLFiles_GeneratePattern.jpg" border="0" /&gt;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;「Apply」をクリックして、権限入力欄にパターン文字列を追加します。&lt;/P&gt;&lt;BLOCKQUOTE&gt;パターン文字列は、ワイルドカードも使用可能なため、特定のロールの証明書のクラスを認証できることに注意してください。証明書のパターンが複数の認証と一致する場合、使用する認証は、特定の認証エントリーに設定された「Rank」値セットによって制御されます。&lt;/BLOCKQUOTE&gt;&lt;P&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;これで、REST api 経由で SAP HANA Cloud, data lake files にアクセスして使用することができます。&lt;BR /&gt;&lt;BR /&gt;私のテストではうまくいったcurl コマンドのサンプルがあります。接続が成功しているかどうかvalicate できます。(インスタンス ID とファイル REST API エンドポイントは HANA Cloud Centralのインスタンス詳細からコピーすることができます)。&lt;BR /&gt;&lt;BR /&gt;上記で生成して認証の作成に使用したクライアント証明書とキーを使用してください。&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;/P&gt;&lt;BLOCKQUOTE&gt;&lt;EM&gt;curl &lt;/EM&gt;&lt;EM&gt;は少し tricky なことに注意してください。Windows で試していましたが、Windows 10 バージョン用の curl を動作させることができませんでした。最終的に新しい curl version (7.75.0) をダウンロードしたところ、機能しましたが、Windows で curl から証明書ストアへどうアクセスするのかわからなかったため、SAP HANA Cloud サーバー証明書の認証をスキップするために –insecure’ オプションを使用しなければなりませんでした。&lt;/EM&gt;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;/BLOCKQUOTE&gt;&lt;P&gt;&lt;BR /&gt;curl --insecure -H "x-sap-filecontainer:&amp;nbsp;&lt;EM&gt;&amp;lt;instance_id&amp;gt;&lt;/EM&gt;" --cert ./client.crt --key ./client.key "https://&lt;EM&gt;&amp;lt;Files REST API endpoint&amp;gt;&lt;/EM&gt;/webhdfs/v1/?op=LISTSTATUS" -X GET&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;上記のコマンドは、以下を返します (空のSAP HANA Cloud, data lake)。&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;{"FileStatuses":{"FileStatus":[]}}&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;これで、SAP HANA Cloud, data lake files を使用して、あらゆるタイプのファイルを SAP HANA Cloud に格納するための設定は終了です。&lt;BR /&gt;&lt;BR /&gt;ファイルの管理でサポートされている REST API と引数のフルセットについては、&lt;A href="https://help.sap.com/doc/9d084a41830f46d6904fd4c23cd4bbfa/QRC_2_2021/en-US/html/index.html" target="_blank" rel="noopener noreferrer"&gt;マニュアル&lt;/A&gt;を参照ください。&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;HR /&gt;&lt;P&gt;&lt;BR /&gt;&lt;BR /&gt;オリジナルのブログはここまでです。&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;HR /&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;IMG src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/12/data_pyramid_1-2.jpg" border="0" /&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;IMG src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/12/data_pyramid_2-2.jpg" border="0" /&gt;&lt;/P&gt;&lt;P&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;HR /&gt;&lt;P&gt;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-hana-cloud-data-lake-files-%E3%81%B8%E3%81%AE%E6%9C%80%E5%88%9D%E3%81%AE%E3%82%A2%E3%82%AF%E3%82%BB%E3%82%B9%E8%A8%AD%E5%AE%9A/ba-p/13574382"/>
    <published>2023-12-19T07:37:51+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/sap-hana-cloud-hana-%E3%83%87%E3%83%BC%E3%82%BF%E3%83%99%E3%83%BC%E3%82%B9%E3%81%8B%E3%82%89-sap-hana-cloud-data-lake/ba-p/13574477</id>
    <title>SAP HANA Cloud, HANA データベースから SAP HANA Cloud, data lake リレーショナルエンジンへの最速のデータ移動方法と移動速度テスト結果</title>
    <updated>2023-12-19T07:47:55+01:00</updated>
    <author>
      <name>Sawa_Ito</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/7449</uri>
    </author>
    <content>&lt;P&gt;このブログは、2022 年 11 月 17 日に SAP ジャパン公式ブログに掲載されたものを SAP ジャパン公式ブログ閉鎖に伴い転載したものです。&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;HR /&gt;&lt;P&gt;&lt;BR /&gt;&lt;BR /&gt;このブログは、&lt;SPAN class=""&gt;douglas.hoover&lt;/SPAN&gt;&amp;nbsp;が執筆したブログ「&lt;A href="https://blogs.sap.com/2022/03/08/the-fastest-way-to-load-data-from-hana-cloud-hana-into-hana-cloud-hana-data-lake/" target="_blank" rel="noopener noreferrer"&gt;&lt;STRONG&gt;The fastest way to load data from HANA Cloud, HANA into HANA Cloud, HANA Data Lake&lt;/STRONG&gt;&lt;/A&gt;（2022 年 3 月 8 日）の抄訳です。オリジナルのブログページでのコメントのやりとりなどもぜひご参照ください。&lt;BR /&gt;&lt;BR /&gt;最新の情報は、&lt;A href="https://blogs.sap.com/tags/7efde293-f35d-4737-b40f-756b6a798216/" target="_blank" rel="noopener noreferrer"&gt;SAP Community の最新ブログ&lt;/A&gt;や&lt;A href="https://help.sap.com/docs/SAP_HANA_DATA_LAKE?locale=en-US" target="_blank" rel="noopener noreferrer"&gt;マニュアル&lt;/A&gt;を参照してください。&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;HR /&gt;&lt;P&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;H2 id="toc-hId-963956100"&gt;&amp;nbsp;&lt;/H2&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-767442595"&gt;&amp;nbsp;&lt;/H2&gt;&lt;P&gt;&lt;BR /&gt;このブログは、SAP HANA データ戦略ブログシリーズの1つです。&lt;BR /&gt;&lt;A href="https://blogs.sap.com/2019/10/14/sap-hana-data-strategy/" target="_blank" rel="noopener noreferrer"&gt;https://blogs.sap.com/2019/10/14/sap-hana-data-strategy/&lt;/A&gt;&lt;/P&gt;&lt;H1 id="toc-hId-441846371"&gt;&amp;nbsp;&lt;/H1&gt;&lt;P&gt;&lt;BR /&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="toc-hId-245332866"&gt;&amp;nbsp;&lt;/H1&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="toc-hId-48819361"&gt;概要&lt;/H1&gt;&lt;P&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;SAP HANA Cloud の HANA データベースから SAP HANA Cloud, data lake リレーショナルエンジンにより大きなテーブルを移動するお客様が増えるにつれ、SAP HANA Cloud の HANA データベースから SAP HANA Cloud, data lake リレーショナルエンジンへのデータ移動の最速の方法について聞かれるようになりました。&lt;BR /&gt;&lt;BR /&gt;より正確にいうと、SAP HANA Cloud, data lake リレーショナルエンジン仮想テーブルに対してシンプルに HANA INSERT を実行するよりも高速な方法があるのか聞かれるようになりました。&lt;BR /&gt;&lt;BR /&gt;なぜ SAP HANA Cloud の HANA データベースから SAP HANA Cloud, data lake リレーショナルエンジンに大きなテーブルを移動するお客様がいるのか疑問に思う方もいるかもしれません。&lt;BR /&gt;&lt;BR /&gt;最もよくある利用ケースは、大きなデータベースの最初のマテリアライズあるいは古いデータをSAP HANA Cloud, data lake リレーショナルエンジンにアーカイブするためです。&lt;BR /&gt;&lt;BR /&gt;これらのお客様の大半は、通常 SAP HANA Smart Data Integration (SDI) を使用してこのマテリアライゼーションを行っており、これらのテーブルを最新の状態にキープするために SDI の Flowgraphs や SDI のリアルタイムレプリケーションを使用した &lt;STRONG&gt;Change Data Capture &lt;/STRONG&gt;を同じインターフェースを使用して行っています。&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;SAP HANA SDI &lt;/STRONG&gt;&lt;STRONG&gt;の詳細については以下のブログを参照してください&lt;/STRONG&gt;&lt;STRONG&gt;:&lt;/STRONG&gt;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;SAP HANA&amp;nbsp;&lt;/STRONG&gt;&lt;STRONG&gt;データ戦略&lt;/STRONG&gt;&lt;STRONG&gt;:&amp;nbsp;&lt;/STRONG&gt;&lt;STRONG&gt;リアルタイム &lt;/STRONG&gt;&lt;STRONG&gt;Change Data Capture&amp;nbsp;&lt;/STRONG&gt;&lt;STRONG&gt;を含む高速データ投入（英語）&lt;/STRONG&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;A href="https://blogs.sap.com/2020/06/18/hana-data-strategy-data-ingestion-including-real-time-change-data-capture/" target="test_blank" rel="noopener noreferrer"&gt;https://blogs.sap.com/2020/06/18/hana-data-strategy-data-ingestion-including-real-time-change-data-capture/&lt;/A&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;SAP HANA&amp;nbsp;&lt;/STRONG&gt;&lt;STRONG&gt;データ戦略&lt;/STRONG&gt;&lt;STRONG&gt;:&amp;nbsp;&lt;/STRONG&gt;&lt;STRONG&gt;高速データ投入&lt;/STRONG&gt;&lt;STRONG&gt;&amp;nbsp;–&amp;nbsp;&lt;/STRONG&gt;&lt;STRONG&gt;仮想化（英語）&lt;/STRONG&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;A href="https://blogs.sap.com/2020/03/09/hana-data-strategy-data-ingestion-virtualization/" target="test_blank" rel="noopener noreferrer"&gt;https://blogs.sap.com/2020/03/09/hana-data-strategy-data-ingestion-virtualization/&lt;/A&gt;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;ここで実験するデータ移動に関するシンプルな方法は以下の 3 種です:&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;UL&gt;&lt;UL&gt;&lt;LI&gt;シンプルに SAP HANA Cloud, data lake リレーショナルエンジン仮想テーブルへの HANA INSERT&lt;/LI&gt;&lt;/UL&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;UL&gt;&lt;UL&gt;&lt;LI&gt;HANA 仮想テーブルにアクセスし、シンプルに SAP HANA Cloud, data lake リレーショナルエンジンから data lake INSERT&lt;/LI&gt;&lt;/UL&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;UL&gt;&lt;UL&gt;&lt;LI&gt;HANA エクスポートと data lake LOAD&lt;/LI&gt;&lt;/UL&gt;&lt;/UL&gt;&lt;P&gt;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;こう質問する人もいるかもいれません：&lt;BR /&gt;&lt;BR /&gt;「なぜ SAP HANA Cloud, HANA データベース経由で行うのか?」&lt;BR /&gt;&lt;BR /&gt;「なぜ SAP HANA Cloud, data lake リレーショナルエンジンに直接データをロードしないのか?」&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;繰り返しますが、これらのお客様はターゲットとして HANAオブジェクト（ローカルまたは仮想）を必要とする HANA Enterprise Information Management （EIM）ツールを使用しています。&lt;BR /&gt;&lt;BR /&gt;将来のブログでは、SAP IQ クライアントサイドロード、Data Services、Data Intelligence 経由の SAP HANA Cloud, data lake リレーショナルエンジンへの直接のデータロードについて説明したいと思います。&lt;BR /&gt;&lt;BR /&gt;SAP HANA Cloud, HANA データベースから SAP HANA Cloud, data lake リレーショナルエンジンへの最速のデータロード方法は、SAP HANA Cloud, data lake リレーショナルエンジンから、SAP HANA Cloud, HANA データベースの物理テーブルを指定するプロキシテーブルを作成するために「create existing local temporary table」を使用して HANA&amp;nbsp; テーブルから SELECT で INSERT 文を実行する方法です。&lt;BR /&gt;（詳細は以下のテーブル参照）&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;/P&gt;&lt;TABLE&gt;&lt;TBODY&gt;&lt;TR&gt;&lt;TD width="156"&gt;&lt;STRONG&gt;方法&lt;/STRONG&gt;&lt;/TD&gt;&lt;TD width="156"&gt;&lt;STRONG&gt;行&lt;/STRONG&gt;&lt;/TD&gt;&lt;TD width="156"&gt;&lt;STRONG&gt;データサイズ&lt;/STRONG&gt;&lt;/TD&gt;&lt;TD width="156"&gt;&lt;STRONG&gt;時間 （秒）&lt;/STRONG&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="156"&gt;HANA Cloud, data lake/IQ INSERT..SELECT&lt;/TD&gt;&lt;TD width="156"&gt;28,565,809&lt;/TD&gt;&lt;TD width="156"&gt;3.3 GB&lt;/TD&gt;&lt;TD width="156"&gt;&lt;STRONG&gt;52.86&lt;/STRONG&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="156"&gt;*HANA Cloud, data lake/IQ&lt;BR /&gt;LOAD&lt;BR /&gt;Azure ファイルシステム&lt;/TD&gt;&lt;TD width="156"&gt;28,565,809&lt;/TD&gt;&lt;TD width="156"&gt;3.3 GB&lt;/TD&gt;&lt;TD width="156"&gt;116 (1分56秒)&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="156"&gt;*HANA Cloud, data lake/IQ&lt;BR /&gt;LOAD&lt;BR /&gt;Data Lake ファイルシステム&lt;/TD&gt;&lt;TD width="156"&gt;28,565,809&lt;/TD&gt;&lt;TD width="156"&gt;3.3 GB&lt;/TD&gt;&lt;TD width="156"&gt;510 (8分30秒)&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD width="156"&gt;HANA INSERT..SELECT&lt;/TD&gt;&lt;TD width="156"&gt;28,565,809&lt;/TD&gt;&lt;TD width="156"&gt;3.3 GB&lt;/TD&gt;&lt;TD width="156"&gt;1277 (21分7秒)&lt;/TD&gt;&lt;/TR&gt;&lt;/TBODY&gt;&lt;/TABLE&gt;&lt;P&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;* HANA データベースからファイルシステムへのデータエクスポート時間は含めていません。&lt;BR /&gt;&lt;BR /&gt;28,565,809 行、約 3.3 GB の TPC-D ORDERS テーブルを使用し、SAP HANA Cloud, data lake リレーショナルエンジンの小さめの設定でロードしています。 &lt;STRONG&gt;&amp;nbsp;&lt;/STRONG&gt;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId--18611425"&gt;&amp;nbsp;&lt;/H2&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId--215124930"&gt;以下の SAP HANA Cloud 設定を使用してテストしました&lt;/H2&gt;&lt;P&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;SAP HANA Cloud, HANA データベース：60 GB / 200 GB、4 vCPU&lt;BR /&gt;&lt;BR /&gt;SAP HANA Cloud, data lake リレーショナルエンジン：16 TB、ワーカー 8 vCPU /コーディネーター 8 vCPU&lt;BR /&gt;&lt;BR /&gt;SAP HANA Cloud, data lake リレーショナルエンジンでは、より多くの並列処理を実行するには（特により大きなテーブルの場合）より多くの vCPU 数を設定します。&lt;BR /&gt;&lt;BR /&gt;より多くの TB を SAP HANA Cloud, data lake リレーショナルエンジンに追加することで、より大きなディスク I/O スループットを得ることができます。&lt;/P&gt;&lt;H1 id="toc-hId--540721154"&gt;&amp;nbsp;&lt;/H1&gt;&lt;P&gt;&lt;BR /&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="toc-hId--737234659"&gt;テストで使用した詳細設定と構文&lt;/H1&gt;&lt;P&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;SAP HANA Cockpit をスタートして SAP HANA Cloud を管理します。&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;IMG src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2022/03/HANACockpitManager.png" border="0" /&gt;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;SAP HANA Cloud, data lake リレーショナルエンジンから「Open in SAP HANA Database Explorer」を選択します。&lt;BR /&gt;&lt;BR /&gt;もしこれが初回であれば、SAP HANA Cloud, data lake リレーショナルエンジンの ADMIN パスワードを求められます。&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;IMG src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2022/03/HANACockpitManager2.png" border="0" /&gt;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;SQL コマンドを入力し、クリックして実行します。&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;IMG src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2022/03/HANADBExplorer.png" border="0" /&gt;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId--804665445"&gt;&amp;nbsp;&lt;/H2&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId--653924593"&gt;以下を作成するための SAP HANA Cloud, data lake コマンド&lt;/H2&gt;&lt;P&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;OL&gt;&lt;OL&gt;&lt;LI&gt;SAP HANA Cloud, data lake リレーショナルエンジンから、SAP HANA Cloud HANA データベースへ接続しているサーバー&lt;/LI&gt;&lt;/OL&gt;&lt;/OL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;OL&gt;&lt;OL&gt;&lt;LI&gt;データをロードして作成するためのローカルの SAP HANA Cloud, data lake リレーショナルエンジンテーブル&lt;/LI&gt;&lt;/OL&gt;&lt;/OL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;OL&gt;&lt;OL&gt;&lt;LI&gt;SAP HANA Cloud インスタンスのテーブルを指定するローカルのテンポラリープロキシーテーブル&lt;/LI&gt;&lt;/OL&gt;&lt;/OL&gt;&lt;P&gt;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;CREATE SERVER&lt;/STRONG&gt;&lt;BR /&gt;&lt;BR /&gt;–DROP SERVER DRHHC2_HDB&lt;BR /&gt;&lt;BR /&gt;CREATE SERVER DRHHC2_HDB CLASS ‘HANAODBC’ USING ‘Driver=libodbcHDB.so;ConnectTimeout=60000;ServerNode=xyxy.hana.prod-us10.hanacloud.ondemand.com:443;ENCRYPT=TRUE;ssltruststore=xyxy.hana.prod-us10.hanacloud.ondemand.com;ssltrustcert=Yes;UID=DBADMIN;PWD=xyxyx;’&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;CREATE TARGET TABLE&lt;/STRONG&gt;&lt;BR /&gt;&lt;BR /&gt;CREATE&amp;nbsp; TABLE REGIONPULL (&lt;BR /&gt;&lt;BR /&gt;R_REGIONKEY&amp;nbsp;&amp;nbsp; bigint&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;R_NAME&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; varchar(25)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;R_COMMENT&amp;nbsp;&amp;nbsp;&amp;nbsp; varchar(152)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;primary key (R_REGIONKEY)&lt;BR /&gt;&lt;BR /&gt;);&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;CREATE local temporary PROXY&lt;/STRONG&gt;&lt;BR /&gt;&lt;BR /&gt;create existing local temporary table REGION_PROXY (&lt;BR /&gt;&lt;BR /&gt;R_REGIONKEY&amp;nbsp;&amp;nbsp; bigint&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;R_NAME&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; varchar(25)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;R_COMMENT&amp;nbsp;&amp;nbsp;&amp;nbsp; varchar(152)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;primary key (R_REGIONKEY)&lt;BR /&gt;&lt;BR /&gt;)&lt;BR /&gt;&lt;BR /&gt;at ‘DRHHC2_HDB..TPCD.REGION’;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;INSERT DATA&lt;/STRONG&gt;&lt;BR /&gt;&lt;BR /&gt;INSERT into REGIONPULL SELECT * from REGION_PROXY;&lt;BR /&gt;&lt;BR /&gt;Commit;&lt;BR /&gt;&lt;BR /&gt;–1.9s&lt;/P&gt;&lt;H2 id="toc-hId--850438098"&gt;&amp;nbsp;&lt;/H2&gt;&lt;P&gt;&lt;BR /&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId--1046951603"&gt;ORDERS テーブルテストコマンド&lt;/H2&gt;&lt;P&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;–DROP TABLE ORDERSPULL;&lt;BR /&gt;&lt;BR /&gt;create table ORDERSPULL (&lt;BR /&gt;&lt;BR /&gt;O_ORDERKEY&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; BIGINT&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;O_CUSTKEY&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; BIGINT&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;O_ORDERSTATUS&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; VARCHAR(1)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;O_TOTALPRICE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; DECIMAL(12,2)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;O_ORDERDATE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; DATE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;O_ORDERPRIORITY&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; VARCHAR(15)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;O_CLERK&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; VARCHAR(15)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;O_SHIPPRIORITY&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; INTEGER&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;O_COMMENT&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; VARCHAR(79) &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;not null,&lt;BR /&gt;&lt;BR /&gt;primary key (O_ORDERKEY)&lt;BR /&gt;&lt;BR /&gt;);&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;create existing local temporary table ORDERS_PROXY (&lt;BR /&gt;&lt;BR /&gt;O_ORDERKEY&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; BIGINT&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;O_CUSTKEY&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; BIGINT&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;O_ORDERSTATUS&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; VARCHAR(1)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;not null,&lt;BR /&gt;&lt;BR /&gt;O_TOTALPRICE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; DECIMAL(12,2)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;O_ORDERDATE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; DATE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;O_ORDERPRIORITY&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; VARCHAR(15)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;O_CLERK&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; VARCHAR(15)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;O_SHIPPRIORITY&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; INTEGER&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;O_COMMENT&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; VARCHAR(79)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null&lt;BR /&gt;&lt;BR /&gt;)&lt;BR /&gt;&lt;BR /&gt;at ‘DRHHC2_HDB..TPCD.ORDERS’;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;INSERT into ORDERSPULL SELECT * from ORDERS_PROXY;&lt;BR /&gt;&lt;BR /&gt;Commit;&lt;BR /&gt;&lt;BR /&gt;–59s&lt;BR /&gt;&lt;BR /&gt;–52.86 s&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;SELECT COUNT(*) FROM ORDERSPULL;&lt;BR /&gt;&lt;BR /&gt;–28,565,809&lt;/P&gt;&lt;H2 id="toc-hId--1243465108"&gt;&amp;nbsp;&lt;/H2&gt;&lt;P&gt;&lt;BR /&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId--1439978613"&gt;LINEITEM テーブルテストコマンド&lt;/H2&gt;&lt;P&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;create table LINEITEM (&lt;BR /&gt;&lt;BR /&gt;L_ORDERKEY&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; BIGINT&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_PARTKEY&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; BIGINT&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_SUPPKEY&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; BIGINT&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_LINENUMBER&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; INTEGER&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;not null,&lt;BR /&gt;&lt;BR /&gt;L_QUANTITY&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; DECIMAL(12,2)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_EXTENDEDPRICE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; DECIMAL(12,2)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_DISCOUNT&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; DECIMAL(12,2)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_TAX&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; DECIMAL(12,2)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_RETURNFLAG&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;VARCHAR(1)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_LINESTATUS&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; VARCHAR(1)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_SHIPDATE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; DATE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_COMMITDATE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; DATE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_RECEIPTDATE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; DATE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_SHIPINSTRUCT&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; VARCHAR(25)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_SHIPMODE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; VARCHAR(10)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_COMMENT&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; VARCHAR(44)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;primary key (L_ORDERKEY,L_LINENUMBER)&lt;BR /&gt;&lt;BR /&gt;);&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;create existing local temporary table LINEITEM_PROXY (&lt;BR /&gt;&lt;BR /&gt;L_ORDERKEY&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; BIGINT&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_PARTKEY&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; BIGINT&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_SUPPKEY&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; BIGINT&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_LINENUMBER&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; INTEGER&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_QUANTITY&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; DECIMAL(12,2)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_EXTENDEDPRICE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; DECIMAL(12,2)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_DISCOUNT&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; DECIMAL(12,2)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_TAX&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; DECIMAL(12,2)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_RETURNFLAG&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; VARCHAR(1)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_LINESTATUS&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; VARCHAR(1)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_SHIPDATE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; DATE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_COMMITDATE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; DATE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_RECEIPTDATE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; DATE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_SHIPINSTRUCT&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; VARCHAR(25)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_SHIPMODE&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; VARCHAR(10)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null,&lt;BR /&gt;&lt;BR /&gt;L_COMMENT&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; VARCHAR(44)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; not null&lt;BR /&gt;&lt;BR /&gt;)&lt;BR /&gt;&lt;BR /&gt;at ‘DRHHC2_HDB..TPCD.LINEITEM’;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;INSERT into LINEITEM SELECT * from LINEITEM_PROXY;&lt;BR /&gt;&lt;BR /&gt;Commit;&lt;BR /&gt;&lt;BR /&gt;— Rows affected:&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 114,129,863&lt;BR /&gt;&lt;BR /&gt;— Client elapsed time: 4 m 52 s&lt;/P&gt;&lt;H1 id="toc-hId--1343089111"&gt;&amp;nbsp;&lt;/H1&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H1 id="toc-hId--1539602616"&gt;&lt;STRONG&gt;&lt;BR /&gt;まとめ&lt;/STRONG&gt;&lt;/H1&gt;&lt;P&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;SAP HANA Cloud, HANA データベースから SAP HANA Cloud, data lake リレーショナルエンジンへのデータの最速のロード方法は、SAP HANA Cloud, data lake リレーショナルエンジンから、SAP HANA Cloud, HANA データベースの物理テーブルを指定するプロキシテーブルを作成するために「create existing local temporary table」を使用してHANA テーブルから SELECT で INSERT 文を実行する方法です。&lt;BR /&gt;&lt;BR /&gt;これは、このブログで紹介しているコマンドを使用することで、とても容易に行うことができます。&lt;BR /&gt;&lt;BR /&gt;あるいは、これらのコマンドを生成するプロシージャーを作成すると、さらに容易になります。（下の Daniel のブログを参照してください。）&lt;/P&gt;&lt;H2 id="toc-hId--2029519128"&gt;&amp;nbsp;&lt;/H2&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-2068934663"&gt;&amp;nbsp;&lt;/H2&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-1872421158"&gt;&amp;nbsp;&lt;/H2&gt;&lt;P&gt;&lt;BR /&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-1844091344"&gt;以下も参考にしてください&lt;/H2&gt;&lt;P&gt;&lt;BR /&gt;Jason Hinsperger の「SAP HANA Cloud, data lakeへのデータロード」のブログでは、SAP HANA Cloud, data lake リレーショナルエンジンの vCPU 数やデータベースサイズを増やすとロードのパフォーマンスにどのような影響があるか説明しています。&lt;BR /&gt;&lt;BR /&gt;&lt;A href="https://blogs.sap.com/?p=1866471" target="_blank" rel="noopener noreferrer"&gt;https://blogs.sap.com/?p=1866471&lt;/A&gt;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;Daniel Utvich の「SAP HANA Cloud, HANA データベースから SAP HANA Cloud, data lake へのデータの高速移動」のブログでは、システムテーブル情報をベースにした SQL コードを生成するプロシージャーの例を紹介しています。&lt;BR /&gt;&lt;BR /&gt;&lt;A href="https://blogs.sap.com/?p=1867099" target="_blank" rel="noopener noreferrer"&gt;https://blogs.sap.com/?p=1867099&lt;/A&gt;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;/P&gt;&lt;H3 id="toc-hId-1354174832"&gt;&amp;nbsp;&lt;/H3&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H3 id="toc-hId-1157661327"&gt;&lt;STRONG&gt;SAP HANA データ戦略ブログインデックス&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;&lt;BR /&gt;&lt;A href="https://blogs.sap.com/2019/10/14/sap-hana-data-strategy/" target="_blank" rel="noopener noreferrer"&gt;SAP HANA Data Strategy&lt;/A&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;UL&gt;&lt;UL&gt;&lt;LI&gt;&lt;A href="https://blogs.sap.com/2019/11/14/sap-hana-data-strategy-hana-data-modeling-a-detailed-overview/" target="_blank" rel="noopener noreferrer"&gt;SAP HANA Data Strategy: HANA Data Modeling a Detailed&amp;nbsp;&lt;/A&gt;&lt;A href="https://blogs.sap.com/2019/11/14/sap-hana-data-strategy-hana-data-modeling-a-detailed-overview/" target="_blank" rel="noopener noreferrer"&gt;Overview&lt;/A&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;UL&gt;&lt;UL&gt;&lt;LI&gt;&lt;A href="https://blogs.sap.com/2020/06/18/hana-data-strategy-data-ingestion-including-real-time-change-data-capture/?update=updated" target="_blank" rel="noopener noreferrer"&gt;HANA Data Strategy: Data Ingestion including Real-Time Change Data Capture&lt;/A&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;UL&gt;&lt;UL&gt;&lt;LI&gt;&lt;A href="https://blogs.sap.com/2020/03/16/access-sap-erp-data-from-sap-hana-through-sdi-abap-adapter-2/" target="_blank" rel="noopener noreferrer"&gt;Access&amp;nbsp;SAP ERP data from SAP HANA through SDI ABAP&amp;nbsp;Adapter by&amp;nbsp;Maxime Simon&lt;/A&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;/UL&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;UL&gt;&lt;UL&gt;&lt;LI&gt;&lt;A href="https://blogs.sap.com/2020/03/09/hana-data-strategy-data-ingestion-virtualization/" target="_blank" rel="noopener noreferrer"&gt;HANA Data Strategy: Data Ingestion – Virtualization&lt;/A&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;UL&gt;&lt;UL&gt;&lt;LI&gt;&lt;A href="https://blogs.sap.com/2020/02/12/hana-data-strategy-hana-data-tiering/" target="_blank" rel="noopener noreferrer"&gt;HANA Data Strategy: HANA Data Tiering&lt;/A&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;UL&gt;&lt;UL&gt;&lt;LI&gt;&lt;A href="https://blogs.sap.com/2019/06/19/store-more-with-sps04/" target="_blank" rel="noopener noreferrer"&gt;Store More with&amp;nbsp;&lt;/A&gt;&lt;A href="https://blogs.sap.com/2019/06/19/store-more-with-sps04/" target="_blank" rel="noopener noreferrer"&gt;SPS04 – NSE BLOG&lt;/A&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;/UL&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;/UL&gt;&lt;P&gt;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;HR /&gt;&lt;P&gt;&lt;BR /&gt;&lt;BR /&gt;オリジナルのブログはここまでです。&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;HR /&gt;&lt;P&gt;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;IMG src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/12/data_pyramid_1-4.jpg" border="0" /&gt;&lt;BR /&gt;&lt;IMG src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/12/data_pyramid_2-4.jpg" border="0" /&gt;&lt;/P&gt;&lt;P&gt;&lt;BR /&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/sap-hana-cloud-hana-%E3%83%87%E3%83%BC%E3%82%BF%E3%83%99%E3%83%BC%E3%82%B9%E3%81%8B%E3%82%89-sap-hana-cloud-data-lake/ba-p/13574477"/>
    <published>2023-12-19T07:47:55+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/using-data-lake-and-sql-to-create-custom-reporting-models/ba-p/13579874</id>
    <title>Using Data lake and SQL to create custom reporting models</title>
    <updated>2023-12-20T10:29:19+01:00</updated>
    <author>
      <name>Roney_Mathew</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/147426</uri>
    </author>
    <content>&lt;P&gt;&lt;STRONG&gt;Overview:&lt;/STRONG&gt; Through a series of blogs, would like to share scripts that utilize data lakes built for SAP tables, to create reporting models that represent certain sections of SAP screens/transactions or areas of analysis. Hopefully, these scripts serve as an accelerator to cater multiple use cases.For this first script we'll look at building User Status using JCDS and JEST.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;U&gt;Background:&lt;/U&gt;&lt;/STRONG&gt; &amp;nbsp;Most structured reporting tools (eg:BW) or ETL processes don’t bring in all fields available in source systems, these are deployed using a predefined datamodel (dimensions/measures) that collects fields from different tables and &amp;nbsp;limit what’s initially available for reporting, restricting the ability of Analysts to explore additional fields.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Eg: Financial reporting models built using ACDOCA or BSEG or FAGLFLEXA tables- Irrespective of the approach(CDS views or BW models), these don’t bring all fields from the source as they mostly focus on meeting initial requirements from primary stakeholders.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;Additional fields maybe available in SAP transaction systems and to make them available for reporting, multiple cycles of enhancements are implemented, reflecting a dependency on different support teams and time involved to meet these requirements.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;U&gt;Solution &lt;/U&gt;&lt;/STRONG&gt;With a data lake that replicates tables from SAP, Analysts working with functional resources can build models that meet their specific needs. If replications are managed through SAP SLT, then it enables near realtime (possible delay of a few seconds) reporting. Review must be done with functional consultants to ensure that tables being replicated dont have confidential content.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;As part of this blog series, we shall see some models that reflect SAP transactions or commonly used reporting metrics.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;U&gt;Factors that are not addressed in this blog but must be considered:&lt;/U&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;OL&gt;&lt;OL&gt;&lt;LI&gt;Organization of reporting models and data lake tables, if not using similar reference as SAP Application components. This becomes Important for managing confidentiality and ensuring personal information of customers, employees and vendors is only available to those that need it as part of their business roles.&lt;/LI&gt;&lt;/OL&gt;&lt;/OL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;OL&gt;&lt;OL&gt;&lt;LI&gt;Security models needed for&lt;BR /&gt;&lt;BR /&gt;&lt;OL&gt;&lt;OL&gt;&lt;LI&gt;Functional areas of reporting (multiple tables grouped in an area of reporting)&lt;/LI&gt;&lt;/OL&gt;&lt;/OL&gt;&lt;BR /&gt;&lt;OL&gt;&lt;OL&gt;&lt;LI&gt;Row based access&lt;/LI&gt;&lt;/OL&gt;&lt;/OL&gt;&lt;BR /&gt;&lt;OL&gt;&lt;OL&gt;&lt;LI&gt;Any additional configuration needed to secure fields in tables&lt;/LI&gt;&lt;/OL&gt;&lt;/OL&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;/LI&gt;&lt;/OL&gt;&lt;/OL&gt;&lt;P&gt;&lt;BR /&gt;&lt;BR /&gt;Here's the first script:&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;OL&gt;&lt;OL&gt;&lt;LI&gt;&lt;STRONG&gt;&lt;U&gt;Script for Plant maintenance object status&lt;/U&gt;&lt;/STRONG&gt;&lt;/LI&gt;&lt;/OL&gt;&lt;/OL&gt;&lt;P&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;U&gt;Need:&lt;/U&gt; Near real time availability of object status’ for Plant maintenance, eg: an emergency order created for addressing critical equipment failure, the status and progress of investigation needs to be communicated through the manufacturing channels for them to manage bottlnecks in production.&lt;BR /&gt;&lt;BR /&gt;&lt;U&gt;Solution: &lt;/U&gt;Below layout provides a simplified overview of how different tables are joined together with their respective fields.&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;Tables used:&lt;/STRONG&gt;&lt;BR /&gt;&lt;BR /&gt;JEST-Individual Object Status&lt;BR /&gt;&lt;BR /&gt;JCDS-Change Documents for System/User Statuses (Table JEST)&lt;BR /&gt;&lt;BR /&gt;JSTO- Status object information&lt;BR /&gt;&lt;BR /&gt;TJ02-System status&lt;BR /&gt;&lt;BR /&gt;TJ02T - System status texts&lt;BR /&gt;&lt;BR /&gt;TJ04- Status control for object type&lt;BR /&gt;&lt;BR /&gt;TJ30- User Status&lt;BR /&gt;&lt;BR /&gt;TJ30T- Texts for User Status&lt;/P&gt;&lt;P&gt;&lt;IMG src="https://community.sap.com/legacyfs/online/storage/blog_attachments/2023/11/Object-status-table-overview-1.jpg" border="0" /&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P class=""&gt;Object status tables relationship overview&lt;/P&gt;&lt;P&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;Script below provides active status’ for all Plant maintenance objects . To view all instances of status changes remove the JEST.INACT is NULL clause/restriction. Each table and the filter condition starts with a comment(begins with --) to show what it represents. May have to tweak formatting based on tool being used, especially the comments section.&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;/P&gt;&lt;BLOCKQUOTE&gt;&lt;BR /&gt;&lt;PRE&gt;SELECT



JEST.OBJNR AS OBJECT_NUMBER,



JSTO.OBTYP AS OBJECT_CATEGORY,



SUBSTR(JEST.OBJNR, 3) AS OBJECT,



JEST.STAT AS OBJECT_STATUS,



(CASE WHEN LEFT(JEST.STAT, 1) = ‘I’ THEN ‘SYSTEM’ ELSE ‘USER’ END) ASSTATUS_TYPE,



(CASE WHEN LEFT(JEST.STAT, 1) = ‘I’ THEN TJ02T.TXT04



ELSE TJ30T.TXT04 END) AS STATUS_SHORT_TEXT,



(CASE WHEN LEFT(JEST.STAT, 1) = ‘I’ THEN TJ02T.TXT30



ELSE TJ30T.TXT30 END) AS STATUS_LONG_TEXT,



JSTO.STSMA AS STATUS_PROFILE,



JCDS.USNAM AS STATUS_CHANGED_BY,



JCDS.UDATE AS STATUS_CHANGED_DATE,



JCDS.UTIME AS STATUS_CHANGED_TIME,



JCDS.CHIND AS STATUS_CHANGED_TYPE,



TJ04.INIST AS SYSTEM_STATUS_INITIAL_STATUS_FLAG,



TJ04.STATP AS SYSTEM_STATUS_DISPLAY_PRIORITY,



TJ04.LINEP AS SYSTEM_STATUS_LINE_POSITION,



TJ02.NODIS AS SYSTEM_STATUS_NO_DISPLAY_INDICATOR,



TJ02.SETONLY AS SYSTEM_STATUS_SET_ONLY_INDICATOR,



TJ30.STONR AS USER_STATUS_WITH_NUMBER,



TJ30.INIST AS USER_STATUS_INITIAL_STATUS_FLAG_INDICATOR,



TJ30.STATP AS USER_STATUS_DISPLAY_PRIORITY,



TJ30.LINEP AS USER_STATUS_LINE_POSITION,



CASE WHEN TJ30.LINEP = ’01’ THEN TJ30T.TXT04 END ASPOSITION1_USER_STATUS



FROM JEST --Individual object status



INNER JOIN JCDS -- Change Documents for System/User Statuses (Table JEST)



ON JEST.OBJNR = JCDS.OBJNR



AND JEST.STAT = JCDS.STAT



AND JEST.CHGNR = JCDS.CHGNR



LEFT JOIN JSTO -- Status profile information for objects



ON JEST.OBJNR = JSTO.OBJNR



LEFT JOIN TJ02T --System status texts



ON JEST.STAT = TJ02T.ISTAT



AND TJ02T.SPRAS = ‘E’



LEFT JOIN TJ04 -- System status control config table 2



ON JEST.STAT = TJ04.ISTAT



and TJ04.OBTYP = JSTO.OBTYP



LEFT JOIN TJ30T -- User status texts



ON JSTO.STSMA = TJ30T.STSMA



AND JEST.STAT = TJ30T.ESTAT



AND TJ30T.SPRAS = ‘E’



LEFT JOIN TJ02 ”System status config table 1



ON JEST.STAT = TJ02.ISTAT



LEFT JOIN TJ30 -- User status config table 1



ON JSTO.STSMA = TJ30.STSMA



AND JEST.STAT = TJ30.ESTAT



WHERE JEST.INACT is NULL -- remove this to see when a status was set inactive or to get timelines for all status&lt;/PRE&gt;&lt;/BLOCKQUOTE&gt;&lt;P&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;Conclusion : &lt;/STRONG&gt;Using the above code we can active status' and their respective times for all operational objects that have been configured for status tracking. Similar approach can be used to get status' for CRM using table CRM_JEST and CRM_JCDS. Remove the inactive filter to get status' that are currently not active (depending on the values are mapped in data lake i.e default value of blanks as NULLs, NULL may need to be replaced with '')&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;Possible variations based on need&lt;/STRONG&gt;:&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;&lt;OL&gt;&lt;OL&gt;&lt;LI&gt;To plot timeline of how the operational object moved between status' use JCDS&lt;/LI&gt;&lt;/OL&gt;&lt;/OL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;OL&gt;&lt;OL&gt;&lt;LI&gt;Restrict to certain Status profile(s) in table JSTO when requirement is to focus on certain types of objects or group&lt;/LI&gt;&lt;/OL&gt;&lt;/OL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;OL&gt;&lt;OL&gt;&lt;LI&gt;Restrict using change date and time if the need is to focus of recent changes within the hour or day(s)&lt;/LI&gt;&lt;/OL&gt;&lt;/OL&gt;&lt;P&gt;&lt;BR /&gt;&lt;BR /&gt;&amp;nbsp;&lt;BR /&gt;&lt;BR /&gt;Next blog will look at details of combining details of orders and related operational tasks&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/using-data-lake-and-sql-to-create-custom-reporting-models/ba-p/13579874"/>
    <published>2023-12-20T10:29:19+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/empowering-businesses-with-new-insights-the-google-cloud-and-sap-analytics/ba-p/13580348</id>
    <title>Empowering Businesses with New Insights: The Google Cloud and SAP Analytics Partnership</title>
    <updated>2023-12-28T12:35:19+01:00</updated>
    <author>
      <name>Thisgaard</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/4350</uri>
    </author>
    <content>A year ago, the tech giants Google Cloud and SAP embarked on a journey to revolutionize data analytics for businesses. Their goal: to bring together&amp;nbsp;SAP systems and data with Google’s data cloud, offering customers better insights for decision making and innovation. The new SAP Datasphere Replication Flow connector for Google BigQuery is now&lt;A href="https://blogs.sap.com/2023/11/16/replication-flows-sap-datasphere-and-google-big-query/" target="_blank" rel="noopener noreferrer"&gt; available&lt;/A&gt;.&lt;BR /&gt;
&lt;BR /&gt;
From the outset, customers have been excited about the potential of this partnership: integrating SAP's robust data models and real-time processes with Google BigQuery's comprehensive real-time data streams, including search engine data, weather data, marketing data, and customer event data, to inspire new and better ways to do business.&lt;BR /&gt;
&lt;BR /&gt;
Prior to this collaboration, businesses found it challenging to merge data models from either side due to cost, effort, and time. This partnership aims to eliminate these hurdles, providing real-time data streams that adjust dynamically to changes from Google Cloud and SAP. Haridas Nair, the Head of Cross Product Management for Database and Analytics at SAP, stated, "Customers using SAP Business Technology Platform can now extend the reach of curated and modeled SAP business data for downstream consumption with SAP Datasphere Replication Flow. The integration with BigQuery now enables customers to combine SAP business data with Google BigQuery data. This enables new use cases that can unleash significant business value."&lt;BR /&gt;
&lt;BR /&gt;
For example, while enterprises rely on SAP S/4HANA for their financial planning, reporting, and budgeting there are many that also have finance data coming in other systems. Join ventures, new acquisitions or decentralized business models are common cases where finance data will reside in non-SAP S/4HANA systems. Early adopters of such companies are leveraging the Google Cloud and the SAP Datasphere Replication Flow connector to unify accounting data insights into SAP Datasphere to get a single financial dashboard across all their financial sources and thereby enable secure, self-service access to reusable data models, and streamline financial reporting. The result is enhanced analytics that enable new market correlations to transpire, as well as reporting efficiency and reduced data management costs. As such finance and operations experts can receive new insights that improve their business planning.&lt;BR /&gt;
&lt;BR /&gt;
The other common use case relates to Consumer Products and Retail companies. A prime example is a North American consumer products company selling through retailers as well as their online platform. As many other companies in this space, they're investing in brand loyalty and scaling their product portfolio to target different customer segments. The company strives to corelate online customer trends with their retail channel sales using demographics and other consumer data.&lt;BR /&gt;
&lt;BR /&gt;
Their business goals involve improving channel inventory turns, trade promotion management, shelf-availability, SKU margins, and overall understanding of customer buying behavior. Simultaneously, they aim to reduce the cost, effort, and time for accessing SAP data and enable richer SAP ERP data in real-time.&lt;BR /&gt;
&lt;BR /&gt;
To achieve these goals, they have connected Google Cloud and SAP data to gain better insight into their retail channels and to improve their demand forecasting and supply chain algorithms. The more real-time the connectivity between their SAP data and Google BigQuery data, the more confident they'll be in the predictive algorithms they adopt for their supply chain.&lt;BR /&gt;
&lt;BR /&gt;
But this is just the beginning. The Google Cloud and SAP Analytics partnership opens the door to a wide range of strategic customer and supply chain programs that leverage data for advanced predictive demand models. Early examples of customer innovations include true customer 360 insight, improved sales performance, yield-driven pricing, new product introductions, unifying external accounting with SAP, manufacturing automation, and operationalizing sustainability.&lt;BR /&gt;
&lt;BR /&gt;
Both of the Google Cloud and SAP development organizations are excited to see that their work is making a difference. Future releases plan to include more advanced features for managing enterprise scale federation, replication and data catalogs between their respective data platforms. Honza Fedak, Director of BigQuery Engineering at Google Cloud stated, "The combination of Google Cloud's data and AI expertise and SAP's deep understanding of business data is a powerful force that can help businesses unlock the full potential of their data."&lt;BR /&gt;
&lt;BR /&gt;
As more enterprises utilize this partnership, we are confident that the Google Cloud and SAP analytics partnership can provide better insights, enable better decisions, and foster innovation. This partnership is a significant step towards creating more Intelligent Enterprises, and we hope your enterprise will be one of them.</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/empowering-businesses-with-new-insights-the-google-cloud-and-sap-analytics/ba-p/13580348"/>
    <published>2023-12-28T12:35:19+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/integration-options-for-moving-data-from-sap-into-databricks/ba-p/13793459</id>
    <title>Integration Options for moving data from SAP into Databricks</title>
    <updated>2024-08-13T19:58:39.925000+02:00</updated>
    <author>
      <name>STALANKI</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/13911</uri>
    </author>
    <content>&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;Background&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;This blog delves into the various methods for integrating data from your SAP systems into Databricks. This exploration is particularly relevant given SAP's recent announcement of SAP Datasphere in March 2023.&amp;nbsp;This collaboration aims to provide businesses with the power of federated AI-driven analytics. This will allow them to effortlessly analyze structured and unstructured data from both SAP and non-SAP sources within a single, unified platform.&lt;/P&gt;&lt;P class="lia-align-justify" data-unlink="true" style="text-align : justify;"&gt;However, I am not going to discuss how we integrate SAP systems into Databricks via Datasphere or BW4 HANA as we already have great blogs on&amp;nbsp;&lt;A href="https://community.sap.com/t5/technology-blogs-by-sap/replication-flow-blog-series-part-5-integration-of-sap-datasphere-and/ba-p/13604976" target="_self"&gt;datasphere&lt;/A&gt;&amp;nbsp;and &lt;A href="https://community.sap.com/t5/technology-blogs-by-members/sap-bw-hana-to-databricks-via-sap-di-i/ba-p/13580075" target="_self"&gt;BW4HANA.&lt;/A&gt;&lt;/P&gt;&lt;P class="lia-align-justify" data-unlink="true" style="text-align : justify;"&gt;This blog will explore options for migrating data from SAP to Databricks without relying on Datasphere or BW4HANA, even though licensing for transferring SAP data to non-SAP systems might still be necessary.&lt;/P&gt;&lt;P class="lia-align-justify" data-unlink="true" style="text-align : justify;"&gt;&lt;FONT size="4"&gt;&lt;SPAN&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="datalake.jpg" style="width: 470px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/151264i13412C5FE72F646F/image-dimensions/470x527?v=v2" width="470" height="527" role="button" title="datalake.jpg" alt="datalake.jpg" /&gt;&lt;/span&gt;&lt;/SPAN&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P data-unlink="true"&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;Integration Options&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P data-unlink="true"&gt;In this blog, I am discussing 4 different options to move data from SAP into data bricks.&lt;/P&gt;&lt;P data-unlink="true"&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;SAP Data Services ETL Integration&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;We can leverage the popular ETL tool SAP Data Services to move data between SAP and Databricks.&lt;/P&gt;&lt;P&gt;While a direct integration between SAP Data Services and Databricks might not be readily available, you can establish a connection using intermediary stages and leveraging data transfer mechanisms. Here are a few approaches:&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;File-Based Integration:&amp;nbsp;&lt;/STRONG&gt; Initiate the integration by designing and running data extraction jobs within SAP Data Services. These jobs should be configured to export your SAP data in formats readily consumable by Databricks, such as CSV, Parquet, or Avro. Once exported, these files can be seamlessly transferred to a storage service[Ex:&amp;nbsp;Azure Blob Storage or AWS S3, as well as shared file systems] accessible by Databricks.&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Database Staging: &lt;/STRONG&gt;Optimize your data pipeline by using SAP Data Services to efficiently load extracted and transformed data directly into a staging database readily accessible by Databricks.&amp;nbsp;Suitable options for this staging database include Azure SQL Database, Amazon Redshift, or similar platforms. Once the data is in the staging area, establish a connection between Databricks and the database using&amp;nbsp; Spark JDBC connectors or Azure Synapse native connectors and map the respective tables.&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;Custom Integration using APIs:&amp;nbsp;&lt;/STRONG&gt;Investigate the availability of APIs or SDKs provided by both SAP Data Services and Databricks. Develop custom scripts or applications using languages like Python or Java to&amp;nbsp; extract data from SAP Data Services and transfer it to Databricks using their respective APIs.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="SAP Databricks Integration Options v.1.jpg" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/151251iEB94B4428CC11D04/image-size/large?v=v2&amp;amp;px=999" role="button" title="SAP Databricks Integration Options v.1.jpg" alt="SAP Databricks Integration Options v.1.jpg" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&lt;FONT face="trebuchet ms,geneva" size="5"&gt;&lt;STRONG&gt;SAP SLT Integration&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;Replicating SAP data to external systems using SAP SLT can be complex, but leveraging HANA as a staging area provides a pathway for efficient real-time replication. By establishing connectivity through JDBC Spark or SDI HANA connectors, you can move data into Databricks for AI based predictive analytics.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Option2DB.jpg" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/151255i608F834D0F1E5492/image-size/large?v=v2&amp;amp;px=999" role="button" title="Option2DB.jpg" alt="Option2DB.jpg" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT face="trebuchet ms,geneva" size="5"&gt;&lt;STRONG&gt;Event Based Messaging&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;Set up SAP BTP Integration Platform to capture real-time data changes from your SAP system, leveraging Change Data Capture (CDC) mechanisms or APIs for seamless data extraction. Then, integrate SAP BTP&amp;nbsp;Integration Platform&amp;nbsp;with a message queue or streaming platform like Apache Kafka or Azure Event Hubs to reliably publish these captured data changes. Databricks can then tap into these data streams using its robust streaming capabilities, subscribing to and consuming the data from the message queue.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;This approach empowers you with near real-time data ingestion and analysis capabilities within Databricks. For additional flexibility, consider incorporating HANA Cloud as an optional staging area to further transform and prepare your data before it's loaded into Databricks.&lt;/P&gt;&lt;P&gt;&lt;FONT face="trebuchet ms,geneva" size="5"&gt;&lt;STRONG&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="DatabricksKafka.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/151258i3AB70371847AAA0A/image-size/large?v=v2&amp;amp;px=999" role="button" title="DatabricksKafka.png" alt="DatabricksKafka.png" /&gt;&lt;/span&gt;&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT face="trebuchet ms,geneva" size="5"&gt;&lt;STRONG&gt;SNP GLUE&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;SNP Glue is another product that can be used to replicate data from SAP platforms into cloud platforms.&amp;nbsp;While that particular product might have limitations in terms of advanced transformation capabilities, it's essential to investigate its compatibility with other cloud solutions like SuccessFactors and Ariba to ensure a comprehensive integration strategy.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&amp;nbsp;&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&lt;FONT face="trebuchet ms,geneva" size="5"&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="SNP Glue.jpg" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/151263iB02B6E9BCAC781ED/image-size/large?v=v2&amp;amp;px=999" role="button" title="SNP Glue.jpg" alt="SNP Glue.jpg" /&gt;&lt;/span&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;Key Considerations&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;We need consider the following factors when choosing the right tool :&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&lt;STRONG&gt;Data Volume and Frequency:&lt;/STRONG&gt; The chosen integration method should align with the volume of data being transferred and the desired frequency of updates.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&lt;STRONG&gt;Data Transformation:&lt;/STRONG&gt; Determine whether data transformations are necessary before loading into Databricks and whether these transformations are best performed within SAP Data Services or using Databricks' data manipulation capabilities.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&lt;STRONG&gt;Security and Access Control:&lt;/STRONG&gt;Implement appropriate security measures to protect data during transfer and storage, ensuring secure access to both SAP Data Services and Databricks.&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;&lt;STRONG&gt;Data Latency Requirements: &lt;/STRONG&gt;Determine the acceptable latency for data availability in Databricks. The streaming approach offers near real-time capabilities, while the intermediate database approach might involve some delay&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;As you embark on your SAP-Databricks integration journey, carefully consider your specific needs, data characteristics, and latency requirements to select the optimal approach for your business. With a well-planned strategy and the right tools in place, you can harness the combined power of SAP and Databricks for AI powered federated analytics.&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/integration-options-for-moving-data-from-sap-into-databricks/ba-p/13793459"/>
    <published>2024-08-13T19:58:39.925000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/consuming-data-from-datasphere-to-azure-data-factory-via-odbc/ba-p/13869551</id>
    <title>Consuming Data from Datasphere to Azure Data Factory via ODBC</title>
    <updated>2024-09-18T13:43:15.587000+02:00</updated>
    <author>
      <name>vignesh3027</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/160733</uri>
    </author>
    <content>&lt;P&gt;&lt;STRONG&gt;Prerequisites:&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Access:&lt;/STRONG&gt; Access to ADF and Datasphere.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Credentials:&lt;/STRONG&gt; Datasphere and ADF credential details.&lt;/LI&gt;&lt;/UL&gt;&lt;H2 id="toc-hId-1049069880"&gt;&lt;STRONG&gt;Connect Datasphere to Azure Data Factory&lt;/STRONG&gt;&lt;/H2&gt;&lt;H2 id="toc-hId-852556375"&gt;&lt;STRONG&gt;DATASPHERE PART:&lt;/STRONG&gt;&lt;/H2&gt;&lt;UL&gt;&lt;LI&gt;Log in to Datasphere -&amp;gt; Space Management -&amp;gt; Choose the space and select Edit&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="vignesh3027_24-1726656894703.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/167873i670A26213FBEDD04/image-size/large?v=v2&amp;amp;px=999" role="button" title="vignesh3027_24-1726656894703.png" alt="vignesh3027_24-1726656894703.png" /&gt;&lt;/span&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;Click Create and Make sure that you have enabled Expose for consumption by default&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="vignesh3027_25-1726656921632.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/167874i5A227EA201C8241C/image-size/large?v=v2&amp;amp;px=999" role="button" title="vignesh3027_25-1726656921632.png" alt="vignesh3027_25-1726656921632.png" /&gt;&lt;/span&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="vignesh3027_26-1726656953296.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/167875i83CADA8A2CB82EA3/image-size/large?v=v2&amp;amp;px=999" role="button" title="vignesh3027_26-1726656953296.png" alt="vignesh3027_26-1726656953296.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Copy the Database Username, Hostname, Port, Password&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="vignesh3027_27-1726657059047.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/167877i00A6C9F2E1F77457/image-size/large?v=v2&amp;amp;px=999" role="button" title="vignesh3027_27-1726657059047.png" alt="vignesh3027_27-1726657059047.png" /&gt;&lt;/span&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;Go to System-&amp;gt; Configuration-&amp;gt; IP Allowlist-&amp;gt; Trusted Ips&lt;BR /&gt;EXTERNAL IPV4 ADDRESS should be added here, not Internal IPV4&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="vignesh3027_28-1726657167838.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/167879iA19DF6F9E2CD2BD7/image-size/large?v=v2&amp;amp;px=999" role="button" title="vignesh3027_28-1726657167838.png" alt="vignesh3027_28-1726657167838.png" /&gt;&lt;/span&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;To get an External IPV4 Address, use this URL:&amp;nbsp; &lt;SPAN&gt;&lt;A href="https://whatismyipaddress.com/" target="_blank" rel="noopener nofollow noreferrer"&gt;What Is My IP Address - See Your Public Address - IPv4 &amp;amp; IPv6&lt;/A&gt;&lt;/SPAN&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Add and Save &lt;/STRONG&gt;the External ipv4 address in the Datasphere’s IP Allowlist.&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="vignesh3027_29-1726657199288.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/167881i950182F9C33AF190/image-size/large?v=v2&amp;amp;px=999" role="button" title="vignesh3027_29-1726657199288.png" alt="vignesh3027_29-1726657199288.png" /&gt;&lt;/span&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;H3 id="toc-hId-785125589"&gt;&lt;STRONG&gt;ODBC PART:&lt;/STRONG&gt;&lt;/H3&gt;&lt;UL&gt;&lt;LI&gt;Need to install SAP HDODBC driver &lt;SPAN&gt;&lt;A href="https://tools.eu1.hana.ondemand.com/#hanatools" target="_blank" rel="noopener nofollow noreferrer"&gt;SAP Development Tools (ondemand.com)&lt;/A&gt;&lt;/SPAN&gt; in the system.&lt;/LI&gt;&lt;LI&gt;Open ODBC in the system&lt;/LI&gt;&lt;LI&gt;Click Add&lt;/LI&gt;&lt;LI&gt;Select HDODBC&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="vignesh3027_30-1726657244135.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/167882i83514388B627B176/image-size/large?v=v2&amp;amp;px=999" role="button" title="vignesh3027_30-1726657244135.png" alt="vignesh3027_30-1726657244135.png" /&gt;&lt;/span&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="vignesh3027_31-1726657274364.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/167884iDEF7EEAFE2DED4A1/image-size/large?v=v2&amp;amp;px=999" role="button" title="vignesh3027_31-1726657274364.png" alt="vignesh3027_31-1726657274364.png" /&gt;&lt;/span&gt;&lt;P class="lia-align-center" style="text-align: center;"&gt;&amp;nbsp;&lt;/P&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;UL&gt;&lt;UL&gt;&lt;LI&gt;Give any meaningful name to Data source name, description.&lt;/LI&gt;&lt;/UL&gt;&lt;/UL&gt;&lt;UL&gt;&lt;UL&gt;&lt;LI&gt;Database type: SAP HANA Cloud or SAP HANA Single tenant (both will work fine).&lt;/LI&gt;&lt;/UL&gt;&lt;/UL&gt;&lt;UL&gt;&lt;UL&gt;&lt;LI&gt;Already copied Host URL in datasphere space, Paste the copied Host URL.&lt;/LI&gt;&lt;/UL&gt;&lt;/UL&gt;&lt;UL&gt;&lt;UL&gt;&lt;LI&gt;Click &lt;STRONG&gt;Test connection&lt;/STRONG&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;/UL&gt;&lt;UL&gt;&lt;UL&gt;&lt;LI&gt;Paste the Database username in the Username and Password&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="vignesh3027_32-1726657318730.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/167887iF74C597CF9999BF5/image-size/large?v=v2&amp;amp;px=999" role="button" title="vignesh3027_32-1726657318730.png" alt="vignesh3027_32-1726657318730.png" /&gt;&lt;/span&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="vignesh3027_33-1726657370908.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/167892iA02F4188CCA9564E/image-size/large?v=v2&amp;amp;px=999" role="button" title="vignesh3027_33-1726657370908.png" alt="vignesh3027_33-1726657370908.png" /&gt;&lt;/span&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="vignesh3027_34-1726657394852.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/167893i173324864AA39077/image-size/large?v=v2&amp;amp;px=999" role="button" title="vignesh3027_34-1726657394852.png" alt="vignesh3027_34-1726657394852.png" /&gt;&lt;/span&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;/UL&gt;&lt;H3 id="toc-hId-588612084"&gt;&lt;STRONG&gt;AZURE DATA FACTORY PART:&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;&lt;STRONG&gt;Open Azure Data Factory&lt;/STRONG&gt;:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Go to your Azure Data Factory instance via the Azure Portal.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Create a Linked Service&lt;/STRONG&gt;:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;On the left pane, go to &lt;STRONG&gt;Manage&lt;/STRONG&gt; &amp;gt; &lt;STRONG&gt;Linked services&lt;/STRONG&gt;.&lt;/LI&gt;&lt;LI&gt;Click &lt;STRONG&gt;New&lt;/STRONG&gt; to create a new Linked Service.&lt;/LI&gt;&lt;LI&gt;In the search box, search for &lt;STRONG&gt;ODBC&lt;/STRONG&gt; or &lt;STRONG&gt;SAP HANA&lt;/STRONG&gt;.&lt;/LI&gt;&lt;LI&gt;Select &lt;STRONG&gt;ODBC&lt;/STRONG&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="vignesh3027_1-1727088001141.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/169743iA50A4BD9D0CADAF6/image-size/large?v=v2&amp;amp;px=999" role="button" title="vignesh3027_1-1727088001141.png" alt="vignesh3027_1-1727088001141.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Enter Connection Information&lt;/STRONG&gt;:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;In the &lt;STRONG&gt;Connection String&lt;/STRONG&gt; field, enter the connection string:&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;Driver={HDBODBC};ServerNODE=XXXXXXXXXX:443;UID=SAP_CONTENT#XXXX;PWD=XXXXXXXXXXXX;SCHEMA=SAP_CONTENT;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Breakdown of Components:&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Driver: Specifies the SAP HANA ODBC driver needed to connect (e.g., {HDBODBC}).&lt;/LI&gt;&lt;LI&gt;ServerNODE: Indicates the SAP HANA server address and port to connect to (e.g., rw2922...443).&lt;/LI&gt;&lt;LI&gt;UID: The username used to authenticate to SAP HANA (e.g., SAP_CONTENT#XXXX).&lt;/LI&gt;&lt;LI&gt;PWD: The password associated with the provided username for authentication.&lt;/LI&gt;&lt;LI&gt;SCHEMA: The specific database schema where the data is located (e.g., SAP_CONTENT).&lt;/LI&gt;&lt;LI&gt;Encrypt: Ensures the connection is encrypted for secure communication (e.g., Encrypt=1).&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Authentication Type&lt;/STRONG&gt;:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Choose &lt;STRONG&gt;Basic Authentication&lt;/STRONG&gt; since you’re using a username and password.&lt;/LI&gt;&lt;LI&gt;Enter the copied Datasphere’s Space username and password.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Test the Connection&lt;/STRONG&gt;:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Click on the &lt;STRONG&gt;Test Connection&lt;/STRONG&gt; button to make sure the connection is successful.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="vignesh3027_0-1727087847159.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/169740i4A19DDC4CBF6B01D/image-size/large?v=v2&amp;amp;px=999" role="button" title="vignesh3027_0-1727087847159.png" alt="vignesh3027_0-1727087847159.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;To check the connection, create a copy pipeline and choose the ODBC as the connector we used to get data from Datasphere and the corresponding Destination environment.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="vignesh3027_3-1727088706719.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/169751iA390520116BC0E28/image-size/large?v=v2&amp;amp;px=999" role="button" title="vignesh3027_3-1727088706719.png" alt="vignesh3027_3-1727088706719.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;In this case, I have chosen the &lt;STRONG&gt;Azure Data Lake Storage Gen 2&lt;/STRONG&gt; as the Destination environment.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="vignesh3027_1-1727088427086.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/169746i62CD0EECA0EB3B8F/image-size/large?v=v2&amp;amp;px=999" role="button" title="vignesh3027_1-1727088427086.png" alt="vignesh3027_1-1727088427086.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Login to Azure Data Lake Storage via Azure platform and check that the data copied&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="vignesh3027_2-1727088654728.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/169749i829313BE7C531B54/image-size/large?v=v2&amp;amp;px=999" role="button" title="vignesh3027_2-1727088654728.png" alt="vignesh3027_2-1727088654728.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt; &lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Hence the connection is established successfully from Datasphere to Azure Data Factory.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;HINTS: &lt;span class="lia-unicode-emoji" title=":grinning_face:"&gt;😀&lt;/span&gt;🤫&lt;/P&gt;&lt;P&gt;If your connection failed, then you have to check these two things,&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;Make sure your current IPV4 is added to Datasphere's IP Allowlist.&lt;/LI&gt;&lt;LI&gt;Ensure you have entered the correct Datasphere's Space credentials in your systems ODBC and test connection.&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;Thank you!&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/consuming-data-from-datasphere-to-azure-data-factory-via-odbc/ba-p/13869551"/>
    <published>2024-09-18T13:43:15.587000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-members/sap-community-question-in-your-opinion-what-s-the-biggest-use-case-for/ba-p/13936650</id>
    <title>SAP Community Question: In your opinion, what's the biggest use-case for Blockchain at SAP Customers</title>
    <updated>2024-11-13T09:09:19.281000+01:00</updated>
    <author>
      <name>AndySilvey</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1397601</uri>
    </author>
    <content>&lt;P&gt;Good Morning SAP Community,&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;EM&gt;in your opinion what is _the_ biggest use case for Enterprise Blockchain / Distributed Ledger Technology at SAP Customers ?&lt;/EM&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Anybody following my blogs will know &lt;A href="https://community.sap.com/t5/technology-blogs-by-members/why-i-love-sap-and-blockchain-databases-and-why-you-should-too/ba-p/13625869" target="_self"&gt;I like Enterprise Blockchain&lt;/A&gt;, and am interested in &lt;A href="https://community.sap.com/t5/technology-blogs-by-members/sap-enterprise-architecture-positioning-blockchain-database-as-an/ba-p/13629842" target="_self"&gt;positioning Enterprise Blockchain as a Technology Standard&lt;/A&gt; for &lt;A href="https://community.sap.com/t5/technology-blogs-by-members/sap-enterprise-architecture-let-the-use-case-find-the-blockchain/ba-p/13632458" target="_self"&gt;use-cases and business demands&lt;/A&gt; at SAP Customers enabling them to &lt;A href="https://community.sap.com/t5/technology-blogs-by-members/running-your-own-blockchain-on-the-sap-btp-kyma-trial-a-hands-on-how-to/ba-p/13724580" target="_self"&gt;run&lt;/A&gt; Enterprise Blockchain.&lt;/P&gt;&lt;P&gt;I talk to a lot of people about this and regularly get asked the same question, what is _the_ BIG use case for Blockchain in the Enterprise and at SAP Customers ?&lt;/P&gt;&lt;P&gt;So, the floor is open, the comments are open, there is no wrong answer, and everybody is invited to give their opinion on,&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;for SAP Customers, what is the biggest use case for Blockchain ?&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Over to you, feel welcome to put your thoughts in the comments and let's see what we discover, and, there is no wrong answer &lt;span class="lia-unicode-emoji" title=":slightly_smiling_face:"&gt;🙂&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Andy Silvey.&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-members/sap-community-question-in-your-opinion-what-s-the-biggest-use-case-for/ba-p/13936650"/>
    <published>2024-11-13T09:09:19.281000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/beyond-the-data-silo-the-convergence-of-sap-systems-and-modern-data-lake/ba-p/14011954</id>
    <title>Beyond the Data Silo: The Convergence of SAP Systems and Modern Data Lake Technologies</title>
    <updated>2025-02-10T04:53:39.311000+01:00</updated>
    <author>
      <name>rugved88</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/842167</uri>
    </author>
    <content>&lt;P&gt;&lt;SPAN&gt;In today's enterprise landscape, a fascinating transformation is underway. While SAP systems continue to serve as the backbone of business operations, organizations are discovering that traditional approaches to data management no longer meet the demands of our rapidly evolving digital economy. The challenge isn't just about managing data – it's about turning it into actionable intelligence at the speed of business.&lt;/SPAN&gt;&lt;/P&gt;&lt;H2 id="toc-hId-1702688267"&gt;&lt;STRONG&gt;The Evolution of SAP Data Architecture&lt;/STRONG&gt;&lt;/H2&gt;&lt;P&gt;&lt;SPAN&gt;The SAP ecosystem has long been the backbone of enterprise operations, housing critical business data across its various modules. However, today's digital economy demands more than just robust transaction processing – it requires seamless integration of internal and external data sources, real-time analytics, and AI-driven insights.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;The current state of enterprise data presents an intriguing paradox. Consider a typical global manufacturing Company X: Multiple SAP instances span continents, each running critical modules like FI/CO, MM, SD, and PP. In Europe, teams work with one set of customizations and configurations, while their Americas counterparts operate with different chart of accounts and business rules. What seems like a simple question about global inventory levels can spiral into a days-long exercise involving multiple teams, countless Excel sheets, and lengthy email threads.&lt;/SPAN&gt;&lt;/P&gt;&lt;DIV class=""&gt;&amp;nbsp;&lt;/DIV&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="system-architecture_.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/223916iA3D5AA05D3AD969A/image-size/large?v=v2&amp;amp;px=999" role="button" title="system-architecture_.png" alt="system-architecture_.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P class="lia-align-center" style="text-align: center;"&gt;&lt;SPAN&gt;Traditional-architecture&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;These challenges extend beyond just SAP systems. The integration of external data sources – IoT sensors, market data feeds, social media analytics – adds layers of complexity to an already intricate landscape. The time lag between data creation and insight generation has become a critical bottleneck for business agility.&lt;/SPAN&gt;&lt;/P&gt;&lt;H2 id="toc-hId-1506174762"&gt;&lt;STRONG&gt;The Promise of Modern Data Lakes&lt;/STRONG&gt;&lt;/H2&gt;&lt;P&gt;&lt;SPAN&gt;Modern data lake technologies represent a paradigm shift in how enterprises can manage and utilize their data. These platforms bring transformative capabilities through their support for industry-standard formats, enabling seamless data exchange across the enterprise. Their flexible schema management adapts to changing business needs, while enterprise-grade reliability is ensured through ACID transactions. The ability to access and restore historical data states, combined with unified processing for both batch and streaming workloads, creates a robust foundation for next-generation enterprise data management.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;H2 id="toc-hId-1309661257"&gt;&lt;STRONG&gt;The Convergence: A New Enterprise Data Architecture&lt;/STRONG&gt;&lt;/H2&gt;&lt;P&gt;&lt;SPAN&gt;The future lies in architectures that bring together the best of both worlds – SAP's robust business processes and the flexibility of modern data lakes. This next-generation architecture reimagines how enterprise data flows and interacts:&lt;/SPAN&gt;&lt;/P&gt;&lt;DIV class=""&gt;&amp;nbsp;&lt;/DIV&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="Untitled diagram-2025-02-09-090222.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/223917i91AFA408420F800C/image-size/large?v=v2&amp;amp;px=999" role="button" title="Untitled diagram-2025-02-09-090222.png" alt="Untitled diagram-2025-02-09-090222.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P class="lia-align-center" style="text-align: center;"&gt;&lt;SPAN&gt;Future-architecture&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&amp;nbsp;This convergence enables a new approach to data governance, where metadata management, data quality rules, and lineage tracking are centralized across both SAP and non-SAP data. Organizations can achieve zero-latency access to operational data with instant synchronization across systems, while performing real-time analytics without impacting transaction systems.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Perhaps most importantly, this convergence creates an AI-ready data foundation. By standardizing data formats for AI/ML workloads and enabling the integration of structured and unstructured data, organizations can fully leverage the power of large language models and generative AI across their enterprise data landscape.&lt;/SPAN&gt;&lt;/P&gt;&lt;H2 id="toc-hId-1113147752"&gt;&lt;STRONG&gt;Practical Implementation Strategies&lt;/STRONG&gt;&lt;/H2&gt;&lt;P&gt;&lt;SPAN&gt;Organizations looking to modernize their SAP data architecture should begin with a focus on high-value scenarios that require integrated data, particularly in areas where real-time insights drive business value. Projects that combine SAP and external data often yield the most significant returns and provide valuable learning experiences for teams. The foundation of this modernization requires implementing modern data lake capabilities alongside existing systems, while establishing unified governance frameworks. Creating semantic layers that abstract technical complexity enables broader adoption across the organization. Organizations must also prepare their data structures for AI workloads, implementing robust data quality measures and building pipelines for continuous data refreshes.&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;H2 id="toc-hId-916634247"&gt;&lt;STRONG&gt;Conclusion: Preparing for Tomorrow&lt;/STRONG&gt;&lt;/H2&gt;&lt;P&gt;&lt;SPAN&gt;The convergence of SAP systems with modern data lake technologies isn't just a technical evolution – it's a business imperative. Organizations that successfully navigate this transformation will find themselves better positioned to accelerate innovation through integrated data access, improve decision-making with real-time insights, enable AI-driven business processes, and maintain competitive advantage in the digital economy.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;The question isn't whether to embrace this transformation, but how to implement it in a way that maximizes business value while minimizing disruption. As technology leaders, our role is to guide our organizations through this evolution, ensuring we build data architectures that are not just modern, but future-ready.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;What steps is your organization taking to modernize its enterprise data architecture? The journey toward unified, AI-ready data platforms is just beginning, and the decisions we make today will shape our ability to compete tomorrow.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;BR /&gt;&lt;BR /&gt;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/beyond-the-data-silo-the-convergence-of-sap-systems-and-modern-data-lake/ba-p/14011954"/>
    <published>2025-02-10T04:53:39.311000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/sap-for-utilities-blog-posts/announcing-the-water-track-at-the-sap-for-energy-and-utilities-conference/ba-p/14012654</id>
    <title>Announcing the Water Track at the SAP for Energy and Utilities Conference 2025</title>
    <updated>2025-02-10T16:01:41.866000+01:00</updated>
    <author>
      <name>MiquelCarbo</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/176877</uri>
    </author>
    <content>&lt;P&gt;For the third consecutive year, we are happy to announce a dedicated track for water and wastewater companies in the &lt;STRONG&gt;SAP for Energy and Utilities Conference&lt;/STRONG&gt; &lt;A href="https://tac-insights.com/events/sap-for-energy-and-utilities-conference/agenda/" target="_self" rel="nofollow noopener noreferrer"&gt;agenda&lt;/A&gt; for this year.&lt;/P&gt;&lt;P&gt;It will take place on &lt;A href="https://tac-insights.com/events/sap-for-energy-and-utilities-conference/agenda/" target="_blank" rel="noopener nofollow noreferrer"&gt;Wednesday afternoon April 9th&lt;/A&gt;&amp;nbsp;with the aim to recognize the success of this section in previous conference editions and continue giving floor to a selection of exceptional professionals not only presenting, but building relationships and developing partnerships among this industry group.&lt;/P&gt;&lt;P&gt;Shifting from an agenda designed last year to show the concept architecture built by SAP to support water management processes to help organizations leveraging their software investments to tackle processes such as smart meter data management, named the “Smart Platform for Water”, current year be focused on cases where SAP portfolio of applications and technologies can help organizations confront their challenges and obtain operational benefits.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="inge opreel.jpg" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/224312iAA643991ED6C35D0/image-size/large?v=v2&amp;amp;px=999" role="button" title="inge opreel.jpg" alt="inge opreel.jpg" /&gt;&lt;/span&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Mrs Inge Opreel, Farys CIO&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;I am delighted to inform you about the presentions of the Water Track:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;SAP for Water and Wastewater Services: Building the Resilient Water Company (SAP Keynote)&lt;/STRONG&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;ESRI ArcGIS on HANA: Building Next Generation of Integrated Asset Management at FARYS&lt;/STRONG&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Streaming Service Operations: SAP Field Service Management (FSM) Integration at Vitens&lt;/STRONG&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;But the agenda of water-specific items will not be restricted to this track but will be highly recommended water-specific or water-relevant conferences before and after the Water Track which will configure an entire agenda specifically for delegates of this industry:&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="business data cloud.jpg" style="width: 984px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/224313i4516DF41540B8131/image-size/large?v=v2&amp;amp;px=999" role="button" title="business data cloud.jpg" alt="business data cloud.jpg" /&gt;&lt;/span&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;SAP Business Data Cloud&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Wednesday morning, April 9th&amp;nbsp;&lt;/STRONG&gt;(before the Water Track): suggested slots unveil the large ongoing updates in the portfolio of Asset Management, including an interesting use case of Asset Performance Management as well as an excellent presentation by SAP Product Development of the evolution of SAP analytical&amp;nbsp; portfolio, this is Business Data Cloud: &amp;nbsp;&amp;nbsp;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Latest Innovations in SAP Asset Management &lt;/STRONG&gt;(Asset Management track)&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Business Data Cloud – Unlocking the Power of Unified Data&lt;/STRONG&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;SAP Asset Performance Management (APM) at AkerBP:&amp;nbsp; From Test Environment to Production&lt;/STRONG&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;Thursday morning, April 10th&lt;/STRONG&gt; will be a continuation of the water-specific cases, presenting the application of AI with SAP technology platform at FARYS, a live demonstration at the Pop-up Campus (which will require a pre-booking)&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-left" image-alt="SAP Pop Up Campus.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/224365iFE8B27A77E990EF6/image-size/medium?v=v2&amp;amp;px=400" role="button" title="SAP Pop Up Campus.png" alt="SAP Pop Up Campus.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;EM&gt;Inside of SAP Pop-Up Campus&lt;/EM&gt;&lt;/P&gt;&lt;P&gt;A very interesting use case of RISE conversion, from a perspective very close to the reality of many water companies'.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Unlocking the Potential of AI with SAP Business Technology Platform (BTP) and Datasphere&lt;/STRONG&gt; (SAP Microforum at 10:30AM&amp;nbsp;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;A Day in a Life of a Water Drop&lt;/STRONG&gt;: &lt;STRONG&gt;Deep Dive into the Benefits of Asset Management at the Lifecycle of a Waterdrop&lt;/STRONG&gt; (session needs to be pre-booked via the app)&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Transformation from OnPrem to SAP RISE&lt;/STRONG&gt;: &lt;STRONG&gt;Lessons from Andel’s Journey&lt;/STRONG&gt; (AI Track)&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;Overall, the complete conference is designed for an era based in&lt;STRONG&gt; networks, relationships and reputation, where collaboration and developing partnerships is the key of success.&lt;/STRONG&gt; But it is known that such relationships cannot be automated &lt;SPAN&gt;and the soul of this track, and the overall water community formation, is the inheritance of the Executive Value Network for Water and Waste Water Companies workshops. This is a regular meeting of water companies taking place in Europe (already six editions), Latin America (three) and now Africa. Participants highlight common challenges and go along with case studies of how SAP customers are leveraging SAP solutions to address them. &lt;/SPAN&gt;&lt;STRONG&gt;Take this unique opportunity to connect with your peers, gain insights into industry trends, and chart your development plan with the latest technologies of 2025!&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;Looking forward to seeing you in Rotterdam!&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;A href="https://tac-insights.com/events/sap-for-energy-and-utilities-conference/" target="_self" rel="nofollow noopener noreferrer"&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-left" image-alt="EUC Banner Register Now Email Campaign 2.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/224349i21517B4D8ADDD6D3/image-size/medium?v=v2&amp;amp;px=400" role="button" title="EUC Banner Register Now Email Campaign 2.png" alt="EUC Banner Register Now Email Campaign 2.png" /&gt;&lt;/span&gt;&lt;/A&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/sap-for-utilities-blog-posts/announcing-the-water-track-at-the-sap-for-energy-and-utilities-conference/ba-p/14012654"/>
    <published>2025-02-10T16:01:41.866000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/unlock-the-power-of-cloud-ai-and-business-transformation-at-sap-sapphire/ba-p/14074005</id>
    <title>🚀 Unlock the Power of Cloud, AI, and Business Transformation at SAP Sapphire 2025 in Madrid</title>
    <updated>2025-04-12T09:23:10.254000+02:00</updated>
    <author>
      <name>oliverhuschke</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/35096</uri>
    </author>
    <content>&lt;P&gt;&lt;SPAN&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="AdobeStock_1351012730.jpeg" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/249980iA3359038D5A989D0/image-size/large?v=v2&amp;amp;px=999" role="button" title="AdobeStock_1351012730.jpeg" alt="AdobeStock_1351012730.jpeg" /&gt;&lt;/span&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;From &lt;A href="https://www.sap.com/events/sapphire/madrid.html" target="_self" rel="noopener noreferrer"&gt;&lt;STRONG&gt;May 26 to 28, SAP Sapphire in Madrid&lt;/STRONG&gt;&lt;/A&gt; will bring together the brightest minds, boldest innovations, and most impactful customer stories across the SAP ecosystem. Whether you're embarking on your RISE with SAP journey, transforming your support strategy with AI, or streamlining operations with SAP Cloud ALM, this is your chance to connect, learn, and get inspired.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;If you're planning your agenda, make sure to explore the powerful lineup of sessions hosted by our SAP Customer Support &amp;amp; Cloud Lifecycle Management team. These sessions dive deep into real customer use cases, cutting-edge tools, and strategic insights that can help you unlock value, improve resilience, and stay ahead in today’s rapidly evolving business landscape.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&lt;span class="lia-unicode-emoji" title=":sparkles:"&gt;✨&lt;/span&gt;&lt;/SPAN&gt;&lt;SPAN&gt;&lt;STRONG&gt;Don't miss these highlights from our team at SAP Sapphire Madrid 2025&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&lt;span class="lia-unicode-emoji" title=":round_pushpin:"&gt;📍&lt;/span&gt;&lt;/SPAN&gt;&lt;SPAN&gt;&lt;STRONG&gt;Main Theater &amp;amp; Breakout Sessions – Tuesday, May 27&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;A href="https://www.sap.com/events/sapphire/flow/sap/sm25/catalog-inperson/page/catalog/session/1741233073735001KoTL" target="_self" rel="noopener noreferrer"&gt;&lt;SPAN&gt;&lt;STRONG&gt;SER1211&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt; | 01:00 PM | Room 10.17&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;STRONG&gt;Achieving a clean core with your RISE with SAP journey&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;EM&gt;Wieland Schreiner&lt;/EM&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;Learn how RISE with SAP enables a clean core in your transition to cloud ERP. See how SAP Cloud ALM supports transformation across five key dimensions and discover proven strategies for sustainable business success.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;A href="https://www.sap.com/events/sapphire/flow/sap/sm25/catalog-inperson/page/catalog/session/1741233279411001vM5u" target="_self" rel="noopener noreferrer"&gt;&lt;SPAN&gt;&lt;STRONG&gt;SER1213&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt; | 01:30 PM | Room 10.17&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;STRONG&gt;Accelerating success: Unlocking value with smooth customer onboarding&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;EM&gt;Lee Evans&lt;/EM&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;Discover how structured onboarding can fast-track value realization. This session shares strategies to drive adoption, increase customer lifetime value, and build a solid foundation for ongoing success.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;A href="https://www.sap.com/events/sapphire/flow/sap/sm25/catalog-inperson/page/catalog/session/1741233186947001Rzhr" target="_self" rel="noopener noreferrer"&gt;&lt;SPAN&gt;&lt;STRONG&gt;SER1212&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt; | 02:00 PM | Room 10.19&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;STRONG&gt;Streamline your RISE with SAP journey with the power of SAP Cloud ALM&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;EM&gt;Wieland Schreiner&lt;/EM&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;Explore how SAP Cloud ALM works with SAP LeanIX and SAP Signavio to simplify landscape discovery, optimize processes, and enhance visibility. Hear success stories and practical takeaways for smarter transformation.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;A href="https://www.sap.com/events/sapphire/flow/sap/sm25/catalog-inperson/page/catalog/session/1742798561047001Ijpk" target="_self" rel="noopener noreferrer"&gt;&lt;SPAN&gt;&lt;STRONG&gt;SER1372&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt; | 02:30 PM | Room 10.15&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;STRONG&gt;Fast, Smart, and Scalable – How Bosch Leverages SAP S/4HANA Public Cloud&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;EM&gt;Stefan Steinle, Bastian Kloiber (Bosch)&lt;/EM&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;Get an inside look at Bosch’s two-tier ERP strategy using SAP S/4HANA Public Cloud. Learn how automation, SAP Best Practices, and close collaboration with SAP helped ensure smooth rollouts and operational success.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;A href="https://www.sap.com/events/sapphire/flow/sap/sm25/catalog-inperson/page/catalog/session/1741233473888001Qwkb" target="_self" rel="noopener noreferrer"&gt;&lt;SPAN&gt;&lt;STRONG&gt;SER1215&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt; | 03:00 PM | Room 10.18&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;STRONG&gt;Elevating your GROW with SAP journey with SAP Cloud ALM&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;EM&gt;Tonja Kehrer&lt;/EM&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;Discover how SAP Cloud ALM can accelerate your GROW with SAP journey with real-time insights, demo-driven learning, and innovation strategies for scalable growth.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;A href="https://www.sap.com/events/sapphire/flow/sap/sm25/catalog-inperson/page/catalog/session/1741233374809001THRG" target="_self" rel="noopener noreferrer"&gt;&lt;SPAN&gt;&lt;STRONG&gt;SER1214&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt; | 03:30 PM | Room 10.18&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;STRONG&gt;Thriving in holiday peaks: How AI transforms system resilience&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;EM&gt;Stefan Steinle, Suhaim Mohamed (Douglas)&lt;/EM&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;Explore how AI-driven holiday readiness safeguarded €11 billion in GMV for 143 customers. Learn how this initiative transforms system resilience and prepares your business for peak performance.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&lt;span class="lia-unicode-emoji" title=":round_pushpin:"&gt;📍&lt;/span&gt;&lt;/SPAN&gt;&lt;SPAN&gt;&lt;STRONG&gt;Ask the Expert Sessions – Tuesday, May 27 | Room 10.201&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;A href="https://www.sap.com/events/sapphire/flow/sap/sm25/catalog-inperson/page/catalog/session/1741233942773001fAzu" target="_self" rel="noopener noreferrer"&gt;&lt;SPAN&gt;&lt;STRONG&gt;SER1218&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt; | 01:30 PM&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;STRONG&gt;Maximizing business value with AI-driven support tools&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;EM&gt;Wilhelm Juette&lt;/EM&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;Explore how SAP Cloud ALM, Built-In Support, and the ITSM connector help you tailor support and drive value through intelligent automation.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;A href="https://www.sap.com/events/sapphire/flow/sap/sm25/catalog-inperson/page/catalog/session/1741234059084001HOm6" target="_self" rel="noopener noreferrer"&gt;&lt;SPAN&gt;&lt;STRONG&gt;SER1219&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt; | 02:00 PM&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;STRONG&gt;AI – From A to Impact: A case study to measure business value with AI&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;EM&gt;Wilhelm Juette&lt;/EM&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;See how SAP uses process mining and data insights to create a value measurement framework in the world of agentic AI.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;A href="https://www.sap.com/events/sapphire/flow/sap/sm25/catalog-inperson/page/catalog/session/1741234140043001kv1Y" target="_self" rel="noopener noreferrer"&gt;&lt;SPAN&gt;&lt;STRONG&gt;SER1220&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt; | 02:30 PM&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;STRONG&gt;Streamlining migration to SAP S/4HANA with lean selective data transition&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;EM&gt;Thorsten Spihlmann&lt;/EM&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;Get hands-on strategies using the SAP Business Transformation Center to simplify and automate your SAP S/4HANA migration while focusing on what matters most.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;A href="https://www.sap.com/events/sapphire/flow/sap/sm25/catalog-inperson/page/catalog/session/1741234264004001OH16" target="_self" rel="noopener noreferrer"&gt;&lt;SPAN&gt;&lt;STRONG&gt;SER1221&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt; | 03:00 PM&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;STRONG&gt;Safeguarding operations during the smooth transition to SAP S/4HANA Cloud&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;EM&gt;Mirja Kempin, Markus Winter&lt;/EM&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;Learn how to keep business continuity front and center during your move to SAP S/4HANA Cloud Private Edition.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;A href="https://www.sap.com/events/sapphire/flow/sap/sm25/catalog-inperson/page/catalog/session/1741234378182001exJr" target="_self" rel="noopener noreferrer"&gt;&lt;SPAN&gt;&lt;STRONG&gt;SER1222&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/A&gt;&lt;SPAN&gt; | 03:30 PM&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;STRONG&gt;Using AI and data-driven insights for a smooth customer support experience&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;&lt;EM&gt;Vivian Luechau-de la Roche&lt;/EM&gt;&lt;/SPAN&gt;&lt;BR /&gt;&lt;SPAN&gt;Explore how AI and tools like SAP for Me are streamlining support by surfacing relevant case history, resolution paths, and customer insights.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&lt;span class="lia-unicode-emoji" title=":direct_hit:"&gt;🎯&lt;/span&gt;&lt;/SPAN&gt;&lt;SPAN&gt;&lt;STRONG&gt;Why You Should Attend&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;From onboarding to migration, support to system resilience—these sessions are packed with real-world insight, customer voices, and practical frameworks to drive your transformation forward. They also highlight SAP’s ongoing commitment to intelligent, cloud-based innovation that delivers measurable outcomes.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&lt;span class="lia-unicode-emoji" title=":spiral_calendar:"&gt;🗓&lt;/span&gt;️ &lt;/SPAN&gt;&lt;SPAN&gt;&lt;STRONG&gt;Join us in Madrid!&lt;/STRONG&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;We can’t wait to meet you onsite—stop by our sessions, ask your toughest questions, and connect with our experts to explore how AI, automation, and cloud transformation can take your business to the next level.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Let’s build the future together at &lt;A href="https://www.sap.com/events/sapphire/madrid.html" target="_self" rel="noopener noreferrer"&gt;&lt;STRONG&gt;SAP Sapphire Madrid 2025&lt;/STRONG&gt;&lt;/A&gt;.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;#SAPSapphire #SAPCommunity #CustomerSupport #SAPCloudALM #AI #RISEwithSAP #GROWWITHSAP&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/unlock-the-power-of-cloud-ai-and-business-transformation-at-sap-sapphire/ba-p/14074005"/>
    <published>2025-04-12T09:23:10.254000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-blog-posts-by-sap/lets-talk-to-the-database-cap-amp-sqlite-powered-by-google-gemini-2-5-pro/ba-p/14077754</id>
    <title>Lets Talk to the Database - CAP &amp; SQLite Powered by Google Gemini-2.5-Pro</title>
    <updated>2025-04-16T14:08:37.441000+02:00</updated>
    <author>
      <name>ShivamShuklaSAP</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1859416</uri>
    </author>
    <content>&lt;P&gt;&lt;EM&gt;I am sure everyone is doing great !!!&lt;/EM&gt;&lt;/P&gt;&lt;P class="lia-align-justify" style="text-align : justify;"&gt;I am glad and excited to share my learning on Talk to Database , I mean just talk to the database using Natural Language powered by Google Gemini-2.5-pro-exp handled by SAP CAP programming.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="ShivamShuklaSAP_3-1744803613376.png" style="width: 434px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/251466i9EA1A1E439C25D98/image-dimensions/434x283?v=v2" width="434" height="283" role="button" title="ShivamShuklaSAP_3-1744803613376.png" alt="ShivamShuklaSAP_3-1744803613376.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;So what all we have here to achieve this end-to-end ?&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;We have access to Gemini-2.5-pro exprimetnal version available ( PAID ) Get your API Keys to interact through API Call&lt;/LI&gt;&lt;LI&gt;A CAP Application where we have deployed Sales Order header / Line Item / Deliveries Entities Deployed&lt;/LI&gt;&lt;LI&gt;Install SQLITE DB For Data Storage for all the 3 Tables&lt;/LI&gt;&lt;LI&gt;Add some Dummy data into your DB tables So that later we can validate data and interact with the database for fetching requested information and run select statement on top of that.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;Test out Gemini API using POSTMAN before you jump into this.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Data Prep:&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Extracted data from Sales Order Header (VBAK)&lt;/LI&gt;&lt;LI&gt;Extracted data from Sales Line Item (VBAP)&lt;/LI&gt;&lt;LI&gt;Extracted data from Delivery (LIPS)&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;&amp;nbsp;&lt;/STRONG&gt;&lt;STRONG&gt;Tasks Break Down:&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Prompt Prep&lt;/STRONG&gt; :&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;Prepare the prompt and provide enough context to generate SQL&lt;/LI&gt;&lt;LI&gt;Example &amp;nbsp;Prompt – Give me number of Sales Order in System&lt;/LI&gt;&lt;LI&gt;Prompt Specifies --&amp;nbsp; A Count is required from Sales Order Header table&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&lt;STRONG&gt;Context&lt;/STRONG&gt;&lt;SPAN&gt; -&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;Sales Order Header = SalesOrderH Table&lt;/LI&gt;&lt;LI&gt;Sales Order Item = SalesOrderItem&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&lt;STRONG&gt;Instruction – &lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;`You are a specialized SQL query assistant for a computer store database. Your primary goal is to answer user questions by retrieving data using the available tools.&lt;/P&gt;&lt;P&gt;Database Context:&lt;/P&gt;&lt;P&gt;The key tables you will interact with are:&lt;/P&gt;&lt;P&gt;&amp;nbsp; &amp;nbsp;'sp_prompts_Delivery': Contains Delivery Data.&lt;/P&gt;&lt;P&gt;&amp;nbsp; &amp;nbsp;'sp_prompts_SalesOrderHeader': Contains Sales Order Header Data (often referred to as just Sales Orders).&lt;/P&gt;&lt;P&gt;&amp;nbsp; &amp;nbsp;'sp_prompts_SalesOrderItem': Contains detailed Sales Order Item Data. `&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Identify Tables &lt;/STRONG&gt;:&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;Develop NodeJS API to extract tables names Specified&lt;/LI&gt;&lt;LI&gt;Develop NodeJS API to list all the tables:&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;Example&amp;nbsp; - &lt;SPAN&gt;&lt;A href="http://localhost:4004/odata/v4/my/listTables" target="_blank" rel="noopener nofollow noreferrer"&gt;http://localhost:4004/odata/v4/my/listTables&lt;/A&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P class="lia-align-center" style="text-align: center;"&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="ShivamShuklaSAP_4-1744804130786.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/251473i26E6BA17CF10B2DD/image-size/medium?v=v2&amp;amp;px=400" role="button" title="ShivamShuklaSAP_4-1744804130786.png" alt="ShivamShuklaSAP_4-1744804130786.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;STRONG&gt;Benefit&lt;/STRONG&gt; – this will help in finding out extract matching table while building the select query , will help in preparing the right context for model input&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Schema Extraction &lt;/STRONG&gt;: Develop Nodejs API to extract schema of tables ( Entities ) specified&lt;/P&gt;&lt;P&gt;Extract Schema for any give table input&lt;/P&gt;&lt;P&gt;Example –&lt;/P&gt;&lt;P&gt;&lt;A href="http://localhost:4004/odata/v4/my/describeDbTable" target="_blank" rel="noopener nofollow noreferrer"&gt;http://localhost:4004/odata/v4/my/describeDbTable&lt;/A&gt;&lt;/P&gt;&lt;P&gt;Content-Type: application/json&lt;/P&gt;&lt;P&gt;{&amp;nbsp;&amp;nbsp;&amp;nbsp;"tableName":"sp_prompts_SalesOrderHeader"&amp;nbsp; }&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="ShivamShuklaSAP_5-1744804236785.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/251476i2D5452B019C00C94/image-size/medium?v=v2&amp;amp;px=400" role="button" title="ShivamShuklaSAP_5-1744804236785.png" alt="ShivamShuklaSAP_5-1744804236785.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Benefit: &lt;/STRONG&gt;this will help in preparing schema for tables incase we need only few columns or specific column like Sales Order Types / Delivery Types etc.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Execute DB Query: &lt;/STRONG&gt;&amp;nbsp;This is the most important one once Model has generated the select statement for the input , this will execute the query and provide the response&lt;/P&gt;&lt;P&gt;Example – Prompt - SELECT * FROM sp_prompts_SalesOrderItem&lt;/P&gt;&lt;P&gt;Response :&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="ShivamShuklaSAP_6-1744804267933.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/251477iBE0A13AB39D37A7F/image-size/medium?v=v2&amp;amp;px=400" role="button" title="ShivamShuklaSAP_6-1744804267933.png" alt="ShivamShuklaSAP_6-1744804267933.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="ShivamShuklaSAP_7-1744804290070.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/251479i5A807A6998724290/image-size/medium?v=v2&amp;amp;px=400" role="button" title="ShivamShuklaSAP_7-1744804290070.png" alt="ShivamShuklaSAP_7-1744804290070.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Benefit: - &lt;/STRONG&gt;It will run the DB Statements and will return the results&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Prepare Response: &lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;This will come into picture in case we are requesting some information and expect the response in a plain english Language&lt;/P&gt;&lt;P&gt;Example – Give me no of Sales Order and Sales Item from System&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="ShivamShuklaSAP_8-1744804320192.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/251480i34FC5246135E18B2/image-size/medium?v=v2&amp;amp;px=400" role="button" title="ShivamShuklaSAP_8-1744804320192.png" alt="ShivamShuklaSAP_8-1744804320192.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="ShivamShuklaSAP_9-1744804320195.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/251481iA0268A8D9C44C3B4/image-size/medium?v=v2&amp;amp;px=400" role="button" title="ShivamShuklaSAP_9-1744804320195.png" alt="ShivamShuklaSAP_9-1744804320195.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;If you notice the generated sql then you will get the idea what happening behind the scene.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Generated SQL&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="ShivamShuklaSAP_10-1744804320199.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/251482i6B82B2E03E48CF08/image-size/medium?v=v2&amp;amp;px=400" role="button" title="ShivamShuklaSAP_10-1744804320199.png" alt="ShivamShuklaSAP_10-1744804320199.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;CAP Structure:&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;This is how my CAP Structure look like.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="ShivamShuklaSAP_11-1744804381662.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/251484i2E843A3BC29BA6E3/image-size/medium?v=v2&amp;amp;px=400" role="button" title="ShivamShuklaSAP_11-1744804381662.png" alt="ShivamShuklaSAP_11-1744804381662.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;QueryEngine&lt;/STRONG&gt; Contains the Main API Call to Gemini which subsequently calls all the functions required to get data back from Database like below.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;AI Model Used : &amp;nbsp;&lt;/STRONG&gt;Gemini-2.5-pro-exp&lt;/P&gt;&lt;P&gt;Initialize of GenAI Model like below.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="ShivamShuklaSAP_12-1744804407189.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/251485i142D09B94C423CD8/image-size/medium?v=v2&amp;amp;px=400" role="button" title="ShivamShuklaSAP_12-1744804407189.png" alt="ShivamShuklaSAP_12-1744804407189.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;If you see we have passed set of instructions that contains information about the database and tools are nothing the list of function which will give information about Tables and their Schema and later the final function for executing the generated query.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Dbquery.cds&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="ShivamShuklaSAP_13-1744804407201.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/251486i9B5C5B1C7F153E26/image-size/medium?v=v2&amp;amp;px=400" role="button" title="ShivamShuklaSAP_13-1744804407201.png" alt="ShivamShuklaSAP_13-1744804407201.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;This action will be responsible for prompt handing and it will return object of Information.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Dbquery.JS&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="ShivamShuklaSAP_14-1744804551408.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/251487i69DA416D2B273599/image-size/medium?v=v2&amp;amp;px=400" role="button" title="ShivamShuklaSAP_14-1744804551408.png" alt="ShivamShuklaSAP_14-1744804551408.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;This will handle all the incoming requests ---QueryDatabase is the one which will be triggered from outside rest of functions will be called internally , these are listed for standalone testing.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Note&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;I am also attaching a small Video Clip where you can see the execution of various prompts for technical implementation DM me directly , i can help you in building this POC.&lt;/P&gt;&lt;P&gt;&lt;div class="video-embed-center video-embed"&gt;&lt;iframe class="embedly-embed" src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FC00Ut0QeDng%3Ffeature%3Doembed&amp;amp;display_name=YouTube&amp;amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DC00Ut0QeDng&amp;amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FC00Ut0QeDng%2Fhqdefault.jpg&amp;amp;type=text%2Fhtml&amp;amp;schema=youtube" width="200" height="112" scrolling="no" title="CAP and SQLite Powered by Google Gemini 2.5 Pro" frameborder="0" allow="autoplay; fullscreen; encrypted-media; picture-in-picture;" allowfullscreen="true"&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;/P&gt;&lt;P&gt;#KeepLearningKeepSharing&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-blog-posts-by-sap/lets-talk-to-the-database-cap-amp-sqlite-powered-by-google-gemini-2-5-pro/ba-p/14077754"/>
    <published>2025-04-16T14:08:37.441000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/sap-teched-blog-posts/sap-teched-2025-slip-on-speed-up-the-next-decade-of-business/ba-p/14204442</id>
    <title>SAP TechEd 2025: 👟 Slip On, Speed Up: The Next Decade of Business</title>
    <updated>2025-09-02T14:16:17.305000+02:00</updated>
    <author>
      <name>lauramarwood</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/44830</uri>
    </author>
    <content>&lt;H3 id="toc-hId-1343831468" id="toc-hId-1888190302"&gt;The world is in sprint mode ...&lt;/H3&gt;&lt;P&gt;.... AI cycles flip every quarter, supply chains get bendy, climate shocks rewrite plans. As Lenin put it, "there are weeks when decades happen." Winning 2030 isn’t about hoarding data or demo sparkle. It’s about &lt;STRONG&gt;cutting friction&lt;/STRONG&gt;, &lt;STRONG&gt;staying focused&lt;/STRONG&gt;, and &lt;STRONG&gt;moving as one&lt;/STRONG&gt;.&lt;/P&gt;&lt;P&gt;SAP’s 2030 play:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;AI that removes grind -&amp;nbsp;&lt;/STRONG&gt;copilots, automation, anomaly detection.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Data you can trust -&amp;nbsp;&lt;/STRONG&gt;clean, connected, decision-ready.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Networks that sync the ecosystem -&amp;nbsp;&lt;/STRONG&gt;suppliers, partners, customers moving together&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;The metaphor? &lt;STRONG&gt;Lock laces.&lt;/STRONG&gt; Triathletes don’t stop to tie shoes. They slip on and go. In finance, AI is the lock lace: auto-recs, instant cleanup, fewer errors. Hours back. Focus up. Strategy on.&lt;/P&gt;&lt;P&gt;This is the shift: from tying shoes to breaking records. From reconciling ledgers to reshaping the business.&lt;/P&gt;&lt;P&gt;&lt;A href="https://www.sap.com/events/teched/berlin/flow/sap/te25/catalog-inperson/page/catalog/session/1753971252933001gofy" target="_self" rel="noopener noreferrer"&gt;Join &lt;STRONG&gt;Paul Saunders&lt;/STRONG&gt; and &lt;STRONG&gt;Laura Marwood Ph.D.&lt;/STRONG&gt;&lt;/A&gt;&amp;nbsp;at SAP TechEd 2025 to see how we’re building for that future - because in 2030, every second (and every decision) will count.&lt;BR /&gt;__________________________________&lt;BR /&gt;&lt;BR /&gt;&lt;STRONG&gt;Can't make it to Berlin? Dial in for SAP TechEd Virtual!&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;If you can’t make it to Berlin, we’ve still got you covered. &lt;A href="https://www.sap.com/events/teched/virtual.html" target="_blank" rel="noopener noreferrer"&gt;SAP TechEd Virtual&lt;/A&gt; is a free online event that’ll help you broaden your technical expertise and explore SAP’s latest innovations&lt;BR /&gt;&lt;BR /&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="asasa.jpg" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/307878iC9DEC57B450EDCDE/image-size/large?v=v2&amp;amp;px=999" role="button" title="asasa.jpg" alt="asasa.jpg" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/sap-teched-blog-posts/sap-teched-2025-slip-on-speed-up-the-next-decade-of-business/ba-p/14204442"/>
    <published>2025-09-02T14:16:17.305000+02:00</published>
  </entry>
</feed>
