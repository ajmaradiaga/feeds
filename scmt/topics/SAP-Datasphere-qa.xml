<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>https://raw.githubusercontent.com/ajmaradiaga/feeds/main/scmt/topics/SAP-Datasphere-qa.xml</id>
  <title>SAP Community - SAP Datasphere</title>
  <updated>2025-10-02T23:12:40.335460+00:00</updated>
  <link href="https://community.sap.com/t5/c-khhcw49343/SAP Datasphere/pd-p/73555000100800002141" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <subtitle>SAP Datasphere Q&amp;A in SAP Community</subtitle>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/sap-datasphere-data-flow-vs-transformation-flow/qaq-p/14220598</id>
    <title>sap datasphere: Data Flow vs Transformation Flow</title>
    <updated>2025-09-18T02:59:39.527000+02:00</updated>
    <author>
      <name>p_m8</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/401599</uri>
    </author>
    <content>&lt;DIV&gt;Hi,&lt;/DIV&gt;&lt;DIV&gt;&amp;nbsp;&lt;/DIV&gt;&lt;DIV&gt;Can anyone give some use cases or example as to when we should use Data Flow or Transformation Flow?&lt;/DIV&gt;&lt;DIV&gt;It is pretty similar to one another, both can transform data from source to target, with lookup/join to populate additional new column or field.&lt;/DIV&gt;&lt;DIV&gt;&amp;nbsp;&lt;/DIV&gt;&lt;DIV&gt;Thank you in advance.&lt;/DIV&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/sap-datasphere-data-flow-vs-transformation-flow/qaq-p/14220598"/>
    <published>2025-09-18T02:59:39.527000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/sap-datasphere-error-during-package-import/qaq-p/14221769</id>
    <title>SAP Datasphere Error during package import</title>
    <updated>2025-09-18T21:26:10.327000+02:00</updated>
    <author>
      <name>bohdan_kureshov</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/860702</uri>
    </author>
    <content>&lt;P&gt;Hello experts,&lt;/P&gt;&lt;P&gt;for our SAP Datasphere system we use export/import packages as a tool to transport changes from DEV system to PROD. We've already deployed some objects in PROD system. Now we need to transport some changes from DEV to PROD.And during package's import we have faced the error like "VIEW_NAME#USNAM can't be deleted because it's currently used by VIEW_NAME_2".&amp;nbsp; Both views are in this import package, "Dependency Included" mark is set, and both views are from the same space and . Package's option "&lt;SPAN&gt;Overwrite Preferences" was&amp;nbsp;&lt;/SPAN&gt;"&lt;SPAN&gt;Overwrite objects and data", and checkbox "Deploy after import" was selected.&amp;nbsp;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;So, I have 2 questions:&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;Is&amp;nbsp;#USNAM part is somehow meaningful? Because technical name of view doesn't contain that. Or it's just some internal system, stuff?&lt;BR /&gt;&lt;BR /&gt;&lt;/LI&gt;&lt;LI&gt;I don't understand what to do in this case because import package has both objects, so why system didn't overwrite it? Did I miss something?&lt;BR /&gt;Also in the same package we faced same issue, but that was a pair of a local table and replication flow, and to resolve that issue we have deleted RF from PROD system (anyway we had it in the same package), but it isn't looking as a correct way&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;&lt;STRONG&gt;UPD.&lt;/STRONG&gt;&amp;nbsp;VIEW_NAME#USNAM meaned that it was a field USNAM which is deleted from view, but still used by&amp;nbsp;VIEW_NAME_2.&lt;/P&gt;&lt;P&gt;Still don't understand why it didn't overwrite the object if dependent view was also in that package?&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="bohdan_kureshov_0-1758261844277.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/316785iFE819820E544DF11/image-size/medium?v=v2&amp;amp;px=400" role="button" title="bohdan_kureshov_0-1758261844277.png" alt="bohdan_kureshov_0-1758261844277.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/sap-datasphere-error-during-package-import/qaq-p/14221769"/>
    <published>2025-09-18T21:26:10.327000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/replication-stuck-how-to-remove-scheduled-replication/qaq-p/14223791</id>
    <title>replication stuck - how to remove scheduled replication</title>
    <updated>2025-09-22T10:06:58.504000+02:00</updated>
    <author>
      <name>bhat_vaidya2</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/206951</uri>
    </author>
    <content>&lt;P&gt;Hi,&lt;/P&gt;&lt;P&gt;I have stopped a replication flow and I am unable to remove it in LTRC. I cannot use stop button either and I have tried using&amp;nbsp;&lt;SPAN&gt;report IUUC_REPL_RESET_OBJECT but it didn't help either.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;I have tried several tables and created new Mass transfer Id but the result is same.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;How do I remove it from LTRC so that it can continue to replicate the table.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;thanks&lt;/SPAN&gt;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/replication-stuck-how-to-remove-scheduled-replication/qaq-p/14223791"/>
    <published>2025-09-22T10:06:58.504000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/importing-remote-tables-to-sap-datasphere/qaq-p/14223933</id>
    <title>Importing Remote Tables to SAP Datasphere</title>
    <updated>2025-09-22T11:33:25.526000+02:00</updated>
    <author>
      <name>KaleabGirma</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1996538</uri>
    </author>
    <content>&lt;P&gt;Hi experts,&lt;/P&gt;&lt;P&gt;I've set up a SAP HANA (Cloud) connection within Datasphere in order to import remote tables. The connection is&amp;nbsp;&lt;SPAN&gt;valid,&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;Data flows are enabled,&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;Replication flows are enabled and&amp;nbsp;&lt;/SPAN&gt;&lt;SPAN&gt;Remote tables are enabled.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;I am able to access the schemas,&amp;nbsp;import the remote tables&amp;nbsp;from my HANA data base and I see all the right columns within the Data Builder. When I want to view the data in the Data Viewer I get the message 'No data' in all my imported remote tables, even though there is data available in the respective source system (HANA). &lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;I've attached a screenshot of the message for reference.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Does anyone have a clue what the issue around this might be?&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;Best,&lt;/P&gt;&lt;P&gt;Kaleab&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="KaleabGirma_0-1758533513226.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/317806i6DAA36255C9ECD3D/image-size/medium?v=v2&amp;amp;px=400" role="button" title="KaleabGirma_0-1758533513226.png" alt="KaleabGirma_0-1758533513226.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/importing-remote-tables-to-sap-datasphere/qaq-p/14223933"/>
    <published>2025-09-22T11:33:25.526000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/spend-management-q-a/incorrect-url-in-code-for-mission-leverage-sap-ariba-data-model-in-sap/qaq-p/14225163</id>
    <title>Incorrect URL in code for mission Leverage SAP Ariba data model in SAP Datasphere</title>
    <updated>2025-09-23T08:37:46.328000+02:00</updated>
    <author>
      <name>Kalpeet</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1531204</uri>
    </author>
    <content>&lt;P&gt;&lt;SPAN&gt;The URL for extractSupplierRiskData is "/risk-exposure/v1/prod/suppliers/{smVendorId}/exposures" however the active verion for this API is v2. How to change the version in btp application?&lt;/SPAN&gt;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/spend-management-q-a/incorrect-url-in-code-for-mission-leverage-sap-ariba-data-model-in-sap/qaq-p/14225163"/>
    <published>2025-09-23T08:37:46.328000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/replication-flow-with-abap-connection-and-odp-sapi-0fi-ar-4-handling-large/qaq-p/14225281</id>
    <title>Replication Flow with ABAP Connection and ODP/SAPI 0FI_AR_4 – Handling Large Volumes</title>
    <updated>2025-09-23T10:45:24.081000+02:00</updated>
    <author>
      <name>A_Bracco</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1576708</uri>
    </author>
    <content>&lt;P&gt;&lt;SPAN&gt;Hi everyone,&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="2"&gt;we are planning to use in &lt;STRONG&gt;SAP Datasphere a Replication Flow based on an ABAP connection leveraging the ODP/SAPI extractor 0FI_AR_4.&lt;BR /&gt;The context is as follows:&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;FONT size="2"&gt;The data volume is very large (&lt;STRONG&gt;over 2 billion records).&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/LI&gt;&lt;LI&gt;&lt;FONT size="2"&gt;We would like to &lt;STRONG&gt;retrieve historical data from an old BW system.&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/LI&gt;&lt;LI&gt;&lt;FONT size="2"&gt;The goal is to perform some kind of &lt;STRONG&gt;initialization without transferring data from S/4, and then manage only the deltas.&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/LI&gt;&lt;LI&gt;&lt;FONT size="2"&gt;We cannot use a &lt;STRONG&gt;Replication Flow init+delta directly from S/4HANA, because the 0FI_AR_4 extractor without filters is too slow (we estimated about 20 days of extraction) and, based on our tests, it runs into memory errors.&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/LI&gt;&lt;LI&gt;&lt;FONT size="2"&gt;We have also noticed that the &lt;STRONG&gt;“Delta Only” option in Replication Flow does not seem to be applicable to our scenario.&lt;/STRONG&gt;&lt;/FONT&gt;&lt;P&gt;&lt;FONT size="2"&gt;thanks.&lt;/FONT&gt;&lt;/P&gt;&lt;/LI&gt;&lt;/UL&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/replication-flow-with-abap-connection-and-odp-sapi-0fi-ar-4-handling-large/qaq-p/14225281"/>
    <published>2025-09-23T10:45:24.081000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/datasphere-backup/qaq-p/14225829</id>
    <title>Datasphere backup</title>
    <updated>2025-09-23T15:32:27.689000+02:00</updated>
    <author>
      <name>Neyaz</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1927592</uri>
    </author>
    <content>&lt;P&gt;Hi All,&lt;/P&gt;&lt;P&gt;Can anyone tell the steps on how to restore backup of Datasphere tenant in case of any adverse scenario?&lt;/P&gt;&lt;P&gt;Thanks.&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/datasphere-backup/qaq-p/14225829"/>
    <published>2025-09-23T15:32:27.689000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/sap-datasphere-delta-table/qaq-p/14226347</id>
    <title>SAP Datasphere: Delta Table</title>
    <updated>2025-09-24T07:37:25.021000+02:00</updated>
    <author>
      <name>p_m8</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/401599</uri>
    </author>
    <content>&lt;P&gt;&amp;nbsp;Hi,&lt;/P&gt;&lt;P&gt;I have created a new Local Table with "Delta Capture" option On.&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;This will automatically create a new additional Delta table&amp;nbsp;this Local Table.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;But I can't find this Delta table in the repository, do you know where I can find it?&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Many thanks.&lt;/SPAN&gt;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/sap-datasphere-delta-table/qaq-p/14226347"/>
    <published>2025-09-24T07:37:25.021000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/ndc-converter-2-0-automating-your-journey-from-businessobjects-to-cloud/qaq-p/14226676</id>
    <title>NDC Converter 2.0: Automating Your Journey from BusinessObjects to Cloud Analytics</title>
    <updated>2025-09-24T11:39:08.590000+02:00</updated>
    <author>
      <name>JurajKysel4</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/2254330</uri>
    </author>
    <content>&lt;P&gt;SAP Business Data Cloud (BDC) provides a unified platform for data ingestion, semantic modeling, governance and analytics. For teams migrating from SAP BusinessObjects (BO), BDC enables:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;A governed semantic layer in SAP Datasphere that faithfully reproduces and extends your existing BO universes with standardized naming conventions, hierarchies and calculated measures.&lt;/LI&gt;&lt;LI&gt;Cloud-native analytics in SAC for reporting, planning and predictive scenarios, powered by either live data connections or scheduled replication jobs.&lt;/LI&gt;&lt;LI&gt;A clear modernization path for SAP BW/4HANA artifacts and non-SAP sources, preserving embedded business logic while consolidating disparate models.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;By porting Web Intelligence (WebI) content into BDC, you can retire legacy BO infrastructure and establish a sustainable foundation built on remote or replicated tables, graphical and SQL views and published semantic models that are fully consumable in SAC.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-center" image-alt="JurajKysel4_0-1758706198184.png" style="width: 739px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/319400i4235957EF39EDA56/image-dimensions/739x229?v=v2" width="739" height="229" role="button" title="JurajKysel4_0-1758706198184.png" alt="JurajKysel4_0-1758706198184.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;In this follow-up to our NDC Converter &lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/accelerate-migration-of-sap-businessobjects-web-intelligence-reports-to-sap/ba-p/13580516" target="_blank"&gt;introduction&lt;/A&gt;, we’ll showcase its newest capabilities and zoom in on how REST APIs, RPA and the SAP BDC stack work together to fully automate your migration workflow and eliminate uncertainty that is often associated with these sorts of projects.&lt;/P&gt;&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;Analysis &amp;amp; clean-up&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;Migration kicks off with a thorough source analysis powered by our enhanced scanning engine. After installing a lightweight Windows executable in your environment, NDC Converter uses the BusinessObjects REST API to perform a repository-wide scan - cataloging WebI documents, variables, filters and data sources. Parallel processing enables multiple report structures to be inventoried simultaneously, cutting traditional analysis times from days to hours.&lt;/P&gt;&lt;P&gt;If BO access is restricted, you can export a full content catalog instead and NDC Converter will generate a self-service metadata report for rapid sizing and compliance checks. These automated scans deliver hard metrics on:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Volume: total reports, pages, charts and objects&lt;/LI&gt;&lt;LI&gt;Complexity: calculations, variables, dimensions and interactive features&lt;/LI&gt;&lt;LI&gt;Usage: inactive or overlapping content&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;Armed with this detailed data, we can scope migration efforts accurately within days rather than weeks, giving prospects immediate insight into their BI landscape without manual, time-consuming analysis.&lt;/P&gt;&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;Automated report migration&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;Once WebI queries are identified, the next step is to align them with target system (usually SAP Datasphere but other options available too – for reference see previous blog &lt;A href="https://community.sap.com/t5/technology-blog-posts-by-sap/accelerate-migration-of-sap-businessobjects-web-intelligence-reports-to-sap/ba-p/13580516" target="_blank"&gt;post&lt;/A&gt;). Steps here include:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Linking each WebI query to its target system&lt;/LI&gt;&lt;LI&gt;Generating the necessary migration definitions&lt;/LI&gt;&lt;LI&gt;Automatically provisioning and deploying the corresponding tables&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;If you’re working with many tables, the “mass deploy” feature lets you push them live in bulk—no tedious one-by-one clicks.&lt;/P&gt;&lt;P&gt;After tables are deployed, we configure on-demand or scheduled data replication to maintain synchronization between target and source systems, followed by creating graphical and SQL-based views to implement calculation logic. The final step is to publish these views to SAP Analytics Cloud where up-to-date data can be accessed.&lt;/P&gt;&lt;P&gt;The data source layer migration involves analyzing the BO repository, migrating universe metadata and/or transitioning live models based on BEx queries. Upon completion of the initial data migration, a comprehensive semantic model is introduced that mirrors your original WebI structures including dimensions, measures, hierarchies and data. This transformation provides a cloud-ready foundation for all SAP Analytics Cloud initiatives.&lt;/P&gt;&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;NDC Converter 2.0&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;With your semantic model live, we proceed with WebI reports migration into SAP Analytics Cloud as interactive stories with new high functional coverage. NDC Converter retrieves each report’s layout, structures, tables, charts, filters and visual elements via the BusinessObjects REST API. A blueprint compatible with RPA processing is produced, covering:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Layout &amp;amp; structure&lt;/STRONG&gt;: multi‑page documents, IDs, descriptions, pages/tabs.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Tables &amp;amp; crosstabs&lt;/STRONG&gt;: calculated and restricted measures, variables and common WebI calculations.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Charts&lt;/STRONG&gt;: Bar/Column, Line, Numeric Point, Pie, Donut, Heat Map, Tree Map, Bubble, Scatterplot.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Formatting &amp;amp; UX&lt;/STRONG&gt;: predefined template stories, titles, text, headers/footers, icons.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Filters &amp;amp; interactivity&lt;/STRONG&gt;: input controls, query filters, single object filters, drill, hyperlinks.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;The RPA bot, delivered as a Chrome extension, reads this blueprint and automates the reconstruction of each report in SAC Story 2.0, preserving original tables, crosstabs, charts, parameters and filters. The resulting SAC Stories mirror your WebI reports while leveraging cloud-native data models.&lt;/P&gt;&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;What stays manual (and why)&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;NDC Converter automates routine migration tasks, allowing your team to focus on strategic design and governance – here are cases where automation does not bring added value:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;Data model (business semantics) design:&lt;/STRONG&gt; turning technical structures into robust semantic layer requires continuous alignment on reporting vision.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Edge cases &amp;amp; redesign:&lt;/STRONG&gt; in situations where 1:1 replication isn’t optimal, we propose a workaround solution to comply with business and technical needs.&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;Environment setup:&lt;/STRONG&gt; Migrating users, roles and platform configurations using a mix of guided best practices and selective automation.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;FONT size="5"&gt;&lt;STRONG&gt;Collaborate end-end delivery&lt;/STRONG&gt;&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;NDC Converter is an ideal tool for complex migration scenarios, automating the transfer of every structural element—from metadata and queries to filters and visual object definitions. Once the converter has recreated the report skeletons in SAC, your BI team can take over the final steps: refining layouts, adjusting chart properties and validating calculation logic. This collaborative workflow ensures that the most time-consuming technical migrations are handled automatically, while you retain full control over presentation, branding and business requirements.&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/ndc-converter-2-0-automating-your-journey-from-businessobjects-to-cloud/qaq-p/14226676"/>
    <published>2025-09-24T11:39:08.590000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/sap-datasphere-trial-resources-for-excercises-missing/qaq-p/14227218</id>
    <title>SAP Datasphere trial, resources for excercises missing</title>
    <updated>2025-09-24T21:21:33.963000+02:00</updated>
    <author>
      <name>StefaniaF</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1594865</uri>
    </author>
    <content>&lt;P&gt;Create Opportunity Fact View excercise: Resource MCT Opportunity items missing, such resource is not shared from the central data space as suggested by the workbook, nor available in the local space repository.&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/sap-datasphere-trial-resources-for-excercises-missing/qaq-p/14227218"/>
    <published>2025-09-24T21:21:33.963000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/sap-datasphere-transformation-flow-init-and-delta/qaq-p/14227368</id>
    <title>SAP Datasphere Transformation Flow, Init and Delta</title>
    <updated>2025-09-25T06:22:39.345000+02:00</updated>
    <author>
      <name>p_m8</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/401599</uri>
    </author>
    <content>&lt;DIV&gt;Hi,&lt;/DIV&gt;&lt;DIV&gt;&amp;nbsp;&lt;/DIV&gt;&lt;DIV&gt;1. In my Local Table (Employee Source), I have the Delta Capture option ON.&lt;/DIV&gt;&lt;DIV&gt;&lt;DIV&gt;2. I use this as source table in my Transformation Flow and loaded into target table (Employee Target). I chose to load from table Delta Capture.&lt;/DIV&gt;&lt;DIV&gt;3. In the Employee Target table, I can see all records updated correctly, the field Change Type = I, Change Date = Sep 25, 2025, 3:51:01&amp;nbsp;&lt;/DIV&gt;&lt;DIV&gt;4. Now in Local Table (Employee Source), I updated one record only - Employee 123.&lt;/DIV&gt;&lt;DIV&gt;5. Run data load again via Transformation Flow, again, all records updated. I can see the Change Date field updated with current run date/time for all the records.&lt;/DIV&gt;&lt;DIV&gt;6. Why are all records updated again? I thought it is only Employee 123 will be updated as I chose load from table Delta Capture, not All Active Records??&lt;/DIV&gt;&lt;DIV&gt;&amp;nbsp;&lt;/DIV&gt;&lt;DIV&gt;Please advise, thanks.&lt;/DIV&gt;&lt;/DIV&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/sap-datasphere-transformation-flow-init-and-delta/qaq-p/14227368"/>
    <published>2025-09-25T06:22:39.345000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/count-not-returned-anymore-in-odata-calls-to-analytical-models-in-sap/qaq-p/14227580</id>
    <title>$count not returned anymore in OData calls to analytical models in SAP Datasphere</title>
    <updated>2025-09-25T09:57:26.045000+02:00</updated>
    <author>
      <name>JaumeG</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1487649</uri>
    </author>
    <content>&lt;P&gt;Hi everyone,&lt;/P&gt;&lt;P&gt;Until last week, our OData calls to analytical models in SAP Datasphere returned the $count value when using $count=true. Since this week, the requests still work but no longer return any count value.&lt;/P&gt;&lt;P&gt;Has anyone else noticed this change? Do you know if something changed recently in Datasphere OData services regarding $count support or if we need to adapt our queries?&lt;/P&gt;&lt;P&gt;Thanks!&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/count-not-returned-anymore-in-odata-calls-to-analytical-models-in-sap/qaq-p/14227580"/>
    <published>2025-09-25T09:57:26.045000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/questions-on-sap-bdc-integration-scenarios-on-premise-car-ewm-performance/qaq-p/14227914</id>
    <title>Questions on SAP BDC Integration Scenarios (On-Premise, CAR/EWM, Performance)</title>
    <updated>2025-09-25T15:07:13.347000+02:00</updated>
    <author>
      <name>benhaddou</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/891003</uri>
    </author>
    <content>&lt;P&gt;Hello,&lt;/P&gt;&lt;P&gt;I have the following questions regarding SAP BDC and would appreciate any insights:&lt;/P&gt;&lt;OL class="lia-list-style-type-upper-alpha"&gt;&lt;LI&gt;What are the available options to connect SAP BDC with an &lt;STRONG&gt;on-premise SAP ERP&lt;/STRONG&gt;&amp;nbsp; system? Are there recommended best practices for such scenarios?&lt;/LI&gt;&lt;LI&gt;Are SAP-managed Data Products in SAP BDC available for &lt;STRONG&gt;on-premise S/4HANA systems&lt;/STRONG&gt;, or are on-premise customers limited to customer-managed Data Products only?&lt;/LI&gt;&lt;LI&gt;Is it already possible to integrate data from on-premise SAP solutions such as &lt;STRONG&gt;SAP CAR or SAP EWM&lt;/STRONG&gt; into SAP BDC, or is current support mainly focused on cloud solutions like Ariba or SuccessFactors?&lt;/LI&gt;&lt;LI&gt;Are there known performance considerations when integrating data via &lt;STRONG&gt;OData&lt;/STRONG&gt; directly or &lt;STRONG&gt;SAP Datasphere&lt;/STRONG&gt; as the integration layer? If so, what are the best practices for each approach?&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;Many thanks in advance for your support!&lt;/P&gt;&lt;P&gt;Best regards&lt;/P&gt;&lt;P&gt;Thami&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/questions-on-sap-bdc-integration-scenarios-on-premise-car-ewm-performance/qaq-p/14227914"/>
    <published>2025-09-25T15:07:13.347000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/how-to-use-parameter-in-sap-datasphere-task-chain/qaq-p/14229693</id>
    <title>How to use parameter in SAP Datasphere Task Chain</title>
    <updated>2025-09-28T09:13:24.244000+02:00</updated>
    <author>
      <name>Nilesh92</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1515424</uri>
    </author>
    <content>&lt;P&gt;Hello Experts,&lt;/P&gt;&lt;P&gt;With the latest update, we got a parameterized Task chain in SAP Datasphere.&lt;/P&gt;&lt;P&gt;I have been struggling with using the parameter that i have created in one of a task chain.&lt;/P&gt;&lt;P&gt;use case:&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;I have a parameterized data flow.&lt;/LI&gt;&lt;LI&gt;I have created a task chain with a parameter created in it.&lt;/LI&gt;&lt;LI&gt;I have added the dataflow from step 1 in this task chain.&lt;/LI&gt;&lt;LI&gt;Now i want to pass the Task chain parameter in the dataflow.&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;Can someone please help me with this?&lt;/P&gt;&lt;P&gt;&lt;a href="https://community.sap.com/t5/c-khhcw49343/SAP+Datasphere/pd-p/73555000100800002141" class="lia-product-mention" data-product="16-1"&gt;SAP Datasphere&lt;/a&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/how-to-use-parameter-in-sap-datasphere-task-chain/qaq-p/14229693"/>
    <published>2025-09-28T09:13:24.244000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/transformation-flow-init-and-delta/qaq-p/14229927</id>
    <title>Transformation Flow, Init and Delta</title>
    <updated>2025-09-29T07:00:12.853000+02:00</updated>
    <author>
      <name>p_m8</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/401599</uri>
    </author>
    <content>&lt;DIV&gt;Hi,&lt;/DIV&gt;&lt;DIV&gt;&amp;nbsp;&lt;/DIV&gt;&lt;DIV&gt;1. In my Local Table (Employee Source), I have the Delta Capture option ON.&lt;/DIV&gt;&lt;DIV&gt;&lt;DIV&gt;2. I use this as source table in my Transformation Flow and loaded into target table (Employee Target). I chose to load from table Delta Capture.&lt;/DIV&gt;&lt;DIV&gt;3. In the Employee Target table, I can see all records updated correctly, the field Change Type = I, Change Date = Sep 25, 2025, 3:51:01&amp;nbsp;&lt;/DIV&gt;&lt;DIV&gt;4. Now in Local Table (Employee Source), I updated one record only - Employee 123.&lt;/DIV&gt;&lt;DIV&gt;5. Run data load again via Transformation Flow, again, all records updated. I can see the Change Date field updated with current run date/time for all the records.&lt;/DIV&gt;&lt;DIV&gt;6. Why are all records updated again? I thought it is only Employee 123 will be updated as I chose load from table Delta Capture, not All Active Records??&lt;/DIV&gt;&lt;DIV&gt;&amp;nbsp;&lt;/DIV&gt;&lt;DIV&gt;Please advise, thanks.&lt;/DIV&gt;&lt;/DIV&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/transformation-flow-init-and-delta/qaq-p/14229927"/>
    <published>2025-09-29T07:00:12.853000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/delta-table/qaq-p/14229928</id>
    <title>Delta Table</title>
    <updated>2025-09-29T07:02:39.859000+02:00</updated>
    <author>
      <name>p_m8</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/401599</uri>
    </author>
    <content>&lt;P&gt;&amp;nbsp;Hi,&lt;/P&gt;&lt;P&gt;I have created a new Local Table with "Delta Capture" option On.&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;This will automatically create a new additional Delta table&amp;nbsp;this Local Table.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;But I can't find this Delta table in the repository, do you know where I can find it?&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Many thanks.&lt;/SPAN&gt;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/delta-table/qaq-p/14229928"/>
    <published>2025-09-29T07:02:39.859000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/missing-technical-link-from-datasphere-remote-tables-to-source-system/qaq-p/14230586</id>
    <title>Missing technical link from Datasphere remote tables to source system</title>
    <updated>2025-09-29T15:38:54.006000+02:00</updated>
    <author>
      <name>erik_pedersen</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/500541</uri>
    </author>
    <content>&lt;P&gt;Hi experts,&lt;/P&gt;&lt;P&gt;I need to find the link between a select on a remote table in datasphere and the similar select on the source system. I am using DP Agent as connector between Datasphere and in this case a BW/4Hana system.&lt;/P&gt;&lt;P&gt;I can see the SQL select towards the source system in the M_REMOTE_STATEMENTS in datasphere - no problem. But how can I find the unique link to the SQL executed on the source system? By comparing starting time of the statement in Datasphere and source system I can actually find a kind of link when looking into M_EXPENSIVE_STATEMENTS on the source system.&lt;/P&gt;&lt;P&gt;I have tried to connect the 2 statements through transaction_id and/or Connection_id, but without any success. I think I need to access other interface tables, but which ones?&lt;/P&gt;&lt;P&gt;Any Good Ideas,&lt;/P&gt;&lt;P&gt;Regards&lt;/P&gt;&lt;P&gt;Erik Pedersen&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/missing-technical-link-from-datasphere-remote-tables-to-source-system/qaq-p/14230586"/>
    <published>2025-09-29T15:38:54.006000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/how-to-create-export-a-csv-file-from-datasphere/qaq-p/14230939</id>
    <title>How to create/export a CSV file from Datasphere?</title>
    <updated>2025-09-29T21:23:44.854000+02:00</updated>
    <author>
      <name>RafaelBizzotto</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/2059878</uri>
    </author>
    <content>&lt;P&gt;Hi all,&lt;/P&gt;&lt;P&gt;I need to create a CSV file based in a table from Datasphere.&lt;/P&gt;&lt;P&gt;In the past we had a SAP BW sytems which was responsible to export some data saving it in a local share drive by a CSV file.&lt;/P&gt;&lt;P&gt;Is there any option for creating a CSV from Datasphere?&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/how-to-create-export-a-csv-file-from-datasphere/qaq-p/14230939"/>
    <published>2025-09-29T21:23:44.854000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/sap-datasphere-local-file-table-merge-with-custom-application-goes-into/qaq-p/14232394</id>
    <title>SAP Datasphere: local file table Merge with custom application goes into error</title>
    <updated>2025-10-01T10:44:32.848000+02:00</updated>
    <author>
      <name>albertosimeoni</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/829608</uri>
    </author>
    <content>&lt;P&gt;Hello,&lt;/P&gt;&lt;P&gt;when I try to do a merge of a local table (file), with a "spark application" different from the standard (300).&lt;/P&gt;&lt;P&gt;I get an error from the SQL stored procedure that invoke the merge file task.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="albertosimeoni_0-1759307928893.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/321907iEBC86A27319BD43C/image-size/large/is-moderation-mode/true?v=v2&amp;amp;px=999" role="button" title="albertosimeoni_0-1759307928893.png" alt="albertosimeoni_0-1759307928893.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;I am on a TDD Datasphere tenant with Object store activated through the new procedure via the correct ticket yesterday.&lt;/P&gt;&lt;P&gt;Do you also experience this problems with tasks invoked with different applicaiton than the standard defined in the space?&lt;/P&gt;&lt;P&gt;Have you got any workaround? I want to test which is the correct resource allocation to minimize compute costs in terms of CU,&lt;/P&gt;&lt;P&gt;for now seems to me a very high cost for a serverless applicaiton. As it is, it seems that the data lake it costs more than having the data "in memory" for my test (1GB table ~ 140Mi rows).&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Thanks,&lt;/P&gt;&lt;P&gt;Alberto&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/sap-datasphere-local-file-table-merge-with-custom-application-goes-into/qaq-p/14232394"/>
    <published>2025-10-01T10:44:32.848000+02:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/update-er-model/qaq-p/14233211</id>
    <title>Update ER Model</title>
    <updated>2025-10-02T09:02:29.686000+02:00</updated>
    <author>
      <name>p_m8</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/401599</uri>
    </author>
    <content>&lt;P&gt;Hi,&lt;/P&gt;&lt;P&gt;I have created a new ER Model with following:&lt;/P&gt;&lt;P&gt;Billing Transaction (Fact)&lt;/P&gt;&lt;P&gt;Customer (Dimension)&lt;/P&gt;&lt;P&gt;This is saved and deployed. Analytical Model created and deployed successfully on this.&lt;/P&gt;&lt;P&gt;Now I have updated the ER Model with new Store Dimension. I saved and deployed the ER Model successfully. However, this dimension is not available in my existing Analytical Model.&lt;/P&gt;&lt;P&gt;How can I propagate the Store Dimension into my existing Analytical Model?&lt;/P&gt;&lt;P&gt;Please advise, thank you.&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/update-er-model/qaq-p/14233211"/>
    <published>2025-10-02T09:02:29.686000+02:00</published>
  </entry>
</feed>
