<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>https://raw.githubusercontent.com/ajmaradiaga/feeds/main/scmt/topics/SAP-Datasphere-qa.xml</id>
  <title>SAP Community - SAP Datasphere</title>
  <updated>2026-01-01T00:12:35.475595+00:00</updated>
  <link href="https://community.sap.com/t5/c-khhcw49343/SAP Datasphere/pd-p/73555000100800002141" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <subtitle>SAP Datasphere Q&amp;A in SAP Community</subtitle>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/bdc-core-cockpit-missing-in-sap4me/qaq-p/14273098</id>
    <title>BDC Core cockpit missing in SAP4ME?</title>
    <updated>2025-11-19T22:15:00.341000+01:00</updated>
    <author>
      <name>sap_newbie</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1398694</uri>
    </author>
    <content>&lt;P&gt;I don't see it in Portfolio and Products. The tile seems to be missing. I'm checking as a super admin.&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/bdc-core-cockpit-missing-in-sap4me/qaq-p/14273098"/>
    <published>2025-11-19T22:15:00.341000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/executing-dynamic-sql-from-a-parameterized-view-in-sap-datasphere/qaq-p/14273510</id>
    <title>Executing Dynamic SQL from a Parameterized View in SAP Datasphere</title>
    <updated>2025-11-20T10:21:10.616000+01:00</updated>
    <author>
      <name>MayuraNalawade</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/2124059</uri>
    </author>
    <content>&lt;P&gt;Hello SAP Community,&lt;/P&gt;&lt;P&gt;I am working with SAP Datasphere and facing challenges executing dynamic SQL statements that are generated and stored in a parameterized view. I wanted to share my problem in detail and clarify the limitations I encountered, in case it helps others.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Scenario&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;I have a parameterized view DQ_RULE_GENERATED_VW that generates SQL statements dynamically based on some input parameters.&lt;/P&gt;&lt;P&gt;The view returns a column called GENERATED_SQL along with other metadata.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="MayuraNalawade_0-1763630514847.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/343053i7D617515D6025628/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="MayuraNalawade_0-1763630514847.png" alt="MayuraNalawade_0-1763630514847.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;My goal is to execute these SQL statements dynamically within SAP Datasphere and capture the results in a table for downstream processing.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Challenges&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;1. Script Operators in Data Flows cannot execute arbitrary SQL:&lt;/P&gt;&lt;OL&gt;&lt;UL&gt;&lt;LI&gt;Datasphere Python Script Operators only process DataFrames from upstream sources.&lt;/LI&gt;&lt;LI&gt;You cannot call a parameterized view directly or run SQL dynamically using sql.execute_query inside a Script Operator.&lt;/LI&gt;&lt;/UL&gt;&lt;/OL&gt;&lt;P&gt;2. Parameterized views cannot be used as sources:&lt;/P&gt;&lt;OL&gt;&lt;UL&gt;&lt;LI&gt;Script Operators and many Datasphere operators only accept materialized tables or non-parameterized views.&lt;/LI&gt;&lt;/UL&gt;&lt;/OL&gt;&lt;P&gt;3. No direct write access to space-owned tables:&lt;/P&gt;&lt;OL&gt;&lt;UL&gt;&lt;LI&gt;Even if I fetch results externally (e.g., with a Python script using a HANA connection), I cannot write the results back into Datasphere UI due to lack of privileges on space-owned tables.&lt;/LI&gt;&lt;LI&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="MayuraNalawade_1-1763630558284.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/343054i7556FB52D0758277/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="MayuraNalawade_1-1763630558284.png" alt="MayuraNalawade_1-1763630558284.png" /&gt;&lt;/span&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;I tried writing a stored procedure that executes the SQL statements dynamically, but it cannot write the results into the target table for the same reason. It seems DML operations can only be performed inside the Datasphere UI, not externally.&lt;/LI&gt;&lt;LI&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="MayuraNalawade_2-1763630579587.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/343055i5BBD013C2A5FF688/image-size/medium/is-moderation-mode/true?v=v2&amp;amp;px=400" role="button" title="MayuraNalawade_2-1763630579587.png" alt="MayuraNalawade_2-1763630579587.png" /&gt;&lt;/span&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;/OL&gt;&lt;P&gt;4. External execution works but is not automated:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;I was able to fetch the GENERATED_SQL column using an external Python script, execute each statement dynamically, and get the results.&lt;/LI&gt;&lt;LI&gt;However, I cannot send the results back into Datasphere without write access.&lt;/LI&gt;&lt;LI&gt;Exporting results as CSV and manually uploading them is possible, but I would like a fully automated solution without manual steps.&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Request for Community Input&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;I would like to know:&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;Is there a fully supported way in Datasphere to execute SQL dynamically from a parameterized view?&lt;/LI&gt;&lt;LI&gt;Are there recommended best practices for dynamically executing rules generated from parameterized views in Datasphere, especially when you don’t have write access to space-owned tables?&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;Any guidance or insights would be greatly appreciated!&lt;/P&gt;&lt;P&gt;Thanks,&lt;BR /&gt;Mayura&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/executing-dynamic-sql-from-a-parameterized-view-in-sap-datasphere/qaq-p/14273510"/>
    <published>2025-11-20T10:21:10.616000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/datasphere-why-we-cannot-replicate-data-in-delta-from-co-pa-datasource/qaq-p/14273744</id>
    <title>Datasphere : Why we cannot replicate data in delta from CO-PA Datasource ?</title>
    <updated>2025-11-20T13:39:56.952000+01:00</updated>
    <author>
      <name>vincentsgs</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/600003</uri>
    </author>
    <content>&lt;P&gt;I try to create a replication flow from the standard ECC CO-PA datasource to Datasphere, I cannot set the delta mode because of this message :&lt;/P&gt;&lt;P&gt;"&lt;SPAN&gt;Replicating ABAP-based objects that do not have a primary key is only possible for load type "Initial Only".&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Any idea ? I suppose the CO-PA table did have a primary key !&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;Thanks.&lt;/P&gt;&lt;P&gt;Vincent.&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/datasphere-why-we-cannot-replicate-data-in-delta-from-co-pa-datasource/qaq-p/14273744"/>
    <published>2025-11-20T13:39:56.952000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/how-to-calculate-cu-consumption-and-cost-for-a-specific-table-in-sap/qaq-p/14275481</id>
    <title>How to calculate CU consumption and cost for a specific table in SAP Datasphere?</title>
    <updated>2025-11-23T17:35:47.176000+01:00</updated>
    <author>
      <name>Sivaramakrishnan_Manickam</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/874631</uri>
    </author>
    <content>&lt;P&gt;I want to understand how to estimate the &lt;STRONG&gt;Capacity Unit (CU) usage and monthly cost&lt;/STRONG&gt; for a specific table stored in SAP Datasphere / SAP HANA Cloud.&lt;/P&gt;&lt;P&gt;For example, I have a table with the following details&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;SPAN class=""&gt;Size On Disk (MiB)&lt;/SPAN&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;DIV class=""&gt;&lt;SPAN class=""&gt;0.76&lt;/SPAN&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;SPAN class=""&gt;Size In-Memory (MiB)&lt;/SPAN&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;DIV class=""&gt;&lt;SPAN class=""&gt;0.6&lt;/SPAN&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;SPAN class=""&gt;Number of Records&lt;/SPAN&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;DIV class=""&gt;&lt;SPAN class=""&gt;5844&lt;/SPAN&gt;&lt;/DIV&gt;&lt;DIV class=""&gt;&amp;nbsp;&lt;/DIV&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;SPAN&gt;Time Table&lt;/SPAN&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;DIV class=""&gt;&amp;nbsp;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;DIV class=""&gt;&lt;SPAN&gt;SAP.TIME.M_TIME_DIMENSION&lt;/SPAN&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/DIV&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;I’m trying to determine:&lt;/P&gt;&lt;OL&gt;&lt;LI&gt;&lt;P&gt;How to calculate the &lt;STRONG&gt;CU consumption per hour&lt;/STRONG&gt; and &lt;STRONG&gt;per month&lt;/STRONG&gt; for this table.&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;&lt;P&gt;How to convert the CU usage into &lt;STRONG&gt;cost&lt;/STRONG&gt;, using the standard pricing (e.g., 1 CU = USD 0.87 per month).&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;&lt;P&gt;Whether compute memory or storage contributes more to the total CU usage.&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;&lt;P&gt;If there is an official formula or documentation for calculating CU at a per-table level.&lt;/P&gt;&lt;/LI&gt;&lt;/OL&gt;&lt;P&gt;What is the recommended method or formula for estimating CU consumption and cost for an individual table in SAP Datasphere?&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/how-to-calculate-cu-consumption-and-cost-for-a-specific-table-in-sap/qaq-p/14275481"/>
    <published>2025-11-23T17:35:47.176000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/excel-on-share-point-as-a-datasource-for-datasphere/qaq-p/14275678</id>
    <title>Excel on Share-point as a datasource for Datasphere</title>
    <updated>2025-11-24T08:25:42.861000+01:00</updated>
    <author>
      <name>niroshinip</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/556204</uri>
    </author>
    <content>&lt;P&gt;We have many requirements where Excel documents stored in SharePoint needs to be replicated to Datasphere as local or remote table. Since the excel data is constantly changing,&amp;nbsp; the data has to be replicated at least daily. Since there is no native sharepoint connector We have been managing through an open connector on SAP BTP integration suite. However this method is mentioned as a deprecated service from 2026. What are the alternative options we can use?&lt;/P&gt;&lt;P&gt;Thank you.&lt;/P&gt;&lt;P&gt;Niroshini&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/excel-on-share-point-as-a-datasource-for-datasphere/qaq-p/14275678"/>
    <published>2025-11-24T08:25:42.861000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/cds-view-metadata-to-sap-datasphere-in-replication-flows/qaq-p/14277265</id>
    <title>CDS view metadata to SAP Datasphere in replication flows</title>
    <updated>2025-11-25T14:00:41.917000+01:00</updated>
    <author>
      <name>lars_oland</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/547133</uri>
    </author>
    <content>&lt;P&gt;Hi&lt;/P&gt;&lt;P&gt;I am currently trying to get metadata from cds views in replication flows for the logon language of the connection to S4. Even though the base tables/remote tables displays text in logon language it is not the same case with CDS views.&lt;/P&gt;&lt;P&gt;Do anybody know a way to get metadata in other languages than EN to sap datasphere ?&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/cds-view-metadata-to-sap-datasphere-in-replication-flows/qaq-p/14277265"/>
    <published>2025-11-25T14:00:41.917000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/ytd-calculation-in-restricted-calculated-measure-with-200-dimensions-in-sap/qaq-p/14277994</id>
    <title>YTD Calculation in Restricted/Calculated Measure with 200 Dimensions in SAP DataSphere</title>
    <updated>2025-11-26T11:12:02.888000+01:00</updated>
    <author>
      <name>Alejandro</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/17403</uri>
    </author>
    <content>&lt;P&gt;&lt;STRONG&gt;Context and Goal&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;We are building an &lt;STRONG&gt;Analytic Model in SAP Datasphere&lt;/STRONG&gt;, based on a view with approximately 200 columns/dimensions. Our goal is to calculate the Year-to-Date (YTD) value for the measure "SalesAmount". The YTD calculation must be dynamic, allowing the end-user to analyze the result by Year, Month, and any of the ~198 remaining dimensions in the model (example: Customer, Product, Region, etc.).&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Attempted Solutions and Specific Issue&lt;/STRONG&gt;&lt;BR /&gt;&amp;nbsp;We have tried to implement the YTD calculation directly within the Analytic Model, following SAP documentation:&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;1. Restricted Measure Attempt:&amp;nbsp;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;STRONG&gt;Expression&lt;/STRONG&gt;: We defined a restricted measure for "SalesAmount" with the following filter condition on the date dimension ("SalesDate"):&amp;nbsp;&amp;nbsp;&lt;SPAN&gt;SalesDate IN (YTD(SalesDate))&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp;Result:&lt;/STRONG&gt; The model &lt;STRONG&gt;fails to activate&lt;/STRONG&gt;. Upon deployment, Datasphere throws the following syntax error:&lt;/P&gt;&lt;P&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;EM&gt; &amp;nbsp; &amp;nbsp;Reason: Invalid syntax or usage of unsupported function. unexpected character: -&amp;gt;Y&amp;lt;- at offset: 0, skipped 3 characters.&lt;/EM&gt;&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Observation&lt;/STRONG&gt;: The YTD() function does not seem to be recognized or supported in this context/syntax, despite being suggested by the expression editor.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;2. Calculated Measure Attempt:&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;P&gt;&lt;STRONG&gt;Expression&lt;/STRONG&gt;: We also tried creating a calculated measure using the YTD() function:&amp;nbsp;&amp;nbsp;&lt;SPAN&gt;YTD(SalesAmount, SalesDate)&lt;/SPAN&gt;&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;&lt;P&gt;&lt;STRONG&gt;Result:&lt;/STRONG&gt; The outcome is the same; the model &lt;STRONG&gt;fails to activate&lt;/STRONG&gt; and shows the identical syntax error related to the 'Y' character.&lt;/P&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;3. SQL View Implementation (Window Function):&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;P&gt;&lt;STRONG&gt;Approach:&lt;/STRONG&gt; We tried calculating the running total using the &lt;STRONG&gt;&lt;CODE&gt;OVER (PARTITION BY Year ORDER BY Month)&lt;/CODE&gt;&lt;/STRONG&gt; window function within the source view (View Builder).&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;&lt;P&gt;&lt;STRONG&gt;Limitation:&lt;/STRONG&gt; This approach is &lt;STRONG&gt;unworkable&lt;/STRONG&gt; because it &lt;STRONG&gt;limits the drill-down capability&lt;/STRONG&gt;. The calculation is performed only at the Year/Month level, and since it is a calculated column in the view, we cannot dynamically disaggregate the YTD by the other ~198 dimensions in the model (e.g., if we filter by a specific "&lt;SPAN class=""&gt;Customer"&lt;/SPAN&gt;, the calculated YTD would still be the total across all customers up to that "&lt;SPAN class=""&gt;Month"&lt;/SPAN&gt;).&lt;/P&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;STRONG&gt;4. SAC (SAP Analytics Cloud) - Import Model Test:&lt;/STRONG&gt;&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;P&gt;&lt;STRONG&gt;Approach:&lt;/STRONG&gt; We created an &lt;STRONG&gt;Import Model&lt;/STRONG&gt; in SAC, and there, the &lt;STRONG&gt;YTD()&lt;/STRONG&gt; function (implemented as a calculated account) &lt;STRONG&gt;works perfectly&lt;/STRONG&gt; and allows for dynamic drill-down across all dimensions.&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;&lt;P&gt;&lt;STRONG&gt;Limitation:&lt;/STRONG&gt; This forces us to &lt;STRONG&gt;lose the Real-Time capability&lt;/STRONG&gt; of SAP Datasphere and the Live Connection, which is an unacceptable trade-off for our use case.&lt;/P&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;H3 id="toc-hId-1894749284"&gt;&lt;STRONG&gt;Question to the Community&lt;/STRONG&gt;&lt;/H3&gt;&lt;P&gt;&lt;STRONG&gt;Is there an alternative, supported syntax or method in SAP Datasphere, within the &lt;I&gt;Analytic Consumption Model&lt;/I&gt; (or a dynamically aggregate-aware &lt;I&gt;View Builder&lt;/I&gt; approach) to calculate YTD while respecting all dimensions in the model?&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;Any &lt;I&gt;workaround&lt;/I&gt; or confirmation on whether the &lt;CODE&gt;YTD()&lt;/CODE&gt; function in Restricted or Calculated Measures is &lt;STRONG&gt;actually supported&lt;/STRONG&gt; in the current version of Datasphere would be highly appreciated.&lt;/P&gt;&lt;P&gt;Thank you in advance for your assistance!&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/ytd-calculation-in-restricted-calculated-measure-with-200-dimensions-in-sap/qaq-p/14277994"/>
    <published>2025-11-26T11:12:02.888000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/datasphere-sql-expected-unsigned-integer-error-on-tinyint/qaq-p/14280768</id>
    <title>Datasphere SQL: expected_unsigned_integer error on tinyint</title>
    <updated>2025-12-01T08:04:35.051000+01:00</updated>
    <author>
      <name>mitko1994</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/548155</uri>
    </author>
    <content>&lt;P&gt;Hello,&lt;/P&gt;&lt;P&gt;I'm trying to write a simple SQL script that echoes the value passed through an input parameter. I would like to use this in a derive-value variable so the format and length must match. Therefore I would also like to cast the value to a field of certain length (another Input Parameter).&amp;nbsp;&lt;/P&gt;&lt;P&gt;The following SQL code doesn't validate and the error I get is: "&lt;SPAN&gt;expected_unsigned_integer". IP_LEN is type HANA.tinyint which is unsigned, so I'm not sure what the problem is here. Any ideas?&lt;/SPAN&gt;&lt;/P&gt;&lt;pre class="lia-code-sample language-sql"&gt;&lt;code&gt;SELECT CAST(:IP_VAL as VARCHAR(:IP_LEN)) VALUE
FROM DUMMY_TABLE;&lt;/code&gt;&lt;/pre&gt;&lt;P&gt;Regards,&lt;/P&gt;&lt;P&gt;Dimitar&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/datasphere-sql-expected-unsigned-integer-error-on-tinyint/qaq-p/14280768"/>
    <published>2025-12-01T08:04:35.051000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/datasphere-issues-creating-data-federation-connection-between-datasphere/qaq-p/14281277</id>
    <title>Datasphere: Issues creating data federation connection between datasphere and S/4hana on-prem</title>
    <updated>2025-12-01T15:47:53.471000+01:00</updated>
    <author>
      <name>cassio_peia</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/890925</uri>
    </author>
    <content>&lt;P&gt;Hi guys,&lt;/P&gt;&lt;P&gt;we´re currently in the process of connecting our datasphere&amp;nbsp;to S/4HANA on-Prem using ABAP SQL services for data federation. We´re struggling to establish the connection with S/4HANA&lt;/P&gt;&lt;P&gt;During setup we followed the official documentation:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Cloud Connector: &lt;A href="https://help.sap.com/docs/SAP_DATASPHERE/9f804b8efa8043539289f42f372c4862/f289920243a34127b0c8b13012a1a4b5.html?locale=en-US" target="_blank" rel="noopener noreferrer"&gt;https://help.sap.com/docs/SAP_DATASPHERE/9f804b8efa8043539289f42f372c4862/f289920243a34127b0c8b13012a1a4b5.html?locale=en-US&lt;/A&gt;&lt;/LI&gt;&lt;LI&gt;ABAP SQL Services: &lt;A href="https://help.sap.com/docs/SAP_DATASPHERE/9f804b8efa8043539289f42f372c4862/4d7474595a5b41bb986616262ff44a3a.html?locale=en-US" target="_blank" rel="noopener noreferrer"&gt;https://help.sap.com/docs/SAP_DATASPHERE/9f804b8efa8043539289f42f372c4862/4d7474595a5b41bb986616262ff44a3a.html?locale=en-US&lt;/A&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;We get the following error message when trying to import remote tables in datasphere.&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="mfrosch_0-1764600421461.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/347179i999633F6D60AB714/image-size/large?v=v2&amp;amp;px=999" role="button" title="mfrosch_0-1764600421461.png" alt="mfrosch_0-1764600421461.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;According to SAP Note 3076454 we have to install the ODBC driver on S4. However S4 team informed us no ODBC is required on S4 side but the pre-installed in datasphere is sufficient. Hence I´m a bit confused whether ODBC is needed on S4 for our scenario, any thoughts from your side?&lt;/P&gt;&lt;P&gt;In addition we have already checked Note 3586001 (&lt;A href="https://userapps.support.sap.com/sap/support/knowledge/en/3586001" target="_blank" rel="noopener noreferrer"&gt;https://userapps.support.sap.com/sap/support/knowledge/en/3586001&lt;/A&gt;), but haven´t been able to resolve this matter.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;We´ve already double checked:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Cloud connector settings&lt;/LI&gt;&lt;LI&gt;Firewall (hanacloud.ondemand.com / k8s-hana.ondemand.com) and whitelisting all necessary IPs&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;Everything seems fine.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;The S4 connection is validated reachable in cloud connector.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;We looked into the cloud connector logs and see a failing handshake request and terminated connection due to SSL/TLS communication. Any advice how to approach this matter further? Looks to me we might have some certificate issues between cloud connector – BTP – Datapshere - S4, but that´s not really my area of expertise.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Any help appreciated!&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/datasphere-issues-creating-data-federation-connection-between-datasphere/qaq-p/14281277"/>
    <published>2025-12-01T15:47:53.471000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/sap-datasphere-how-to-monitor-real-time-data-replication-status-jobs-memory/qaq-p/14281473</id>
    <title>SAP Datasphere – How to monitor real-time data replication status (jobs, memory, failures) &amp; trigger</title>
    <updated>2025-12-01T21:44:19.419000+01:00</updated>
    <author>
      <name>justin_baboolall</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/2268566</uri>
    </author>
    <content>&lt;P&gt;We are looking to automate monitoring for real-time data replication in SAP Datasphere. Today, we manually check the Data Integration Monitor in Test/Prod for errors, but we want to build an alerting system that proactively notifies us when issues occur. Specifically, we need clarity on which underlying tables, views, or APIs store details about ongoing replication for remote tables—such as run status, timestamps, and any available runtime or resource metrics—so we can track jobs &lt;EM&gt;while&lt;/EM&gt; they are running, not only after completion. Our goal is to automatically detect running jobs, monitor memory consumption where possible, and send immediate email notifications if a job fails or behaves abnormally. Any guidance, including which technical objects to query or recommended approaches for implementing automated alerts, would be greatly appreciated.&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/sap-datasphere-how-to-monitor-real-time-data-replication-status-jobs-memory/qaq-p/14281473"/>
    <published>2025-12-01T21:44:19.419000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/question-about-data-products-with-live-access-in-sap-datasphere/qaq-p/14282056</id>
    <title>Question about Data Products with Live Access in SAP Datasphere</title>
    <updated>2025-12-02T13:49:33.350000+01:00</updated>
    <author>
      <name>benhaddou</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/891003</uri>
    </author>
    <content>&lt;P&gt;Hi everyone,&lt;/P&gt;&lt;P&gt;I’m working with a Data Product that is based on a live connection (&lt;EM&gt;Delivery Mode: Live Access&lt;/EM&gt;). When a user installs this Data Product in a space, the user receives a view.&lt;BR /&gt;However, when the underlying view changes, these changes are &lt;STRONG&gt;not&lt;/STRONG&gt; reflected in the installed Data Product.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;My question:&lt;/STRONG&gt;&lt;BR /&gt;Is it possible for a Data Product using &lt;EM&gt;Live Access&lt;/EM&gt; to automatically pick up changes made to the underlying view?&lt;BR /&gt;If yes, how can this be achieved?&lt;/P&gt;&lt;P&gt;From what I understand, &lt;EM&gt;“Edit Release”&lt;/EM&gt; only applies to Data Products using &lt;EM&gt;Full Replication&lt;/EM&gt;, not Live Access.&lt;/P&gt;&lt;P&gt;Any insights would be greatly appreciated, this would really help me.&lt;/P&gt;&lt;P&gt;Thanks and best regards&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/question-about-data-products-with-live-access-in-sap-datasphere/qaq-p/14282056"/>
    <published>2025-12-02T13:49:33.350000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/how-to-export-analytical-model-output-from-sap-datasphere-to-azure-data/qaq-p/14283304</id>
    <title>How to Export Analytical Model Output from SAP Datasphere to Azure Data Lake Storage?</title>
    <updated>2025-12-03T17:51:46.079000+01:00</updated>
    <author>
      <name>karteeksap</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/2240010</uri>
    </author>
    <content>&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Hello Experts,&lt;/P&gt;&lt;P&gt;I am working on a requirement where I need to export the output of an &lt;STRONG&gt;Analytical Model&lt;/STRONG&gt; in &lt;STRONG&gt;SAP Datasphere&lt;/STRONG&gt; into &lt;STRONG&gt;Azure Data Lake Storage (ADLS)&lt;/STRONG&gt;.&lt;/P&gt;&lt;P&gt;Connections to both &lt;STRONG&gt;Datasphere&lt;/STRONG&gt; and &lt;STRONG&gt;Azure&lt;/STRONG&gt; are already established.&lt;/P&gt;&lt;P&gt;&lt;STRONG&gt;Question:&lt;/STRONG&gt;&lt;BR /&gt;Is there a way to export Analytical Model data into ADLS?&lt;/P&gt;&lt;P&gt;I attempted to create a &lt;STRONG&gt;Replication Flow&lt;/STRONG&gt; with Datasphere as the source. However, when selecting the source object, I only see:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;P&gt;Local Tables&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;&lt;P&gt;Fact Views&lt;/P&gt;&lt;/LI&gt;&lt;LI&gt;&lt;P&gt;Dimension Views&lt;/P&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;The &lt;STRONG&gt;Analytical Models&lt;/STRONG&gt; do not appear as selectable source objects.&lt;/P&gt;&lt;P&gt;Has anyone implemented this scenario or knows if Analytical Models are supported in Replication Flows or if there is an alternative approach?&lt;/P&gt;&lt;P&gt;Thanks in advance!&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;a href="https://community.sap.com/t5/c-khhcw49343/SAP+Datasphere/pd-p/73555000100800002141" class="lia-product-mention" data-product="16-1"&gt;SAP Datasphere&lt;/a&gt;&amp;nbsp;&lt;a href="https://community.sap.com/t5/c-khhcw49343/Cloud+Integration/pd-p/67837800100800006801" class="lia-product-mention" data-product="361-1"&gt;Cloud Integration&lt;/a&gt;&amp;nbsp;&lt;a href="https://community.sap.com/t5/c-khhcw49343/Cloud+Connector/pd-p/0f95abc4-29f3-477d-874c-7c758b81f779" class="lia-product-mention" data-product="1193-1"&gt;Cloud Connector&lt;/a&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/how-to-export-analytical-model-output-from-sap-datasphere-to-azure-data/qaq-p/14283304"/>
    <published>2025-12-03T17:51:46.079000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/sap-datasphere-replication-flow-load-type-initial-and-delta-schedule/qaq-p/14286270</id>
    <title>SAP Datasphere - Replication Flow Load Type Initial and Delta Schedule</title>
    <updated>2025-12-08T20:42:38.284000+01:00</updated>
    <author>
      <name>Lauryn_Harvey</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/2237640</uri>
    </author>
    <content>&lt;P&gt;&lt;FONT size="6"&gt;Is there any feature for Datasphere Replication Flows with load type "Initial and Delta" to be scheduled?&lt;/FONT&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="5"&gt;Scenario 1&lt;/FONT&gt;&lt;/P&gt;&lt;P class="lia-indent-padding-left-30px" style="padding-left : 30px;"&gt;We want a replication flow with load type "Initial and Delta" to start at 9PM. In this scenario, the initial has not yet been loaded. Is there any way to trigger the replication flow&amp;nbsp;to begin at 9PM without having a user log in and manually click a button at 9PM?&lt;/P&gt;&lt;P class="lia-indent-padding-left-30px" style="padding-left : 30px;"&gt;Attempting to a schedule on the replication flow with load type "Initial and Delta" results in the error below.&lt;/P&gt;&lt;P class="lia-indent-padding-left-30px" style="padding-left : 30px;"&gt;&lt;STRONG&gt;&lt;EM&gt;"&lt;SPAN&gt;For replication flows that contain objects with load type "Initial and Delta/Delta Only", no schedule can be created."&lt;/SPAN&gt;&lt;/EM&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P class="lia-indent-padding-left-30px" style="padding-left : 30px;"&gt;Attempting to add the replication flow with load type&amp;nbsp;"Initial and Delta"&amp;nbsp; to a task chain results in the error below.&lt;/P&gt;&lt;P class="lia-indent-padding-left-30px" style="padding-left : 30px;"&gt;&lt;STRONG&gt;&lt;EM&gt;"Object '&amp;lt;ReplicationFlowName&amp;gt;' cannot be part of a task chain because it does not have an end (as it includes objects with load type Initial and Delta/Delta Only)."&lt;/EM&gt;&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;&lt;FONT size="5"&gt;Scenario 2&lt;/FONT&gt;&lt;/P&gt;&lt;P class="lia-indent-padding-left-30px" style="padding-left : 30px;"&gt;We want the replication flow with load type "Initial and Delta" to check for new delta records at 9PM every day. The delta load may take anywhere from 15 minutes up to 1 hour depending on the number of the changes observed. &lt;U&gt;Is there any way to ensure that the replication flow will start to load deltas every day at 9PM?&lt;/U&gt;&lt;/P&gt;&lt;P class="lia-indent-padding-left-30px" style="padding-left : 30px;"&gt;If the frequency is set to 24 hours, we observe that the next delta will begin to be loaded 24 hours &lt;U&gt;after the previous delta load had finished&lt;/U&gt;. After some time, the timing will no longer be close to 9PM. We are aware we can change the update frequency to twice daily, or hourly. Updating on that frequency is not necessary for the use case and there is concern for efficiency.&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/sap-datasphere-replication-flow-load-type-initial-and-delta-schedule/qaq-p/14286270"/>
    <published>2025-12-08T20:42:38.284000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/sap-bdc-object-store-vs-data-lake/qaq-p/14286893</id>
    <title>SAP BDC Object Store vs Data Lake</title>
    <updated>2025-12-09T11:54:30.152000+01:00</updated>
    <author>
      <name>arijitdaspwc</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/841910</uri>
    </author>
    <content>&lt;P&gt;Hello Experts,&lt;/P&gt;&lt;P&gt;Just curious to know - While deciding on the SAP BDC architecture, when should we use Data Lake storage vs when should we use Object Store?&lt;/P&gt;&lt;P&gt;In the BDC Sizing tool, once we add a Datasphere system, we can see that 1TB of Data Lake Storage will require ~560 CU/month. While, same amount of Object Store requires only ~80 CU/month. Refer to the attached screenshots.&lt;/P&gt;&lt;P&gt;What is the difference between these 2 storage options and what additional benefit is offered by the Data Lake Storage?&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="BDC Object Store.png" style="width: 999px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/350208i6E8E666570FD3854/image-size/large?v=v2&amp;amp;px=999" role="button" title="BDC Object Store.png" alt="BDC Object Store.png" /&gt;&lt;/span&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="BDC Data Lake.png" style="width: 637px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/350207i7FAC58B0DF694AC2/image-size/large?v=v2&amp;amp;px=999" role="button" title="BDC Data Lake.png" alt="BDC Data Lake.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Thanks&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/sap-bdc-object-store-vs-data-lake/qaq-p/14286893"/>
    <published>2025-12-09T11:54:30.152000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/how-do-we-join-and-delta-data-through-the-datasphere-system-and-out-to-an/qaq-p/14291482</id>
    <title>How do we join and delta data through the datasphere system and out to an external system</title>
    <updated>2025-12-16T11:42:05.238000+01:00</updated>
    <author>
      <name>RichardW</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1467571</uri>
    </author>
    <content>&lt;P&gt;We want to be able to stage and join SAP S/4 data through datasphere and into an external system.&lt;/P&gt;&lt;P&gt;This would involve creating some kind of view (lets say joining PO Item and PO History), adding logic and business semantics, and then passing out to an external system.&amp;nbsp;&lt;/P&gt;&lt;P&gt;So far so good. But we only want to pass changes to the data. Not full loads.&amp;nbsp;&lt;/P&gt;&lt;P&gt;The 2 CDS views covering this data trigger deltas independently of each other so we cant send the view every time 1 CDS triggers as the other may not have triggered. So in my eyes, we need to persist the joined up view and then send out changes. How is this handled. In BW this would have been staged to a common table (DSO) and this would have detected changes to the joined up data and passed it out of the system&lt;/P&gt;&lt;P&gt;We seem to have gone backwards here !? there seems to be no delta mechanism???&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;thanks -&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/how-do-we-join-and-delta-data-through-the-datasphere-system-and-out-to-an/qaq-p/14291482"/>
    <published>2025-12-16T11:42:05.238000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/how-to-connect-datasphere-to-microsoft-fabric-onelake/qaq-p/14291914</id>
    <title>How to connect Datasphere to Microsoft Fabric/OneLake</title>
    <updated>2025-12-16T20:51:37.879000+01:00</updated>
    <author>
      <name>RTT</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1569307</uri>
    </author>
    <content>&lt;P&gt;How do you connect SAP Datasphere to Microsoft OneLake? I cannot find the redirect URI, or the OAuth Endpoint. Steps are unclear on how to connect both systems, specifically bringing data from Datasphere into OneLake.&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/how-to-connect-datasphere-to-microsoft-fabric-onelake/qaq-p/14291914"/>
    <published>2025-12-16T20:51:37.879000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/how-to-use-azure-data-lake-gen2-parquet-files-as-source-for-sap-datasphere/qaq-p/14293169</id>
    <title>How to use Azure Data Lake Gen2 .parquet files as source for SAP DataSphere Flows?</title>
    <updated>2025-12-18T10:48:33.890000+01:00</updated>
    <author>
      <name>KCMT</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1887399</uri>
    </author>
    <content>&lt;P&gt;Hi all,&lt;/P&gt;&lt;P&gt;I want to use an Azure Data Lake Gen2 storage account as a source in DataSphere. SAP is steering us to only using replication flows and leave data flows behind. It would be nice if Replication Flows would be a valid option everywhere then. Here's what i found:&lt;BR /&gt;Replication flows don't support .parquet files in an Azure Data Lake Gen2 storage account, however it can read .csv files though.&lt;BR /&gt;Data flows support .parquet files and .csv files in a&amp;nbsp;Azure Data Lake Gen2 storage account.&lt;BR /&gt;&lt;BR /&gt;So now we're left with the question: Should i use a Data Flow with .parquet files, as .parquet files are preferred on the data lake side,&amp;nbsp; or should i use .csv files in my data lake, as replication flows are preferred on the DataSphere side?&lt;BR /&gt;&lt;BR /&gt;&lt;a href="https://community.sap.com/t5/c-khhcw49343/SAP+Datasphere/pd-p/73555000100800002141" class="lia-product-mention" data-product="16-1"&gt;SAP Datasphere&lt;/a&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/how-to-use-azure-data-lake-gen2-parquet-files-as-source-for-sap-datasphere/qaq-p/14293169"/>
    <published>2025-12-18T10:48:33.890000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/best-practice-for-incremental-loading-raw-data-for-non-sap-sources-in/qaq-p/14293232</id>
    <title>Best Practice for incremental loading Raw Data for Non SAP Sources in DataSphere without CDC?</title>
    <updated>2025-12-18T12:19:45.406000+01:00</updated>
    <author>
      <name>KCMT</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1887399</uri>
    </author>
    <content>&lt;P&gt;Hi all,&lt;/P&gt;&lt;P&gt;I'm working on a DataSphere implementation with 5 external non SAP ERP sources. For S4HANA Cloud the replication flows work very well. However, for the Non SAP sources most of them don't have CDC.&amp;nbsp;&lt;/P&gt;&lt;P&gt;What is the best practice for incremental loading non SAP raw data without CDC enabled? Would it be creating 2 data flows for every table, 1 for a full load and 1 for the incremental load?&lt;/P&gt;&lt;P&gt;Thanks in advance!&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/best-practice-for-incremental-loading-raw-data-for-non-sap-sources-in/qaq-p/14293232"/>
    <published>2025-12-18T12:19:45.406000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/datasphare-text-doesnt-show-when-using-associated-dimension/qaq-p/14293537</id>
    <title>Datasphare: Text doesnt show  when using associated dimension</title>
    <updated>2025-12-18T16:24:37.920000+01:00</updated>
    <author>
      <name>ejke</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/1781362</uri>
    </author>
    <content>&lt;P&gt;Dear colleagues,&lt;/P&gt;&lt;P&gt;I have a problem, I cant see text next to &lt;SPAN&gt;Associated Dimensions but when I add same dimension directly in my model everything works fine:&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="ejke_0-1766067758292.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/353667iD77D6B645D31F083/image-size/medium?v=v2&amp;amp;px=400" role="button" title="ejke_0-1766067758292.png" alt="ejke_0-1766067758292.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="ejke_1-1766067958282.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/353669i41798940477582E6/image-size/medium?v=v2&amp;amp;px=400" role="button" title="ejke_1-1766067958282.png" alt="ejke_1-1766067958282.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;Setup of associated dimension that doesnt show text:&lt;/P&gt;&lt;P&gt;&lt;span class="lia-inline-image-display-wrapper lia-image-align-inline" image-alt="ejke_2-1766068091970.png" style="width: 400px;"&gt;&lt;img src="https://community.sap.com/t5/image/serverpage/image-id/353670i2240E5B53F8E40D6/image-size/medium?v=v2&amp;amp;px=400" role="button" title="ejke_2-1766068091970.png" alt="ejke_2-1766068091970.png" /&gt;&lt;/span&gt;&lt;/P&gt;&lt;P&gt;Am I missing something or this is just DSP limitation currently so we can not see texts of associated dimensions?&lt;/P&gt;&lt;P&gt;Thank you in advance.&lt;/P&gt;&lt;P&gt;Kind regards,&lt;BR /&gt;Stefan&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/datasphare-text-doesnt-show-when-using-associated-dimension/qaq-p/14293537"/>
    <published>2025-12-18T16:24:37.920000+01:00</published>
  </entry>
  <entry>
    <id>https://community.sap.com/t5/technology-q-a/data-lake-files-physical-storage-and-object-store-in-datasphere-amp-bdc/qaq-p/14298147</id>
    <title>Data Lake Files physical storage and Object Store in Datasphere &amp; BDC context</title>
    <updated>2025-12-27T17:52:56.191000+01:00</updated>
    <author>
      <name>aalbis</name>
      <uri>https://community.sap.com/t5/user/viewprofilepage/user-id/744935</uri>
    </author>
    <content>&lt;P&gt;Greetings SAP experts!&lt;/P&gt;&lt;P&gt;For some time I am trying to understand some concepts in regards to the &lt;U&gt;Object Store&lt;/U&gt; and &lt;U&gt;Data Lake Files physical storage&lt;/U&gt; in Datasphere &amp;amp; BDC context.&lt;/P&gt;&lt;P&gt;First I will start with naming the main service components of the SAP HANA Cloud that you can subscribe in SAP BTP:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;&lt;STRONG&gt;SAP HANA Cloud, SAP HANA database&lt;/STRONG&gt;&lt;/LI&gt;&lt;LI&gt;&lt;STRONG&gt;SAP HANA Cloud, Data&lt;/STRONG&gt; &lt;STRONG&gt;Lake&amp;nbsp;&lt;/STRONG&gt;further formed from:&lt;UL&gt;&lt;LI&gt;Data Lake Files&lt;/LI&gt;&lt;LI&gt;Data Lake Relational Engine&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;A href="https://help.sap.com/docs/hana-cloud/sap-hana-cloud-getting-started-guide/sap-hana-cloud-components" target="_blank" rel="noopener noreferrer"&gt;https://help.sap.com/docs/hana-cloud/sap-hana-cloud-getting-started-guide/sap-hana-cloud-components&lt;/A&gt;&lt;/P&gt;&lt;P&gt;Now let's start with&amp;nbsp;&lt;STRONG&gt;SAP Datasphere&lt;/STRONG&gt;&lt;/P&gt;&lt;P&gt;When we have a&amp;nbsp;&lt;U&gt;Customer Managed&lt;/U&gt; Space:&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;With Storage Type &lt;U&gt;SAP HANA Database (Disk and In-Memory)&lt;/U&gt; -&amp;gt; I believe physical storage will be in &lt;STRONG&gt;SAP HANA Cloud, SAP HANA database&lt;/STRONG&gt; on which the Datasphere tenant itself relies on.&lt;/LI&gt;&lt;/UL&gt;&lt;UL&gt;&lt;LI&gt;When it comes to Storage Type &lt;U&gt;SAP HANA Data Lake Files&lt;/U&gt;:&lt;UL&gt;&lt;LI&gt;Where are the Data Lake Files behind the Local Table(file) physically stored?&lt;UL&gt;&lt;LI&gt;In the Object Store?&lt;UL&gt;&lt;LI&gt;If yes - What kind of Object Store is used?&lt;UL&gt;&lt;LI&gt;SAP Managed Object Store (File Container)?&lt;/LI&gt;&lt;LI&gt;External Object Store (Azure Blob Storage / Azure Data Lake, AWS S3 bucket,&amp;nbsp;&lt;SPAN class=""&gt;Google Cloud Storage&lt;/SPAN&gt;&lt;SPAN&gt;&amp;nbsp;&lt;/SPAN&gt;bucket)&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;LI&gt;In the SAP HANA Cloud, Data Lake Files (service component) of the SAP HAHA Cloud underlying SAP Datasphere tenant?&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;Further, in &lt;STRONG&gt;BDC&lt;/STRONG&gt; context.&lt;/P&gt;&lt;UL&gt;&lt;LI&gt;Let's say we activated a Data Package from BDC Cockpit, &lt;SPAN&gt;APRS starts pushing data to target HANA Data Lake Files i.e. SAP BDC Foundational services (FOS) layer.&lt;/SPAN&gt;&lt;UL&gt;&lt;LI&gt;&lt;SPAN&gt;Again, where are these Data Lake Files physically stored?&lt;/SPAN&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;/LI&gt;&lt;/UL&gt;&lt;P&gt;&lt;SPAN&gt;&lt;A href="https://help.sap.com/docs/hana-cloud-data-lake/welcome-guide/sap-hana-cloud-data-lake-terminology" target="_blank" rel="noopener noreferrer"&gt;https://help.sap.com/docs/hana-cloud-data-lake/welcome-guide/sap-hana-cloud-data-lake-terminology&lt;/A&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;&lt;A href="https://help.sap.com/docs/object-store/object-store-service-on-sap-btp/what-is-object-store" target="_blank" rel="noopener noreferrer"&gt;https://help.sap.com/docs/object-store/object-store-service-on-sap-btp/what-is-object-store&lt;/A&gt;&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&amp;nbsp;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Really appreciate any input helping me to clarify these topics.&lt;/SPAN&gt;&lt;/P&gt;&lt;P&gt;&lt;SPAN&gt;Kind regards!&lt;/SPAN&gt;&lt;/P&gt;</content>
    <link href="https://community.sap.com/t5/technology-q-a/data-lake-files-physical-storage-and-object-store-in-datasphere-amp-bdc/qaq-p/14298147"/>
    <published>2025-12-27T17:52:56.191000+01:00</published>
  </entry>
</feed>
